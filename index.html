<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Alex&apos;s personal blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Alex Chiu">
<meta property="og:url" content="http://AlexanderChiuluvB.github.io/index.html">
<meta property="og:site_name" content="Alex Chiu">
<meta property="og:description" content="Alex&apos;s personal blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Alex Chiu">
<meta name="twitter:description" content="Alex&apos;s personal blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://AlexanderChiuluvB.github.io/"/>





  <title>Alex Chiu</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Alex Chiu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/03/12/计网-运输层/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/12/计网-运输层/" itemprop="url">计网-运输层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-12T12:52:35+08:00">
                2019-03-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="运输层协议"><a href="#运输层协议" class="headerlink" title="运输层协议"></a>运输层协议</h3><p>在不同主机的应用进程之间提供了逻辑通信，通过逻辑通信，运行不同进程的主机就会好像直接相连一样。</p>
<p>而<strong>网络层</strong>提供了主机之间的逻辑联系。</p>
<p>发送端： 运输层把应用程序进程接收到的报文转换成运输层分组，成为运输层报文段。实现方法为把应用报文划分为较小的块，并为每块加上一个运输层首部以生成运输层报文段。在发送端系统当中，运输层把这些报文段传递给网络层，网络层将其封装为网络层分组，向目的地发送。<br>网络路由器仅仅作用于数据报的网络层字段，即不检查封装在该数据报的运输层报文段的字段。</p>
<p>接收端：网络层提取运输层报文段，把报文段上交给运输层，运输层处理接收到的报文段，使得该报文段中的数据为接收应用进程使用。</p>
<h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><p>UDP： 用户数据报协议：不可靠，无连接</p>
<p>TCP： 传输控制协议： 可靠，面向连接</p>
<p>运输层分组：报文段</p>
<p>IP：网络层的一个协议，每台主机一个IP地址，是不可靠服务</p>
<h4 id="多路复用与多路分解"><a href="#多路复用与多路分解" class="headerlink" title="多路复用与多路分解"></a>多路复用与多路分解</h4><p><img src="/home/alex/图片/ssssss.png" alt="upload sucessful"></p>
<p>运输层从下方的网络层接收报文段，运输层负责把这些报文段中的数据交付给主机上运行的应用程序进程。</p>
<p>一个进程有一个或多个套接字，socket作为进程与网络传递数据的一个门户。</p>
<p>那么怎么把运输层报文段定向到适当的套接字？接收端中，运输层检查这些字段，标识出接收套接字，进而把报文段定向到该套接字。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将运输层报文段中的数据交付到正确的套接字叫做多路分解</span><br></pre></td></tr></table></figure>
<p>在源主机从不同的套接字中收集数据块，并为每个数据块封装上首部信息从而生成报文段，然后把报文段传递到网络层，叫做多路复用。</p>
<h5 id="运输层报文段中的源与目的端口字段"><a href="#运输层报文段中的源与目的端口字段" class="headerlink" title="运输层报文段中的源与目的端口字段"></a>运输层报文段中的源与目的端口字段</h5><p><img src="/home/alex/图片/2018-236.png" alt="filename alrea exists, renamed"></p>
<p>1.套接字有唯一标识符<br>2.每个报文段有特殊字段来指示该报文段所要交付到的套接字。</p>
<p>在主机上的每个套接字能够分配一个端口号，当报文段到达主机时，运输层检查报文段中的目的端口号，并且将其定向到相应的套接字。然后报文段中的数据通过套接字进入其所连接的进程。</p>
<h4 id="无连接的多路复用与分解"><a href="#无连接的多路复用与分解" class="headerlink" title="无连接的多路复用与分解"></a>无连接的多路复用与分解</h4><p>无连接是指发送方和接收方的运输层实体之间没有握手。</p>
<p>例子：</p>
<p>主机A一个进程有UDP端口19157，他要发送一个应用程序数据块给位于主机B的另一个进程，该进程具有UDP端口46428,主机A的运输层创建一个运输层报文段，其中包括应用程序数据，源端口号以及目的端口号。</p>
<p>运输层把报文段传递给网络层封装到一个IP数据报，然后交付给主机B。B能够运行多个进程，每个进程有自己的UDP套接字与相应的端口号。B会检查报文段的目的端口号，将每个报文段定向分解到相应的socket</p>
<p>而源端口号的用途就是可以作为B回发报文给A的时候，作为返回地址的一部分。</p>
<h4 id="连接的多路复用与分解"><a href="#连接的多路复用与分解" class="headerlink" title="连接的多路复用与分解"></a>连接的多路复用与分解</h4><p>TCP套接字由一个四元组（源IP地址，源端口号，目的IP地址，目的端口号）来标志。</p>
<p>例子：</p>
<p>1.TCP服务器由一个 socket 它在12000端口上等待来自TCP客户的连接请求。</p>
<p>2.客户发起连接请求。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clientSocket = socket(AF_INET,SOCK_STREAM)</span><br><span class="line"></span><br><span class="line">clientSocket.connect((serverName,12000))</span><br></pre></td></tr></table></figure>
<p>3.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">connectionSoekct,addr = serverSoskct.accept()</span><br></pre></td></tr></table></figure>
<p>该服务器进程在端口12000等待接受连接，并且创建一个新的套接字。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/03/02/计网-应用层/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/02/计网-应用层/" itemprop="url">计网-应用层</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-02T10:32:24+08:00">
                2019-03-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="进程通信"><a href="#进程通信" class="headerlink" title="进程通信"></a>进程通信</h3><p>进行通信实际上是<em>进程</em>，本书关注运行在不同端系统上的进程通信，他们通过跨越计算机网络交换报文而相互通信。</p>
<h4 id="客户和服务器进程"><a href="#客户和服务器进程" class="headerlink" title="客户和服务器进程"></a>客户和服务器进程</h4><p>在每队通信进程中，我们会将这两个进程之一标记为客户，另一个标记为服务器。</p>
<ul>
<li>发起通信的进程被标记为客户，会话开始时候等待联系的进程是服务器。</li>
</ul>
<h4 id="进程与计算机网络之间的借口"><a href="#进程与计算机网络之间的借口" class="headerlink" title="进程与计算机网络之间的借口"></a>进程与计算机网络之间的借口</h4><p>进程通过一个称为套接字(socket)的软件接口向网络发送报文和从网络接收报文。</p>
<ul>
<li>当一个进程想向位于另外一台主机上的另外一个进程发送报文的时候，它会把报文推出该门(socket)，该发送进程假定该门到另外一侧之间有运输的基础设施。一旦该报文抵达目的主机，他会通过接收进程的套接字传递，然后接收进程对该报文进行处理。</li>
</ul>
<p>套接字实际上是网络和应用程序之间的可编程接口，</p>
<h4 id="进程寻址"><a href="#进程寻址" class="headerlink" title="进程寻址"></a>进程寻址</h4><p>接收进程需要一个地址。为了标志该接收进程，需要定义两种信息：1.主机的地址。2.定义在目的主机中的接收进程的标识符。</p>
<p>主机由IP地址标识，是一个32bit的量。同时发送进程还要指定接收进程（接收socket），port number就用于这个目的。</p>
<h4 id="运输服务"><a href="#运输服务" class="headerlink" title="运输服务"></a>运输服务</h4><p><em>运输层协议</em>负责使该报文进入接收进程的套接字。</p>
<p>一个运输层协议能够为调用它的应用程序提供：</p>
<ul>
<li>可靠的数据运输</li>
<li>吞吐量</li>
<li>定时和安全性</li>
</ul>
<h3 id="Internet-提供的运输服务"><a href="#Internet-提供的运输服务" class="headerlink" title="Internet 提供的运输服务"></a>Internet 提供的运输服务</h3><h4 id="TCP服务"><a href="#TCP服务" class="headerlink" title="TCP服务"></a>TCP服务</h4><ul>
<li>面向连接的服务</li>
</ul>
<p>TCP 让客户和服务器互相交换运输层控制信息，这个所谓握手过程提示客户和服务器，使他们为大量分组的到来做好准备。握手之后，一个TCP链接就在两个进程的套接字之间建立。</p>
<ul>
<li>可靠的数据传送服务</li>
</ul>
<p>应用程序一端将字节流传进套接字的时候，能够依靠TCP将相同的字节流交付给接收方的套接字。而没有字节的丢失和冗余。</p>
<h3 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h3><p>超文本传输协议由两个程序实现，一个客户程序和一个服务器程序。客户程序和服务器程序运行在不同的端系统中，通过交换HTTP报文进行会话，HTTP定义了这些报文的结构以及客户和服务器进行报文交换的方式。</p>
<h3 id="Web"><a href="#Web" class="headerlink" title="Web"></a>Web</h3><p>每个URL地址由两个部分组成。存放对象的服务器主机名和对象的路径名。<br><a href="https://github.com/Hananel-Hazan/bindsnet" target="_blank" rel="noopener">https://github.com/Hananel-Hazan/bindsnet</a></p>
<p>github.com 就是主机名 后面的是路径名</p>
<p>web 浏览器实现了HTTP的客户端，所以在web环境中交替使用浏览器和客户两个术语。</p>
<p>web服务器实现了HTTP的服务器端，它用于存储web对象。</p>
<p>HTTP定义了web客户向web服务器请求web页面的方式，以及服务器向客户传送web页面的方式。HTTP使用TCP作为其支撑运输协议，HTTP客户首先发起一个与服务器的TCP连接，一旦链接建立，浏览器和服务器进程就可以通过套接字接口访问TCP。</p>
<p>同时TCP也是一个无状态协议：服务器向客户发送被请求的文件，但是不存储任何关于该客户的状态信息。</p>
<h3 id="非持续链接"><a href="#非持续链接" class="headerlink" title="非持续链接"></a>非持续链接</h3><p>每个请求和响应对是经过一个单独的TCP连接发送的。</p>
<h3 id="持续链接"><a href="#持续链接" class="headerlink" title="持续链接"></a>持续链接</h3><p>所有的请求及其响应都经过相同的TCP连接发送</p>
<h3 id="TCP的三次握手"><a href="#TCP的三次握手" class="headerlink" title="TCP的三次握手"></a>TCP的三次握手</h3><p>我们给出往返时间的定义。即一个短分组从客户到服务器然后再返回客户所花费的时间。</p>
<p>当浏览器在它和web服务器发起一个TCP链接。</p>
<p>第一次握手：客户向服务器发送一个TCP报文段。</p>
<p>第二次握手：然后服务器用一个小TCP报文段做出确认和响应。</p>
<p>第三次握手：客户的确认阶段，向该TCP链接发送一个HTTP请求报文，一旦报文到达服务器，服务器就在该TCP链接上发送HTML文件。该HTTP请求/响应用去了另外一个RTT。</p>
<p>因此总响应时间是两个RTT加上服务器传输HTML文件的时间。</p>
<h3 id="用户与服务器的交互：cookie"><a href="#用户与服务器的交互：cookie" class="headerlink" title="用户与服务器的交互：cookie"></a>用户与服务器的交互：cookie</h3><p>cookie用于标识一个用户。用户首次访问一个站点的时候，可能需要提供一个用户标识，在后继会话中，浏览器向服务器提供一个cookie首部，从而向该服务器标识了用户。</p>
<h3 id="Web缓存（proxy-cache）"><a href="#Web缓存（proxy-cache）" class="headerlink" title="Web缓存（proxy cache）"></a>Web缓存（proxy cache）</h3><p>也叫代理服务器，能够代表初始web服务器来满足HTTP请求的网络实体。web缓存器有自己的磁盘存储空间，并在存储空间中保存最近请求过的对象的副本。</p>
<p>用户的所有HTTP请求首先会指向web缓存器，例如要访问<a href="http://www.someschool.edu/campus.gif，会发生如下情况：" target="_blank" rel="noopener">http://www.someschool.edu/campus.gif，会发生如下情况：</a></p>
<ul>
<li><p>浏览器建立一个到web服务器的TCP链接，并向Web缓存器的对象发送一个HTTP请求</p>
</li>
<li><p>web缓存器进行检查，查看本地是否存储了该对象副本，如果有，web缓存器就向客户浏览器用HTTP响应报文返回该对象。</p>
</li>
<li><p>如果web缓存器没有对象，就打开一个与该对象的初始服务器的TCP链接，web缓存器则在这个缓存器到服务器的TCP链接上发送一个对该对象的HTTP请求。在收到该请求后，初始服务器向该web缓存器发送具有该对象的HTTP响应。</p>
</li>
<li><p>当web缓存器接收到该对象的时候，会在本地存储空间存储一个副本，并且向客户的浏览器用HTTP响应报文发送该副本。</p>
</li>
</ul>
<p>当它接收浏览器的请求并发挥响应的时候，他是服务器，当他向初始服务器发出请求并接收响应的时候，他是一个客户。</p>
<h3 id="条件GET方法"><a href="#条件GET方法" class="headerlink" title="条件GET方法"></a>条件GET方法</h3><p>用来保证缓存器所存储的对象是最新的。</p>
<p>如果请求报文使用get方法而且请求报文中包含一个“If modifies since”首部行，那么HTTP请求报文就是一个条件GET请求报文。</p>
<h3 id="DNS：因特网的目录服务"><a href="#DNS：因特网的目录服务" class="headerlink" title="DNS：因特网的目录服务"></a>DNS：因特网的目录服务</h3><p>主机的标识方法：可以用主机名(hostname）或者(IP address)</p>
<p>而（DOMAIN NAME SYSTEM）就是用来进行主机名到IP地址转换的目录服务。</p>
<h4 id="DNS-是什么"><a href="#DNS-是什么" class="headerlink" title="DNS 是什么"></a>DNS 是什么</h4><ul>
<li>DNS 是一个有分层的DNS服务器实现的分布式数据库.</li>
<li>也可以理解为一个使得主机能查询分布式数据库的应用层协议。</li>
<li>DNS 服务器通常是运行在BIND软件的UNIX机器。DNS协议运行在UDP之上，使用53号端口。</li>
</ul>
<h4 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h4><ul>
<li><p>同一台用户主机上运行着DNS应用的客户端</p>
</li>
<li><p>浏览器从URL中抽取主机名，并把这台主机名传给DNS应用的客户端。</p>
</li>
<li><p>DNS客户向DNS服务器发送一个包含主机名的请求</p>
</li>
<li><p>DNS客户最终收到一份回答报文，含有对应于该主机名的IP地址</p>
</li>
<li><p>浏览器接收到来自DNS的该IP地址，能够向位于该IP地</p>
</li>
</ul>
<h4 id="DNS-功能"><a href="#DNS-功能" class="headerlink" title="DNS 功能"></a>DNS 功能</h4><p>1.进行主机名到IP地址的转换。</p>
<p>2.主机别名（host aliasing）</p>
<p>为复杂主机名的主机提供简单的别名</p>
<p>3.邮件服务器别名。应用程序因此可以调用DNS，对提供邮件服务器别名进行解析，以获得该主机的规范主机名以及IP地址。</p>
<p>4.负载分配。</p>
<p>例如繁忙的站点会分布在多台机器上，每台服务器均运行在不同的端系统上，每个都有不同的IP地址，因此客户发出DNS请求的时候，服务器用IP地址的整个集合来进行响应。</p>
<h4 id="分布式，层次的数据库"><a href="#分布式，层次的数据库" class="headerlink" title="分布式，层次的数据库"></a>分布式，层次的数据库</h4><p>例子: 一个客户要决定<a href="http://www.amazon.com" target="_blank" rel="noopener">www.amazon.com</a> 的IP地址，先和根服务器之一联系，它将返回顶级域名com的TLD服务器的IP地址。该客户与这些TLD服务器之一联系，服务器将返回权威服务器的IP地址。</p>
<ul>
<li><p>根DNS服务器<br>全球有13个，用于返回TLD服务器的IP地址</p>
</li>
<li><p>顶级域（DNS）服务器（TLD）</p>
</li>
</ul>
<p>负责顶级域名如 com、org、net、edu 和 gov，以及所有国家的顶级域名如 uk、fr 等。TLD 服务器返回权威服务器的 IP 地址</p>
<ul>
<li>权威DNS服务器</li>
</ul>
<p>管理属于同一机构同一域名但是的不同IP地址的不同主机。</p>
<ul>
<li>本地DNS服务器<br>起着代理的作用。每个Internet service provider (ISP)都有一台本地的DNS服务器，本地DNS服务器起着代理的作用，本地主机把DNS请求发向本地DNS服务器，本地DNS服务器把该请求转发到DNS服务器层次结构当中。</li>
</ul>
<p><img src="/home/alex/图片/2018-234.png" alt="filename alreadyexists, renamed"></p>
<h4 id="DNS的解析流程"><a href="#DNS的解析流程" class="headerlink" title="DNS的解析流程"></a>DNS的解析流程</h4><p>浏览器访问域名的前置步骤：</p>
<p>1.先检查缓存中是否有该域名对应的解析过的IP地址，命中则解析过程结束，否则进行步骤2<br>2.检查操作系统缓存中是否有域名对应的DNS解析结果，命中则解析过程结束。</p>
<h4 id="DNS在服务器之间的解析步骤"><a href="#DNS在服务器之间的解析步骤" class="headerlink" title="DNS在服务器之间的解析步骤"></a>DNS在服务器之间的解析步骤</h4><p>下图例子假设主机 cs.ustc.edu 想知道主机 cs.csu.edu 的 IP 地址，假设 USTC 大学的本地 DNS 服务器为 dns.ustc.edu，同时假设 CSU 大学的权威 DNS 服务器为 dns.csu.edu。</p>
<p>1.主机向本地DNS服务器dns.ustc.edu发送请求报文，报文中含有被转换的主机名cs.csu.edu</p>
<p>2.本地DNS服务器把查询报文转发给根DNS服务器，根服务器注意到edu<br>前缀，于是把负责edu的TLD的IP地址列表返回给本地DNS服务器</p>
<p>3.本地DNS服务器再次向TLD服务器之一发送DNS请求报文，TLD服务器注意到csu.edu的前缀，于是把权威服务器dns.csu.edu的IP地址返回给DNS服务器</p>
<p>4.本地DNS服务器向权威服务器dns.csu.edu 发送请求报文，权威服务器用cs.csu.edu的IP地址进行响应。</p>
<p>5.最后本地DNS服务器把查询得到的IP地址返回给主机cs.ustc.edu</p>
<h4 id="DNS缓存"><a href="#DNS缓存" class="headerlink" title="DNS缓存"></a>DNS缓存</h4><p>本地DNS服务器在完成一次查询之后会把得到的主机名到IP地址的映射缓存到本地，加快DNS的解析速度。解析大多是在本地服务器完成的</p>
<h4 id="DNS记录"><a href="#DNS记录" class="headerlink" title="DNS记录"></a>DNS记录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(Name,Value,Type,TTL)</span><br></pre></td></tr></table></figure>
<p>1.Type=A,则Name是主机名，Value是主机对应的Ip地址</p>
<p>2.Type=NS，Name是域名，比如csu.edu，Value是知道如何获得域中主机IP地址的权威DNS服务器的主机名。</p>
<p>3.TYpe=CNAME，VALUE是别名为Name的主机对应的规范主机名</p>
<p>如果服务器不是用于某主机名的权威服务器，那么该服务器将包含一条类型 NS 记录，该记录对应于包含主机名的域；它还将包括一条类型 A 记录，该记录提供了在 NS 记录的 Value 字段中的 DNS 服务器的 IP 地址。举例来说，假设一台 edu TLD 服务器不是主机 gaia.cs.umass.edu 的权威 DNS 服务器，则该服务器将包含一条包括主机 cs.umass.edu 的域记录，如（umass.edu，dns.umass.edu，NS）；该 edu TLD 服务器还将包含一条类型 A 记录，如（dns.umass.edu，128.119.40.111，A），该记录将名字 dns.umass.edu 映射为一个 IP 地址。</p>
<p><strong>注册一个全新的域名最少要向对应的 TLD 注入 A 型与 NS 型两种记录。</strong></p>
<h4 id="DNS-报文"><a href="#DNS-报文" class="headerlink" title="DNS 报文"></a>DNS 报文</h4><p><img src="/home/alex/图片/2018-235.png" alt="filename alredy exists, renamed"></p>
<blockquote>
</blockquote>
<pre><code>DNS 报文格式前 12 个字节是首部区域，其中有几个字段。第一个字段（标识符）是一个 16 比特的数，用于标识该查询。这个标识符会被复制到对查询的回答报文中，以便让客户用它来匹配发送的请求和接收到的回答。标志字段中含有若干标志。1 比特的“查询/回答”标志位指出报文是查询报文（0）还是回答报文（1）。当某 DNS 服务器是所请求名字的权威 DNS 服务器时，1 比特的“权威的”标志位被置在回答报文中。如果客户（主机或者DNS 服务器）在该 DNS 服务器没有某记录时希望它执行递归查询，将设置 1 比特的“希望递归”标志位。如果该 DNS 服务器支持递归查询，在它的回答报文中会对 1 比特的“递归可用”标志位置位。在该首部中，还有 4 个有关数量的字段，这些字段指出了在首部后的 4 类数据区域出现的数量。
问题区域包含着正在进行的查询信息。该区域包括：①名字字段，指出正在被查询的主机名字；②类型字段，它指出有关该名字的正被询问的问题类型，例如主机地址是与一个名字相关联（类型 A）还是与某个名字的邮件服务器相关联（类型 MX）。
在来自 DNS 服务器的回答中，回答区域包含了对最初请求的名字的资源记录。前面讲过每个资源记录中有 Type（如 A、NS、CNAME 和 MX）字段、Value 字段和 TTL 字段。在回答报文的回答区域中可以包含多条 RR，因此一个主机名能够有多个 IP 地址（例如，就像本节前面讨论的冗余 Web 服务器）。
权威区域包含了其他权威服务器的记录。
附加区域包含了其他有帮助的记录。例如，对于一个 MX 请求的回答报文的回答区域包含了一条资源记录，该记录提供了邮件服务器的规范主机名。该附加区域包含一个类型 A 记录，该记录提供了用于该邮件服务器的规范主机名的 IP 地址。
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/02/15/Nodejs-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/Nodejs-1/" itemprop="url">webstorm中Express项目的创建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T20:27:33+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/home/alex/图片/2018-230.png" alt="flename already exists, renamed"></p>
<h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">var express = require(&apos;express&apos;);</span><br><span class="line">var app = express();</span><br><span class="line"></span><br><span class="line">app.get(&apos;/&apos;,function(req,res)&#123;</span><br><span class="line">    res.send(&apos;Hello world&apos;);</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">var server = app.listen(8080,function ()</span><br><span class="line">&#123;</span><br><span class="line">    console.log(&apos;Server listening at http://&apos;+server.address().address+&apos;:&apos;+server.address().port);</span><br><span class="line"></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h3 id="Express-的路由"><a href="#Express-的路由" class="headerlink" title="Express 的路由"></a>Express 的路由</h3><p>对于任何一个应用服务器，核心在与它是否有一个强劲的路由，路由指<em>客户端所发出的网络请求机制</em>，web在URL中指明它想要的内容，客户端通过路由，为不同的访问路径指定不同的处理方法。</p>
<p>app.get(‘/‘,function(req,res){ … });</p>
<p>第一个参数是访问路径，/表示根路径，第二个参数是回调函数，它的req参数表示客户端发来的HTTP请求，res参数表示服务器返回给客户端的响应，两个参数都是对象，在回调函数内部，使用HTTP响应的send方法，表示向浏览器发送一个字符串。</p>
<p>express框架相当于在HTTP模块上添加了一个中间件</p>
<h3 id="Express-中间件"><a href="#Express-中间件" class="headerlink" title="Express 中间件"></a>Express 中间件</h3><p>中间件是对HTTP请求功能的一种封装，是一个处理HTTP请求的函数，他有三个参数，一个网络请求对象，一个服务器响应对象，一个next函数。最大特点是，一个中间件处理完，再传递给下一个中间件，应用程序在运行过程中，会调用一系列的中间件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">var express = require(&apos;express&apos;);</span><br><span class="line">var app = express();</span><br><span class="line"></span><br><span class="line">app.use(function(req,res,next)&#123;</span><br><span class="line">    console.log(&apos;processing request for &apos; +req.url);</span><br><span class="line">    next();</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">app.use(function(req,res,next)&#123;</span><br><span class="line">    console.log(&apos;terminating request&apos;);</span><br><span class="line">    res.send(&apos;thanks for playing&apos;);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">app.use(function(req,res,next)&#123;</span><br><span class="line">   console.log(&apos;I never get called!&apos;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>第一个中间件只输出了一个log信息，把请求传给了下一个中间件，而第二个中间件真正的处理请求，这时候不会调用next（）方法，请求处理这个时候已经终止了。</p>
<p>Express中的cookie与session</p>
<p>cookie是HTTP协议的一部分，处理分为以下几步：</p>
<p>1.服务器向客户端发送cookie</p>
<p>2.浏览器把cookie保存</p>
<p>3.浏览器每次请求的时候，都会把之前保存的Cookie发给服务器。</p>
<h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>HTTP协议是一种无状态的协议，用户从页面A跳转到页面B的时候，会重新发送一次HTTP请求，而服务器端在返回响应的时候无法获知该用户在请求页面B之前做了什么，所以需要Session来保存用户的状态。</p>
<h3 id="Express中的网络请求方法"><a href="#Express中的网络请求方法" class="headerlink" title="Express中的网络请求方法"></a>Express中的网络请求方法</h3><p>1.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">app.get(&apos;/&apos;,function(req,res)&#123;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p>2.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">app.get(&apos;/user/:id&apos;,function(req,res)</span><br><span class="line">&#123;</span><br><span class="line">res.send(&apos;user &apos; + res.params.id);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p>function(req,res)是一个回调函数，通过req对象可以获取请求的params参数。</p>
<h3 id="req-params"><a href="#req-params" class="headerlink" title="req.params"></a>req.params</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function(req,res)是一个回调函数，通过req对象能够获得请求的param参数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">var express = require(&apos;express&apos;);</span><br><span class="line">var app = express();</span><br><span class="line"></span><br><span class="line">app.get(&apos;/user/:id&apos;,function(req,res)&#123;</span><br><span class="line">    res.send(&apos;user is &apos;+req.params.id);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">app.listen(8888,function()&#123;</span><br><span class="line">    console.log(&apos;Server is listening&apos;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/02/13/DL-C5W3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/DL-C5W3/" itemprop="url">DL-C5W3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-13T16:51:13+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Seq2Seq-模型"><a href="#Seq2Seq-模型" class="headerlink" title="Seq2Seq 模型"></a>Seq2Seq 模型</h3><h4 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h4><p><img src="/home/alex/图片/2018-220.png" alt="filenamalready exists, renamed"></p>
<p>先建立一个编码网络，是一个RNN的结构，RNN的单元可以使GRU也可以是LSTM，每次向该神经网络输入一个法语单词，将输入序列接收完毕之后，RNN会输出一个向量来代表这个输入序列。</p>
<p>接着建立一个解码网络，如上图二所示，以编码网络的输出作为解码网络的输入。之后它可以被训练为每次输出一个翻译后的单词，一直到它输出序列的结尾或者句子结尾标记。而且每次生成一个标记，都会传递到下一个单元中进行预测。</p>
<h4 id="图像描述"><a href="#图像描述" class="headerlink" title="图像描述"></a>图像描述</h4><p><img src="/home/alex/图片/image captioning.png" alt="upload succeful"></p>
<p>先把图片输入到卷积神经网络中，如一个预训练的AlexNet结构，然后让其学习图片的编码，如果是图片分类任务，最后得到的一个特征向量输出到一个softmax层，而这里把softmax层替换为一个RNN，RNN要做的就是生成图像的描述，每次生成一个单词。</p>
<h3 id="选择最有可能的句子"><a href="#选择最有可能的句子" class="headerlink" title="选择最有可能的句子"></a>选择最有可能的句子</h3><h4 id="机器翻译（条件语言模型）"><a href="#机器翻译（条件语言模型）" class="headerlink" title="机器翻译（条件语言模型）"></a>机器翻译（条件语言模型）</h4><p><img src="/home/alex/图片/2018-221.png" alt="filename alrady exists, renamed"></p>
<p>绿色表示encoder网络，紫色表示decoder网络，对于生成语言模型来说，encoder网络是以零向量开始，对于机器翻译模型来说，encoder网络会计算出一系列向量来表示输入的句子，有了这个输入句子，decoder网络就可以从这个句子开始，而不是从零向量开始，所以叫<em>条件语言模型</em></p>
<p><img src="/home/alex/图片/2018-222.png" alt="filename already exists, rnamed"></p>
<p>x这里是法语句子，当进行机器翻译的时候，要找到一个英语句子y，使得条件概率最大化。通常使用<em>束搜索</em>方法。</p>
<p>我们W1生成的语言模型是在一个句子当中随机生成单词，但是例如下面句子：</p>
<p><img src="/home/alex/图片/句子.png" alt="successful"></p>
<p>随机生成的语言模型挑选了JANE IS 之后，由于going在英语中更加常见，因此很有可能采取下面那一句实际上并不太好的翻译。</p>
<h3 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h3><p><img src="/home/alex/图片/2018-223.png" alt="filename alady exists, renamed"></p>
<p>列出一个10000词的词汇表，为了简化问题，忽略大小写，把所有单词都以小写列出来，首先利用编码部分评估第一个单词的概率值，贪婪算法只会挑出最可能的一个单词，而beam search有一个参数B，例如设为3，那么就会考虑三个不同的单词。</p>
<p>所以第一步就是输入法语句子到编码网络，然后会解码这个网络，softmax层会输出10000个概率值，取前三个存起来。</p>
<p>第二步，假设已经选出了in,jane,september。作为第一个单词红最有可能的选择，算法接下来会针对每一个单词考虑第二个单词是什么。</p>
<p><img src="/home/alex/图片/for.png" alt="upload ccessful"></p>
<p>把第一个单词输出作为下一个单元的输入，表示考虑第一个单词的情况下预测第二个单词。在第二步中我们更关心的事要找到最可能的第一个和第二个单词对，写成条件概率，就是上面的7式，等于第一个单词出现的概率乘以考虑第一个单词的情况下第二个单词的概率。</p>
<p><img src="/home/alex/图片/2018-224.png" alt="filename alrey exists, renamed"></p>
<p>当选中了第一个单词的3个候选的时候，每个候选单词有10000个选择，所以一共有3x10000=30000中可能结果，按照第一和第二个词的概率，选出前三个，把这30000个可能性又变成了3个。</p>
<p>如果找到了第一个和第二个单词对最有可能的三个选择分别是in September,jane is 和jane visits，那么就去掉了september作为第一个单词的可能。</p>
<h3 id="改进集束搜索"><a href="#改进集束搜索" class="headerlink" title="改进集束搜索"></a>改进集束搜索</h3><p><img src="/home/alex/图片/2018-225.png" alt="filename already xists, renamed"></p>
<p>但概率都小于1，很多个小于1的数相乘，会得到很小很小的数字，会造成数值下溢。因此可以加上log，最大化这个log求和的概率值。</p>
<p><img src="/home/alex/图片/2018-226.png" alt="filename aeady exists, renamed"></p>
<p>对于目标函数可以做一些改变，如果要预测一个很长的句子，那么每个单词都要乘以一个很小的数字，最后得到一个更小的概率值，所以这个目标函数有一个缺点，就是会不自然地倾向于更短的翻译结果。</p>
<p>我们可以不再最大化这个目标函数，而是把它归一化通过除以翻译结果的单词数量。这时候这个目标函数也叫作归一化的对数似然函数，也就是取每个单词的概率对数值的平均，这样很明显地减少了对输出长的结果的惩罚，</p>
<p>同时我们也可以对这个单词数量加上一个指数α，这是一个超参数，如果α=0，则不进行归一化，如果α为1，相当于用完全长度来归一化。</p>
<p>总结一下如何运行这个算法：</p>
<p><img src="/home/alex/图片/2018-227.png" alt="filename alreaexists, renamed"></p>
<h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>模型有两个部分，一个神经网络模型（seq-to-seq），我们称为RNN模型，实际上是个编码器和解码器。另一部分是束搜索算法，以某个集束宽度B运行。</p>
<p>如果翻译错误的话，究竟是模型的那个部分出了错呢？</p>
<p><img src="/home/alex/图片/2018-228.png" alt="filename eady exists, renamed"></p>
<p>假设y*是理想结果，而与y^是机器给出的结果，如果是情况一的话，RNN模型输出的结果是</p>
<p><img src="/home/alex/图片/2018-229.png" alt="filename aleady exists, renamed"></p>
<p>但是束搜索却选择了y^，明明y*的得分更高，因此是束搜索算法出了错。</p>
<p>第二种情况则是RNN模型出了错，它评分大小比较给错了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/02/10/DL-C5W2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/10/DL-C5W2/" itemprop="url">DL-C5W2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-10T13:32:24+08:00">
                2019-02-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="词汇表征"><a href="#词汇表征" class="headerlink" title="词汇表征"></a>词汇表征</h3><h4 id="one-hot-encoding"><a href="#one-hot-encoding" class="headerlink" title="one-hot encoding"></a>one-hot encoding</h4><p>比如如果man在词典里是第 5391 个,那么就可以表示成一个向量,只在第 5391 处为 1(上图<br>编号 2 所示),我们用O 5391 代表这个量,这里的O代表 one-hot。接下来,如果 woman 是编<br>号 9853(上图编号 3 所示),那么就可以用O 9853 来表示,这个向量只在 9853 处为 1</p>
<p><em>缺点 孤立了单词，使得算法对相关词的泛化能力不强</em></p>
<p><img src="/home/alex/图片/2018-199.png" alt="filename already exists, named"></p>
<h4 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h4><p>例如要把单词表征为一个三百维的向量，每一维数值大小表示这个单词与对应类别的联系程度，如上图，man与woman与gender联系比较大，因此数值接近于1，而与royal联系比较小，因此数值趋近于0.</p>
<p><img src="/home/alex/图片/leibi.png" alt="upload sccessful"></p>
<p>你能做的就是找到单词 w 来<br>使得,e man − e woman ≈ e king − e w 这个等式成立,你需要的就是找到单词 w 来最大化e w 与<br>e king − e man + e woman 的相似度,即</p>
<p><img src="/home/alex/图片/2018-200.png" alt="filename already existsrenamed"></p>
<p>而计算相似度，通常使用余弦相似度。</p>
<p><img src="/home/alex/图片/2018-201.png" alt="filename alre exists, renamed"></p>
<h4 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h4><p><img src="/home/alex/图片/2018-202.png" alt="filename alreadyexists, renamed"></p>
<p>假如要把单词压缩成一个300维的嵌入向量，然后一共有10000个单词，那么可以随机初始化一个300x10000的矩阵。每一列代表一个单词的嵌入向量，那么把这个随机的矩阵与一个one-hot向量相乘就得到了这个单词的嵌入向量。</p>
<p><img src="/home/alex/图片/so.png" alt="upload succsful"></p>
<p>然后把如果你要预测 </p>
<p>“I want a glass of orange ___.”,<br>的下一个词</p>
<p><img src="/home/alex/图片/2018-203.png" alt="filename already exists, named"></p>
<p>把这个句子所有单词转换为嵌入向量后，全部放到一个隐藏层中（上图3），然后再通过一个分类的softmax层，预测出对应单词。</p>
<p>在这里这个隐藏层是一个1800维的向量（6x300）,有自己的参数。</p>
<p>如果你的目标是建立一个语言模型，那么一般选取目标词之前的几个词作为上下文。</p>
<p>但如果你的目标不是学习语言模型本身，也可以选择其他上下文。</p>
<p><img src="/home/alex/图片/2018-204.png" alt="filee already exists, renamed"></p>
<p>例如要预测一个句子中间的单词，可以用前面和后面各4个单词作为上下文。</p>
<p>或者你想用一个更简单的上下文,也许只提供目标词的前一个词。</p>
<h4 id="word2Vec"><a href="#word2Vec" class="headerlink" title="word2Vec"></a>word2Vec</h4><p>假设在训练集中给定了一个这样的句子:“I want a glass of orange juice to go along with<br>my cereal.”,在 Skip-Gram 模型中,我们要做的是抽取上下文和目标词配对,来构造一个监<br>督学习问题。</p>
<p>我们要随机选一个单词作为上下文词，比如选orange，然后要随机在一定词距内选择另外一个词，作为目标词，通过这种方法来学到一个好的词嵌入模型。</p>
<h3 id="Word2Vec-Model"><a href="#Word2Vec-Model" class="headerlink" title="Word2Vec Model"></a>Word2Vec Model</h3><p>有两个模型，一个是skip-gram，另外一个是Word2Vec模型。</p>
<p><img src="/home/alex/图片/2018-205.png" alt="filename alrey exists, renamed"></p>
<p><img src="/home/alex/图片/xxx.png" alt="successful"></p>
<p>但是这种方法对于词汇量很大的词汇表来说运算速度会变得非常非常的慢。</p>
<p>解决方法：</p>
<p><em>分级softmax分类器</em></p>
<p><img src="/home/alex/图片/2018-206.png" alt="filename already exists, renamed"></p>
<p>意思就是<br>说不是一下子就确定到底是属于 10,000 类中的哪一类。想象如果你有一个分类器(上图编<br>号 1 所示),它告诉你目标词是在词汇表的前 5000 个中还是在词汇表的后 5000 个词中,假<br>如这个二分类器告诉你这个词在前 5000 个词中(上图编号 2 所示),然后第二个分类器会<br>告诉你这个词在词汇表的前 2500 个词中,或者在词汇表的第二组 2500 个词中,诸如此类</p>
<p>常用的词会在树的顶部，不常用的词会在树的更加深的地方。</p>
<p>而且实际上上下文词的概率p(c)的分布并不是单纯的在训练集语料库上均匀且随机的采样得到的,而是采用了<br>不同的分级来平衡更常见的词和不那么常见的词</p>
<h4 id="Cbow-与-skip-gram"><a href="#Cbow-与-skip-gram" class="headerlink" title="Cbow 与 skip-gram"></a>Cbow 与 skip-gram</h4><p>CBOW是从原始语句推测目标字词，对于小型数据库比较合适。而skip-gram正好相反，是从目标字词测出原始语句，在大型语料中表现更好。</p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>上面介绍的skip-gram模型实际上是构造一个监督学习的任务，把上下文映射到了目标词上面，让你学到一个实用的词嵌入，缺点在于softmax计算起来很慢。</p>
<p>在这个算法中要做的是构造一个新的监督学习问题，给定一对单词比如orange和juice，去预测这是否是一对上下文词-目标词（context-target）</p>
<p><img src="/home/alex/图片/2018-207.png" alt="filename already exists, named"></p>
<p><img src="/home/alex/图片/2018-208.png" alt="filename already exists, renamed"></p>
<p>然后接下来构造一个监督学习问题，其中学习算法输入x，输入这对词，编号7，要去预测目标的标签，编号8，即预测输出y，因此问题就是给定一对词像orange和juice，判断这两个词究竟是分别在文本和字典中随机选取得到的，还是通过对靠近两个词采样获得的，这个算法就是要分辨这两种不同的采样方式。</p>
<p>*选取K：</p>
<p>K次就是确定了上下文词后，在字典中抽取K次随机的词</p>
<p>如何选取K？如果是小数据集，5-20比较好，如果数据集很大，K2-5比较好。</p>
<h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p><img src="/home/alex/图片/shit.png" alt="upload successfl"></p>
<p>学习从x映射到y的监督学习模型，这个(上图编号 2 所示)将是新的输入x,这个(上图编号 3 所示)将是你要预测的值y。</p>
<p>为了定义模型，将记号c表示上下文词，记号t表示可能的目标词，再用y表示0和1，表示是否是一对上下文-目标词，要做的就是定义一个逻辑回归模型，给定输入的c,t对的条件下，y=1的概率，即:</p>
<p><img src="/home/alex/图片/2018-209.png" alt="filename alrdy exists, renamed"></p>
<p>这个模型基于逻辑回归模型，不同的是我们把一个sigmoid函数作用于θ_t^Te_c,θ_t表示目标词对应的参数向量，而e_c对应上下文词的嵌入向量，如果有K个样本，可以把它看做1/K的正负样本比例，即每一个正样本你都有K个对应的负样本来训练一个类似逻辑回归的模型。</p>
<p>然后把它看成一个神经网络，如果输入词是orange，即词6257，你要做的事输入one-hot向量，再传递给E，通过两者相乘获得嵌入向量e6257，就得到了10000个可能的逻辑回归分类问题。其中一个将会是用来判断目标词是否是juice的分类器，但并不是每次迭代都训练全部10000个，只训练其中的（K+1）个，这样会减少计算成本，只需要更新K+1个逻辑单元。</p>
<p>说白了负采样就是有一个正样本词orange和juice，然后你会特意去生成一系列负样本，因此叫负采样。每次迭代你选择K个不同的随机的负样本词去训练你的算法。</p>
<p>该怎样选取负样本？</p>
<p>如果均匀且随机抽取负样本，这对于英文单词的分布非常没有代表性，而如果根据其在语料中的经验频率进行采样，会导致在like,the,of,and等词汇上有很高的频率。</p>
<p><img src="/home/alex/图片/2018-210.png" alt="filename already exts, renamed"></p>
<h3 id="Glove-词向量"><a href="#Glove-词向量" class="headerlink" title="Glove 词向量"></a>Glove 词向量</h3><p><img src="/home/alex/图片/2018-212.png" alt="filen already exists, renamed"></p>
<p>我们曾通过挑选语料库中位置相近的两个词，列举出词对，glove算法就是使其关系明确化。<br>假定Xij是单词i在单词j上下文中出现的次数，你也可以遍历你的训练集，然后数出单词i在不同单词j上下文中出现的个数。如果定义上下文和目标词为任意两个位置相近的单词，假设是左右各10个词的距离，那么Xij就是一个能够获取单词i和单词j出现位置相近时候频率的计数器。</p>
<p>glove就是把他们的差距作最小化处理。</p>
<p><img src="/home/alex/图片/2018-213.png" alt="filename already ests, renamed"></p>
<p>f(Xij)是一个加权项，如果Xij为0的话，f(Xij)也为0，防止出现log0的情况！</p>
<p>另一个作用是有的词在英语里出现十分频繁，比如this,is,of,a等等，这叫做停用词。但也有些不常用的词，比如durion，想把它考虑在内但是又不像那些常用词那样频繁，因此加权因子f(Xij)就可以是一个函数，即使是像durion这样不常用的词也能给予大量有意义的运算。</p>
<h3 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h3><p><img src="/home/alex/图片/2018-214.png" alt="filename lready exists, renamed"></p>
<h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>把这个句子里的所有单词对应的嵌入向量求和或者取平均，得到一个平均值计算单元（上图编号3），把这个特征向量送入softmax分类器，然后输出y。</p>
<p>但是这种方法没有考虑词序，例如Completely lacking in good taste,good service and good ambiance.</p>
<p>good出现了很多次，分类器会有可能认为这是一个很好的评论，实际这是一个差评。</p>
<h4 id="方法二：RNN"><a href="#方法二：RNN" class="headerlink" title="方法二：RNN"></a>方法二：RNN</h4><p><img src="/home/alex/图片/many.png" alt="upload succesful"></p>
<p>用一个多对一的RNN，考虑词的顺序效果。</p>
<p>由于你的词嵌入是在一个更大的数据集里训练的，这样的效果会更好，更好的泛化一些没有见过的新单词。</p>
<h3 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h3><p><a href="https://github.com/rohit12sharma/Operations-on-word-vectors/blob/master/Operations%2Bon%2Bword%2Bvectors%2B-%2Bv2.ipynb" target="_blank" rel="noopener">https://github.com/rohit12sharma/Operations-on-word-vectors/blob/master/Operations%2Bon%2Bword%2Bvectors%2B-%2Bv2.ipynb</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/01/11/DL-C5w1HW1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/11/DL-C5w1HW1/" itemprop="url">DL-C5w1HW1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-11T13:46:03+08:00">
                2019-01-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Overview-of-the-model"><a href="#Overview-of-the-model" class="headerlink" title="Overview of the model"></a>Overview of the model</h3><p>创建一个基于字符的语言模型</p>
<p><img src="/home/alex/图片/2018-185.png" alt="filename already exists, renamed"></p>
<p>在每一个时间步，RNN基于先前的字符去预测下一个字符是什么，dataset X = {x1,x2,….xT}是输入，而输出Y={y1,y2…yT}使得y^T = x^(T+1)</p>
<h4 id="大概步骤"><a href="#大概步骤" class="headerlink" title="大概步骤"></a>大概步骤</h4><pre><code>Initialize parameters
Run the optimization loop
    Forward propagation to compute the loss function
    Backward propagation to compute the gradients with respect to the loss function
    Clip the gradients to avoid exploding gradients
    Using the gradients, update your parameter with the gradient descent update rule.
Return the learned parameters
</code></pre><h4 id="clip"><a href="#clip" class="headerlink" title="clip"></a>clip</h4><p>防止梯度爆炸或者弥散，让所有参数的梯度都限定在一个范围内，如某一个值如果大于maxValue，则这个值设置为maxValue</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">def clip(gradients, maxValue):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    Clips the gradients&apos; values between minimum and maximum.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    gradients -- a dictionary containing the gradients &quot;dWaa&quot;, &quot;dWax&quot;, &quot;dWya&quot;, &quot;db&quot;, &quot;dby&quot;</span><br><span class="line">    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    gradients -- a dictionary with the clipped gradients.</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    </span><br><span class="line">    dWaa, dWax, dWya, db, dby = gradients[&apos;dWaa&apos;], gradients[&apos;dWax&apos;], gradients[&apos;dWya&apos;], gradients[&apos;db&apos;], gradients[&apos;dby&apos;]</span><br><span class="line">   </span><br><span class="line">    for gradient in [dWax, dWaa, dWya, db, dby]:</span><br><span class="line">        np.clip(gradient,-maxValue,maxValue,out=gradient)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dWaa&quot;: dWaa, &quot;dWax&quot;: dWax, &quot;dWya&quot;: dWya, &quot;db&quot;: db, &quot;dby&quot;: dby&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure>
<h4 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h4><p><img src="/home/alex/图片/sample.png" alt="upload sucessful"></p>
<ul>
<li><p>step1 设置好第0层激活层和第一个输入字符为0向量</p>
</li>
<li><p>forward propagate</p>
</li>
</ul>
<p><img src="/home/alex/图片/disap.png" alt="upload succesful"></p>
<p>生成一个概率向量，就是这个向量所有数加起来是1，每个数代表着某个字符出现的概率。</p>
<ul>
<li><p>采样，根据生成的概率向量，选择下一个生成的字符</p>
</li>
<li><p>替换x^t的值为x^(t+1)，然后在forward propagate中继续重复这个过程，直到遇到”\n”字符。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">def sample(parameters, char_to_ix, seed):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Sample a sequence of characters according to a sequence of probability distributions output of the RNN</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. </span><br><span class="line">    char_to_ix -- python dictionary mapping each character to an index.</span><br><span class="line">    seed -- used for grading purposes. Do not worry about it.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    indices -- a list of length n containing the indices of the sampled characters.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Retrieve parameters and relevant shapes from &quot;parameters&quot; dictionary</span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[&apos;Waa&apos;], parameters[&apos;Wax&apos;], parameters[&apos;Wya&apos;], parameters[&apos;by&apos;], parameters[&apos;b&apos;]</span><br><span class="line">    vocab_size = by.shape[0]</span><br><span class="line">    n_a = Waa.shape[1]</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)</span><br><span class="line">    x = np.zeros((vocab_size, 1))</span><br><span class="line">    # Step 1&apos;: Initialize a_prev as zeros (≈1 line)</span><br><span class="line">    a_prev = np.zeros((n_a, 1))</span><br><span class="line">    </span><br><span class="line">    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)</span><br><span class="line">    indices = []</span><br><span class="line">    </span><br><span class="line">    # Idx is a flag to detect a newline character, we initialize it to -1</span><br><span class="line">    idx = -1 </span><br><span class="line">    </span><br><span class="line">    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append </span><br><span class="line">    # its index to &quot;indices&quot;. We&apos;ll stop if we reach 50 characters (which should be very unlikely with a well </span><br><span class="line">    # trained model), which helps debugging and prevents entering an infinite loop. </span><br><span class="line">    counter = 0</span><br><span class="line">    newline_character = char_to_ix[&apos;\n&apos;]</span><br><span class="line">    </span><br><span class="line">    while (idx != newline_character and counter != 50):</span><br><span class="line">        </span><br><span class="line">        # Step 2: Forward propagate x using the equations (1), (2) and (3)</span><br><span class="line">        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)</span><br><span class="line">        z = np.dot(Wya, a) + by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        </span><br><span class="line">        # for grading purposes</span><br><span class="line">        np.random.seed(counter+seed) </span><br><span class="line">        </span><br><span class="line">        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y</span><br><span class="line">        idx = np.random.choice(list(range(vocab_size)), p = y.ravel())</span><br><span class="line"></span><br><span class="line">        # Append the index to &quot;indices&quot;</span><br><span class="line">        indices.append(idx)</span><br><span class="line">        </span><br><span class="line">        # Step 4: Overwrite the input character as the one corresponding to the sampled index.</span><br><span class="line">        x = np.zeros((vocab_size, 1))</span><br><span class="line">        x[idx] = 1</span><br><span class="line">        </span><br><span class="line">        # Update &quot;a_prev&quot; to be &quot;a&quot;</span><br><span class="line">        a_prev = a</span><br><span class="line">        </span><br><span class="line">        # for grading purposes</span><br><span class="line">        seed += 1</span><br><span class="line">        counter +=1</span><br><span class="line">        </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    if (counter == 50):</span><br><span class="line">        indices.append(char_to_ix[&apos;\n&apos;])</span><br><span class="line">    </span><br><span class="line">    return indices</span><br></pre></td></tr></table></figure>
<h4 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Execute one step of the optimization to train the model.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.</span><br><span class="line">    Y -- list of integers, exactly the same as X but shifted one index to the left.</span><br><span class="line">    a_prev -- previous hidden state.</span><br><span class="line">    parameters -- python dictionary containing:</span><br><span class="line">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span><br><span class="line">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span><br><span class="line">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span><br><span class="line">                        b --  Bias, numpy array of shape (n_a, 1)</span><br><span class="line">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span><br><span class="line">    learning_rate -- learning rate for the model.</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    loss -- value of the loss function (cross-entropy)</span><br><span class="line">    gradients -- python dictionary containing:</span><br><span class="line">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span><br><span class="line">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span><br><span class="line">                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)</span><br><span class="line">                        db -- Gradients of bias vector, of shape (n_a, 1)</span><br><span class="line">                        dby -- Gradients of output bias vector, of shape (n_y, 1)</span><br><span class="line">    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Forward propagate through time (≈1 line)</span><br><span class="line">    loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line">    </span><br><span class="line">    # Backpropagate through time (≈1 line)</span><br><span class="line">    gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line">    </span><br><span class="line">    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)</span><br><span class="line">    gradients = clip(gradients, maxValue=5)</span><br><span class="line">    </span><br><span class="line">    # Update parameters (≈1 line)</span><br><span class="line">    parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line">    </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return loss, gradients, a[len(X)-1]</span><br></pre></td></tr></table></figure>
<h4 id="model"><a href="#model" class="headerlink" title="model"></a>model</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># GRADED FUNCTION: model</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def model(data, ix_to_char, char_to_ix, num_iterations = 50000, n_a = 50, dino_names = 7, vocab_size = 27):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Trains the model and generates dinosaur names. </span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    data -- text corpus</span><br><span class="line">    ix_to_char -- dictionary that maps the index to a character</span><br><span class="line">    char_to_ix -- dictionary that maps a character to an index</span><br><span class="line">    num_iterations -- number of iterations to train the model for</span><br><span class="line">    n_a -- number of units of the RNN cell</span><br><span class="line">    dino_names -- number of dinosaur names you want to sample at each iteration. </span><br><span class="line">    vocab_size -- number of unique characters found in the text, size of the vocabulary</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- learned parameters</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Retrieve n_x and n_y from vocab_size</span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters</span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    # Initialize loss (this is required because we want to smooth our loss, don&apos;t worry about it)</span><br><span class="line">    loss = get_initial_loss(vocab_size, dino_names)</span><br><span class="line">    </span><br><span class="line">    # Build list of all dinosaur names (training examples).</span><br><span class="line">    with open(&quot;names.txt&quot;) as f:</span><br><span class="line">        examples = f.readlines()</span><br><span class="line">    examples = [x.lower().strip() for x in examples]</span><br><span class="line">    </span><br><span class="line">    # Shuffle list of all dinosaur names</span><br><span class="line">    np.random.seed(0)</span><br><span class="line">    np.random.shuffle(examples)</span><br><span class="line">    </span><br><span class="line">    # Initialize the hidden state of your LSTM</span><br><span class="line">    a_prev = np.zeros((n_a, 1))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    lossList = []</span><br><span class="line">    # Optimization loop</span><br><span class="line">    for j in range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        ### START CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Use the hint above to define one training example (X,Y) (≈ 2 lines)</span><br><span class="line">        index = j % len(examples)</span><br><span class="line">        X = [None] + [char_to_ix[ch] for ch in examples[index]] </span><br><span class="line">        Y = X[1:] + [char_to_ix[&quot;\n&quot;]]</span><br><span class="line">        </span><br><span class="line">        # Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters</span><br><span class="line">        # Choose a learning rate of 0.01</span><br><span class="line">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)</span><br><span class="line">        </span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.</span><br><span class="line">        loss = smooth(loss, curr_loss)</span><br><span class="line">        lossList.append(loss)</span><br><span class="line">        # Every 2000 Iteration, generate &quot;n&quot; characters thanks to sample() to check if the model is learning properly</span><br><span class="line">        if j % 2000 == 0:</span><br><span class="line">            </span><br><span class="line">            print(&apos;Iteration: %d, Loss: %f&apos; % (j, loss) + &apos;\n&apos;)</span><br><span class="line">            </span><br><span class="line">            # The number of dinosaur names to print</span><br><span class="line">            seed = 0</span><br><span class="line">            for name in range(dino_names):</span><br><span class="line">                </span><br><span class="line">                # Sample indices and print them</span><br><span class="line">                sampled_indices = sample(parameters, char_to_ix, seed)</span><br><span class="line">                print_sample(sampled_indices, ix_to_char)</span><br><span class="line">                </span><br><span class="line">                seed += 1  # To get the same result for grading purposed, increment the seed by one. </span><br><span class="line">      </span><br><span class="line">            print(&apos;\n&apos;)</span><br><span class="line">                </span><br><span class="line">    plt.plot(range(num_iterations),lossList)</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>
<p>训练数据是88000个英文last name</p>
<p><img src="/home/alex/图片/2018-186.png" alt="filename aeady exists, renamed"></p>
<hr>
<h3 id="手把手搭建RNN"><a href="#手把手搭建RNN" class="headerlink" title="手把手搭建RNN"></a>手把手搭建RNN</h3><h4 id="RNN-cell"><a href="#RNN-cell" class="headerlink" title="RNN cell"></a>RNN cell</h4><p>一个RNN可以看做是一个cell的重复</p>
<pre><code>1.Compute the hidden state with tanh activation: a⟨t⟩=tanh(Waaa⟨t−1⟩+Waxx⟨t⟩+ba).
2.Using your new hidden state a⟨t⟩, compute the prediction ŷ ⟨t⟩=softmax(Wyaa⟨t⟩+by). We provided you a function: softmax.
3.Store (a⟨t⟩,a⟨t−1⟩,x⟨t⟩,parameters) in cache
4.Return a⟨t⟩ , y⟨t⟩  and cache
</code></pre> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line">def rnn_cell_forward(xt, a_prev, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements a single forward step of the RNN-cell as described in Figure (2)</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span><br><span class="line">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span><br><span class="line">    parameters -- python dictionary containing:</span><br><span class="line">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span><br><span class="line">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span><br><span class="line">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span><br><span class="line">                        ba --  Bias, numpy array of shape (n_a, 1)</span><br><span class="line">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span><br><span class="line">    Returns:</span><br><span class="line">    a_next -- next hidden state, of shape (n_a, m)</span><br><span class="line">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span><br><span class="line">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Retrieve parameters from &quot;parameters&quot;</span><br><span class="line">    Wax = parameters[&quot;Wax&quot;]</span><br><span class="line">    Waa = parameters[&quot;Waa&quot;]</span><br><span class="line">    Wya = parameters[&quot;Wya&quot;]</span><br><span class="line">    ba = parameters[&quot;ba&quot;]</span><br><span class="line">    by = parameters[&quot;by&quot;]</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (≈2 lines)</span><br><span class="line">    # compute next activation state using the formula given above</span><br><span class="line">    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)</span><br><span class="line">    # compute output of the current cell using the formula given above</span><br><span class="line">    yt_pred = softmax(np.dot(Wya, a_next) + by)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # store values you need for backward propagation in cache</span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    return a_next, yt_pred, cache</span><br></pre></td></tr></table></figure>
<h4 id="RNN-forward"><a href="#RNN-forward" class="headerlink" title="RNN forward"></a>RNN forward</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def rnn_forward(x, a0, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span><br><span class="line">    a0 -- Initial hidden state, of shape (n_a, m)</span><br><span class="line">    parameters -- python dictionary containing:</span><br><span class="line">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span><br><span class="line">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span><br><span class="line">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span><br><span class="line">                        ba --  Bias numpy array of shape (n_a, 1)</span><br><span class="line">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span><br><span class="line">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span><br><span class="line">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Initialize &quot;caches&quot; which will contain the list of all caches</span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    # Retrieve dimensions from shapes of x and Wy</span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[&quot;Wya&quot;].shape</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # initialize &quot;a&quot; and &quot;y&quot; with zeros (≈2 lines)</span><br><span class="line">    a = np.zeros((n_a,m,T_x))</span><br><span class="line">    y_pred= np.zeros((n_y,m,T_x))</span><br><span class="line">    # Initialize a_next (≈1 line)</span><br><span class="line">    a_next = a0</span><br><span class="line">    </span><br><span class="line">    # loop over all time-steps</span><br><span class="line">    for t in range(T_x):</span><br><span class="line">        # Update next hidden state, compute the prediction, get the cache (≈1 line)</span><br><span class="line">        a_next,yt_pred,cache = rnn_cell_forward(x[:,:,t], a_next, parameters)</span><br><span class="line">        # Save the value of the new &quot;next&quot; hidden state in a (≈1 line)</span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        # Save the value of the prediction in y (≈1 line)</span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        # Append &quot;cache&quot; to &quot;caches&quot; (≈1 line)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # store values needed for backward propagation in cache</span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    return a, y_pred, caches</span><br></pre></td></tr></table></figure>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>遗忘门</p>
<p>我们假设我们正在阅读一段文本中的单词, 并希望使用 LSTM 来跟踪语法结构, 例如主语是单数还是复数。如果主语从一个单数词变为复数词, 我们需要找到一种方法来去除以前存储的单数/复数状态的内存值。在 LSTM 中, 遗忘门让我们这样做:</p>
<p><img src="/home/alex/图片/2018-187.png" alt="filename eady exists, renamed"></p>
<p>更新门</p>
<p><img src="/home/alex/图片/2018-188.png" alt="filename aeady exists, renamed"></p>
<p>输出门</p>
<p><img src="/home/alex/图片/2018-189.png" alt="filename ready exists, renamed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: lstm_cell_forward</span><br><span class="line"></span><br><span class="line">def lstm_cell_forward(xt, a_prev, c_prev, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span><br><span class="line">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span><br><span class="line">    c_prev -- Memory state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span><br><span class="line">    parameters -- python dictionary containing:</span><br><span class="line">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bc --  Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span><br><span class="line">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span><br><span class="line">                        </span><br><span class="line">    Returns:</span><br><span class="line">    a_next -- next hidden state, of shape (n_a, m)</span><br><span class="line">    c_next -- next memory state, of shape (n_a, m)</span><br><span class="line">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span><br><span class="line">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span><br><span class="line">          c stands for the memory value</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # Retrieve parameters from &quot;parameters&quot;</span><br><span class="line">    Wf = parameters[&quot;Wf&quot;]</span><br><span class="line">    bf = parameters[&quot;bf&quot;]</span><br><span class="line">    Wi = parameters[&quot;Wi&quot;]</span><br><span class="line">    bi = parameters[&quot;bi&quot;]</span><br><span class="line">    Wc = parameters[&quot;Wc&quot;]</span><br><span class="line">    bc = parameters[&quot;bc&quot;]</span><br><span class="line">    Wo = parameters[&quot;Wo&quot;]</span><br><span class="line">    bo = parameters[&quot;bo&quot;]</span><br><span class="line">    Wy = parameters[&quot;Wy&quot;]</span><br><span class="line">    by = parameters[&quot;by&quot;]</span><br><span class="line">    </span><br><span class="line">    # Retrieve dimensions from shapes of xt and Wy</span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # Concatenate a_prev and xt (≈3 lines)</span><br><span class="line">    concat = np.zeros((n_a+n_x,m))</span><br><span class="line">    concat[:n_a,:] = a_prev</span><br><span class="line">    concat[n_a:,:] = xt</span><br><span class="line">    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span><br><span class="line">    ft = sigmoid(np.dot(Wf,concat)+bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi,concat)+bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc,concat)+bc)</span><br><span class="line">    c_next = ft*c_prev+it*cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo,concat)+bo)</span><br><span class="line">    a_next = ot*np.tanh(c_next)</span><br><span class="line">    # Compute prediction of the LSTM cell (≈1 line)</span><br><span class="line">    yt_pred = softmax(np.dot(Wy,a_next)+by)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    # store values needed for backward propagation in cache</span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    return a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">def lstm_forward(x, a0, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span><br><span class="line">    a0 -- Initial hidden state, of shape (n_a, m)</span><br><span class="line">    parameters -- python dictionary containing:</span><br><span class="line">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bc -- Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span><br><span class="line">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span><br><span class="line">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span><br><span class="line">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span><br><span class="line">                        </span><br><span class="line">    Returns:</span><br><span class="line">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span><br><span class="line">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span><br><span class="line">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # Initialize &quot;caches&quot;, which will track the list of all the caches</span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # Retrieve dimensions from shapes of x and Wy (≈2 lines)</span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[&quot;Wy&quot;].shape</span><br><span class="line">    </span><br><span class="line">    # initialize &quot;a&quot;, &quot;c&quot; and &quot;y&quot; with zeros (≈3 lines)</span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = a</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    # Initialize a_next and c_next (≈2 lines)</span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros(a_next.shape)</span><br><span class="line">    </span><br><span class="line">    # loop over all time-steps</span><br><span class="line">    for t in range(T_x):</span><br><span class="line">        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)</span><br><span class="line">        # Save the value of the new &quot;next&quot; hidden state in a (≈1 line)</span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        # Save the value of the prediction in y (≈1 line)</span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        # Save the value of the next cell state (≈1 line)</span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        # Append the cache into caches (≈1 line)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # store values needed for backward propagation in cache</span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    return a, y, c, caches</span><br></pre></td></tr></table></figure>
<h4 id="RNN-backward-pass"><a href="#RNN-backward-pass" class="headerlink" title="RNN backward pass"></a>RNN backward pass</h4><p><img src="/home/alex/图片/love.png" alt="upload succul"></p>
<h3 id="LSTM-music-generation"><a href="#LSTM-music-generation" class="headerlink" title="LSTM music generation"></a>LSTM music generation</h3><p><img src="/home/alex/图片/2018-190.png" alt="filename exists, renamed"></p>
<p>从一个更长的序列中随机选取30个数值片段来训练模型，因此不会设置x(1) 为 零向量。 设置每个片段都有相同长度Tx=30，使得矢量化更容易。</p>
<h4 id="building-model"><a href="#building-model" class="headerlink" title="building model"></a>building model</h4><p>对于序列生成，测试的时候不能提前知道所有的x^t数值，而使用x^t = y^(t-1)一次生成一个，因此要实现for循环来访问不同的时间步，函数djmodel()将使用for循环调用LSTM层TX T次，并且每次第Tx步都应该有共享权重，而不应该重新初始化权重。</p>
<p>在Keras中实现具有共享权重层的关键步骤</p>
<p>1.定义层对象</p>
<p>2.前向传播输入时候调用这些对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_a = 64 </span><br><span class="line">reshapor = Reshape((1, n_values))</span><br><span class="line">LSTM_cell = LSTM(n_a, return_state = True)</span><br><span class="line">Densor = Dense(n_values, activation=&apos;softmax&apos;)</span><br></pre></td></tr></table></figure>
<p><img src="/home/alex/图片/2018-191.png" alt="filename exists, renamed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def create_model(Tx, n_a, n_values):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the model</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    Tx -- length of the sequence in a corpus</span><br><span class="line">    n_a -- the number of activations used in our model</span><br><span class="line">    n_values -- number of unique values in the music data </span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    model -- a keras model with the </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Define the input of your model with a shape </span><br><span class="line">    X = Input(shape=(Tx, n_values))</span><br><span class="line">    </span><br><span class="line">    # Define s0, initial hidden state for the decoder LSTM</span><br><span class="line">    a0 = Input(shape=(n_a,), name=&apos;a0&apos;)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=&apos;c0&apos;)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line">    </span><br><span class="line">    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)</span><br><span class="line">    output = []</span><br><span class="line">    </span><br><span class="line">    # Step 2: Loop</span><br><span class="line">    for t in range(Tx):</span><br><span class="line">        </span><br><span class="line">        # Step 2.A: select the &quot;t&quot;th time step vector from X. </span><br><span class="line">        x = Lambda(lambda x:X[:,t,:])(X)</span><br><span class="line">        # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)</span><br><span class="line">        x = reshapor(x)</span><br><span class="line">        # Step 2.C: Perform one step of the LSTM_cell</span><br><span class="line">        a,_,c = LSTM_cell(x,initial_state = [a,c])</span><br><span class="line">        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell</span><br><span class="line">        out = Densor(a)</span><br><span class="line">        # Step 2.E: add the output to &quot;outputs&quot;</span><br><span class="line">        output.append(out)</span><br><span class="line">    # Step 3: Create model instance</span><br><span class="line">    model = Model([X,a0,c0],output)</span><br><span class="line">    </span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = create_model(Tx = 30 , n_a = n_a, n_values = n_values)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opt = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, decay=0.003)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=opt, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = 60</span><br><span class="line">a0 = np.zeros((m,n_a))</span><br><span class="line">c0 = np.zeros((m,n_a))</span><br><span class="line">model.fit([X, a0, c0], list(Y), epochs=500)</span><br></pre></td></tr></table></figure>
<h3 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h3><p><img src="/home/alex/图片/2018-192.png" alt="filename exists, renamed"></p>
<p>用上个cell预测的值来作为下个cell的输入值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def music_inference_model(LSTM_cell, densor, n_values = n_values, n_a = 64, Ty = 100):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Uses the trained &quot;LSTM_cell&quot; and &quot;densor&quot; from model() to generate a sequence of values.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    LSTM_cell -- the trained &quot;LSTM_cell&quot; from model(), Keras layer object</span><br><span class="line">    densor -- the trained &quot;densor&quot; from model(), Keras layer object</span><br><span class="line">    n_values -- integer, umber of unique values</span><br><span class="line">    n_a -- number of units in the LSTM_cell</span><br><span class="line">    Ty -- integer, number of time steps to generate</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    inference_model -- Keras model instance</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Define the input of your model with a shape </span><br><span class="line">    x0 = Input(shape=(1, n_values))</span><br><span class="line">    </span><br><span class="line">    # Define s0, initial hidden state for the decoder LSTM</span><br><span class="line">    a0 = Input(shape=(n_a,), name=&apos;a0&apos;)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=&apos;c0&apos;)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line">    x = x0</span><br><span class="line"></span><br><span class="line">    # Step 1: Create an empty list of &quot;outputs&quot; to later store your predicted values (≈1 line)</span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    # Step 2: Loop over Ty and generate a value at every time step</span><br><span class="line">    for t in range(Ty):</span><br><span class="line">        </span><br><span class="line">        # Step 2.A: Perform one step of LSTM_cell (≈1 line)</span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        </span><br><span class="line">        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)</span><br><span class="line">        out = densor(a)</span><br><span class="line"></span><br><span class="line">        # Step 2.C: Append the prediction &quot;out&quot; to &quot;outputs&quot;. out.shape = (None, 78) (≈1 line)</span><br><span class="line">        outputs.append(out)</span><br><span class="line">        </span><br><span class="line">        # Step 2.D: Select the next value according to &quot;out&quot;, and set &quot;x&quot; to be the one-hot representation of the</span><br><span class="line">        #           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided </span><br><span class="line">        #           the line of code you need to do this. </span><br><span class="line">        x = Lambda(one_hot)(out)</span><br><span class="line">        </span><br><span class="line">    # Step 3: Create model instance with the correct &quot;inputs&quot; and &quot;outputs&quot; (≈1 line)</span><br><span class="line">    inference_model = Model(inputs = [x0, a0, c0], outputs = outputs)</span><br><span class="line">    </span><br><span class="line">    return inference_model</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/01/03/DL-C5W1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/03/DL-C5W1/" itemprop="url">DL-C5W1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-03T14:47:59+08:00">
                2019-01-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>循环神经网络是从左向右扫描数据,同时每个时间步的参数也是共享的,所以下页幻灯<br>片中我们会详细讲述它的一套参数,我们用W ax 来表示管理着从x <1> 到隐藏层的连接的一系列参数,每个时间步使用的都是相同的参数W ax 。而激活值也就是水平联系是由参数W aa 决<br>定的,同时每一个时间步都使用相同的参数W aa ,同样的输出结果由W ya 决定。下图详细讲述<br>这些参数是如何起作用。</1></p>
<p><img src="/home/alex/图片/letitgo.png" alt="upad successful"></p>
<p>意思是说当预测y^[3]的时候不仅仅需要输入的x^[3]信息，还需要输入的x^[1],x^[2]信息。</p>
<h4 id="forward-propagation"><a href="#forward-propagation" class="headerlink" title="forward propagation"></a>forward propagation</h4><p><img src="/home/alex/图片/thisone.png" alt="upload sful"></p>
<h4 id="simplification-notation"><a href="#simplification-notation" class="headerlink" title="simplification notation"></a>simplification notation</h4><p><img src="/home/alex/图片/notation.png" alt="upload succesul"></p>
<p><img src="/home/alex/图片/2018-169.png" alt="filename already existsenamed"></p>
<p><img src="/home/alex/图片/2018-170.png" alt="filename already exists, enamed"></p>
<h4 id="backward-propagation"><a href="#backward-propagation" class="headerlink" title="backward propagation"></a>backward propagation</h4><p><img src="/home/alex/图片/fanxiangchuanbo.png" alt="upload succeul"></p>
<p>求导示意图：</p>
<p><img src="/home/alex/图片/求导.png" alt="upload succesul"></p>
<h4 id="RNN种类"><a href="#RNN种类" class="headerlink" title="RNN种类"></a>RNN种类</h4><p><img src="/home/alex/图片/2018-171.png" alt="filename lready exists, renamed"></p>
<ol>
<li><p>1 to 1 </p>
</li>
<li><p>1 to many 音乐生成、序列生成</p>
</li>
<li><p>many to 1 情感判断</p>
</li>
<li><p>many to many 如 name entity recognition</p>
</li>
<li><p>many to many 如 翻译，例如输入的x是中文，输出的y是英文</p>
</li>
</ol>
<h3 id="训练一个语言模型"><a href="#训练一个语言模型" class="headerlink" title="训练一个语言模型"></a>训练一个语言模型</h3><h4 id="1"><a href="#1" class="headerlink" title="1"></a>1</h4><p>建立一个字典，把输入句子每个单词转化为对应的one-hot向量。</p>
<p>有时候要在句子末尾添加一个EOS标记，表示句子的结束。未知的词用UNK来代替。</p>
<h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><p><img src="/home/alex/图片/RNN model.png" alt="uplo successful"></p>
<p>建立RNN模型，x^[1]会被设为一个0向量，在之前的a^[0]也会被设为一个0向量，于是a^[1]要做的是通过softmax进行一些预测来计算第一个词可能会是什么，结果就是y^[1]，这一步就是通过一个softmax层来预测字典中任意单词会是第一个词的概率，输出是softmax的计算结果，结果个数就是字典的词的个数。</p>
<p><img src="/home/alex/图片/2018-172.png" alt="filename already exists, renamed"></p>
<p>然后预测第二个词：是在考虑预测了第一个词的基础上预测到第二个词的概率。</p>
<p>如此类推，这是一个全概率公式。</p>
<p>把这三个概率相乘，含义就是最后得到这个含3个词的整个句子的概率。</p>
<h3 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h3><p>训练了一个序列模型之后，想要了解这个模型学到了什么，一种非正式方法就是进行一次新序列采样。</p>
<p><img src="/home/alex/图片/2018-173.png" alt="fileme already exists, renamed"></p>
<p>就是如何从RNN语言模型中生成一个随机选择的句子：</p>
<p>第一步要做的就是对你想要模型生成的第一个词进行采样,于是你输入x <1> = 0,<br>a <0> = 0,现在你的第一个时间步得到的是所有可能的输出是经过 softmax 层后得到的概率,<br>然后根据这个 softmax 的分布进行随机采样。Softmax 分布给你的信息就是第一个词 a 的概<br>率是多少,第一个词是 aaron 的概率是多少,第一个词是 zulu 的概率是多少,还有第一个词<br>是 UNK(未知标识)的概率是多少,这个标识可能代表句子的结尾,然后对这个向量使用例<br>如 numpy 命令, np.random.choice (上图编号 3 所示),来根据向量中这些概率的分布<br>进行采样,这样就能对第一个词进行采样了。</0></1></p>
<p>然后再到下一个时间步,无论你得到什么样的用 one-hot 码表示的选择结果,都把它传<br>递到下一个时间步,然后对第三个词进行采样。不管得到什么都把它传递下去,一直这样直<br>到最后一个时间步。</p>
<p>这就得到了一个随机生成的句子。</p>
<h3 id="基于字符的语言生成模型"><a href="#基于字符的语言生成模型" class="headerlink" title="基于字符的语言生成模型"></a>基于字符的语言生成模型</h3><p>对于英文来说，字典仅包含26个英文字母大小写，标点符号等等。</p>
<p>优点：不必担心出现未知的的标识。</p>
<p>缺点：不能像基于词汇的模型，可以捕捉长范围上下文的关系，而且计算成本会很高。</p>
<h3 id="RNN的问题"><a href="#RNN的问题" class="headerlink" title="RNN的问题"></a>RNN的问题</h3><p>1.因为梯度消失，不擅长处理长期依赖的问题。</p>
<p><img src="/home/alex/图片/2018-174.png" alt="filename lready exists, renamed"></p>
<p>对于 RNN,首先从左到右前向传播,然后反向传播。但是反向传播会很困难,因为同样的梯度消失的问题,后面层的输出误差很难影响前面层的计算，实际上很难让一个神经网络能够意识到他要记住看到的是单数名词还是复数名词。</p>
<p>2.对于梯度爆炸问题则比较好解决。</p>
<p>一个方法是 gradient clipping,意思是观察你的梯度向量，如果它大于某个阈值，则缩放梯度向量，保证他不会太大。</p>
<h3 id="GRU-（gated-recurrent-unit）"><a href="#GRU-（gated-recurrent-unit）" class="headerlink" title="GRU （gated recurrent unit）"></a>GRU （gated recurrent unit）</h3><p>门控循环单元</p>
<p><img src="/home/alex/图片/2018-175.png" alt="fename already exists, renamed"></p>
<p>GRU 单元将会有个新的变量称为c,代表细胞(cell),即记忆细胞(下图编号 1 所示)。记忆细胞<br>的作用是提供了记忆的能力,比如说一只猫是单数还是复数,所以当它看到之后的句子的时<br>候,它仍能够判断句子的主语是单数还是复数。于是在时间t处,有记忆细胞c <t> ,然后我<br>们看的是,GRU 实际上输出了激活值a <t> ,c <t> = a <t> (下图编号 2 所示)。于是我们想<br>要使用不同的符号c和a来表示记忆细胞的值和输出的激活值,即使它们是一样的。我现在使<br>用这个标记是因为当我们等会说到 LSTMs 的时候,这两个会是不同的值,但是现在对于 GRU,c <t> 的值等于a <t> 的激活值。</t></t></t></t></t></t></p>
<p>所以这些等式表示了 GRU 单元的计算,在每个时间步,我们将用一个候选值重写记忆<br>细胞,即c̃ <t> 的值,所以它就是个候选值,替代了c <t> 的值。然后我们用 tanh 激活函数来<br>计算, c̃ <t> = tanh(W c [c &lt;t−1&gt; , x <t> ] + b c ),所以c̃ <t> 的值就是个替代值,代替表示c <t> 的值<br>(下图编号 3 所示)。</t></t></t></t></t></t></p>
<p><img src="/home/alex/图片/GRU.png" alt="uplsuccessful"></p>
<p>所以我们接下来要给 GRU 用的式子就是c <t> = Γ u ∗ c̃ <t> + (1 − Γ u ) ∗ c &lt;t−1&gt; (上图编号<br>1 所示)。你应该注意到了,如果这个更新值Γ u = 1,也就是说把这个新值,即c <t> 设为候<br>选值(Γ u = 1时简化上式,c <t> = c̃ <t> )。将门值设为 1(上图编号 2 所示),然后往前再<br>更新这个值。对于所有在这中间的值,你应该把门的值设为 0,即Γ u = 0,意思就是说不更<br>新它,就用旧的值。因为如果Γ u = 0,则c <t> = c &lt;t−1&gt; ,c <t> 等于旧的值。</t></t></t></t></t></t></t></p>
<h4 id="完整的GRU单元"><a href="#完整的GRU单元" class="headerlink" title="完整的GRU单元"></a>完整的GRU单元</h4><p><img src="/home/alex/图片/kmt.png" alt="uplo successful"></p>
<p>  总结：</p>
<p>  GRU是用于解决RNN深层网络中梯度弥散问题的一种结构，引入gamma门参数用来决定该时间步的激活层是来自于上一层（保留记忆）还是新计算的结果（不保留记忆）</p>
<p><img src="/home/alex/图片/2018-177.png" alt="filename already exists,enamed"></p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>回顾GRU<br><img src="/home/alex/图片/LSTM.png" alt="upload l"></p>
<p><img src="/home/alex/图片/2018-178.png" alt="filename already exists,enamed"></p>
<p>我们像以前那样有一个更新门Γ u 和表示更新的参数W u ,Γ u = σ(W u [a &lt;t−1&gt; , x <t> ] + b u )(上图编号 5 所示)。一个 LSTM 的新特性是不只有一个更新门控制,这里的这两项(上图编号 6,7 所示),我们将用不同的项来代替它们,要用别的项来取代Γ u 和1 − Γ u ,这里(上图编号 6 所示)我们用Γ u 。</t></p>
<p>然后这里(上图编号 7 所示)用<em>遗忘门</em>(the forget gate),我们叫它Γ f ,所以这个Γ f =<br>σ(W f [a &lt;t−1&gt; , x <t> ] + b f )(上图编号 8 所示);</t></p>
<p>然后我们有一个新的输出门,Γ o = σ(W o [a &lt;t−1&gt; , x <t> ]+&gt; b o )(上图编号 9 所示);</t></p>
<p>于是记忆细胞的更新值c <t> = Γ u ∗ c̃ <t> + Γ f ∗ c &lt;t−1&gt; (上图编号 10 所示);<br>所以这给了记忆细胞选择权去维持旧的值c &lt;t−1&gt; 或者就加上新的值c̃ <t> ,所以这里用了<br>单独的更新门Γ u 和遗忘门Γ f ,然后这个表示更新门(Γ u = σ(W u [a &lt;t−1&gt; , x <t> ] + b u )上图编号 5 所示);</t></t></t></t></p>
<p>遗忘门(Γ f = σ(W f [a &lt;t−1&gt; , x <t> ] + b f )上图编号 8 所示)和输出门(上图编号 9 所示)。</t></p>
<p>最后a <t> = c <t> 的式子会变成a <t> = Γ o ∗ c <t> 。</t></t></t></t></p>
<h4 id="peephole-connection"><a href="#peephole-connection" class="headerlink" title="peephole connection"></a>peephole connection</h4><p>门值不仅取决于a &lt;t−1&gt; 和x <t> ,有时候也可以偷窥一下c &lt;t−1&gt; 的值(上图编号 13 所示),<br>这叫做“窥视孔连接”(peephole connection)。虽然不是个好听的名字,但是你想,“偷窥孔<br>连接”其实意思就是门值不仅取决于a &lt;t−1&gt; 和x <t> ,也取决于上一个记忆细胞的值(c &lt;t−1&gt; ),<br>然后“偷窥孔连接”就可以结合这三个门(Γ u 、Γ f 、Γ o )来计算了</t></t></p>
<p><img src="/home/alex/图片/jiuming.png" alt="upload succesul"></p>
<p><img src="/home/alex/图片/qiupiandao.png" alt="uploaduccessful"></p>
<p>详情可见 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<h4 id="forward-propagate"><a href="#forward-propagate" class="headerlink" title="forward propagate"></a>forward propagate</h4><p><img src="/home/alex/图片/2018-179.png" alt="filename already ists, renamed"></p>
<p><img src="/home/alex/图片/2018-180.png" alt="filename aready exists, renamed"></p>
<p><img src="/home/alex/图片/2018-181.png" alt="filename aleady exists, renamed"></p>
<p><img src="/home/alex/图片/2018-182.png" alt="filename aeady exists, renamed"></p>
<p>这里的i_t就是update gate,f_t就是forget gate</p>
<h3 id="BRNN"><a href="#BRNN" class="headerlink" title="BRNN"></a>BRNN</h3><p><img src="/home/alex/图片/2018-183.png" alt="filenamready exists, renamed"></p>
<p><img src="/home/alex/图片/2018-184.png" alt="filename already exists,enamed"></p>
<h3 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h3><p><img src="/home/alex/图片/ssssz.png" alt="upload succsful"></p>
<p>不再用原来的a <0> 表示 0 时刻的激活值了,而是用a [1]<0> 来表示第一层<br>(上图编号 4 所示),所以我们现在用a [l]<t> 来表示第 l 层的激活值,这个<t>表示第t个时<br>间点,这样就可以表示。第一层第一个时间点的激活值a [1]<1> ,这(a [1]<2> )就是第一层第<br>二个时间点的激活值,a [1]<3> 和a [1]<4> 。然后我们把这些(上图编号 4 方框内所示的部分)<br>堆叠在上面,这就是一个有三个隐层的新的网络。<br>我们看个具体的例子,看看这个值(a [2]<3> ,上图编号 5 所示)是怎么算的。激活值<br>a [2]<3> 有两个输入,一个是从下面过来的输入(上图编号 6 所示),还有一个是从左边过来<br>[2]<br>[2]<br>的输入(上图编号 7 所示),a [2]<3> = g(W a [a [2]<2> , a [1]<3> ] + b a ),这就是这个激活值的计算方法。参数W a 和b a 在这一层的计算里都一样,相对应地第一层也有自己的参数W a[1]<br>和b a 。</3></2></3></3></3></4></3></2></1></t></t></0></0></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2019/01/01/DL-C4W4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/DL-C4W4/" itemprop="url">DL-C4W4</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-01T15:58:22+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h3><h4 id="one-shot-learning"><a href="#one-shot-learning" class="headerlink" title="one shot learning"></a>one shot learning</h4><p><img src="/home/alex/图片/2018-159.png" alt="filename dy exists, renamed"></p>
<p>因为公司的数据库里面很可能只有该名员工一张照片，因此用一张图片投入神经网络，通过softmax输出分类显然不可行。因此应该学习一个相似函数，如果函数的结果大于某个阈值，说明不匹配，否则说明匹配。</p>
<h4 id="siamese-network"><a href="#siamese-network" class="headerlink" title="siamese network"></a>siamese network</h4><p><img src="/home/alex/图片/2018-160.png" alt="filename already enamed"></p>
<p>x1投入网络中，得到全连接层一个output vector，这个vector维度是128x1,记为encoding of x1</p>
<p>另外一张图片x2喂入网络，得到另外一个vector叫做encoding of x2</p>
<p>然后把二者距离定义为二者编码之差的范数。</p>
<p>更准确地说,神经网络的参数定义了一个编码函数f(x (i) ),如果给定输入图像x(i),这个网络会输出x (i) 的 128 维的编码。</p>
<h4 id="三元组损失"><a href="#三元组损失" class="headerlink" title="三元组损失"></a>三元组损失</h4><p><img src="/home/alex/图片/2018-161.png" alt="filenamalready exists, renamed"></p>
<p>目标：</p>
<p><img src="/home/alex/图片/something.png" alt="upload succesul"></p>
<p>遇到的问题：</p>
<p>如果f总是输出0，上面式子无意义</p>
<p>为了阻止网络出现这种情况,我们需要修改这个目标,也就是,这个不能是刚好小于等<br>于 0,应该是比 0 还要小,所以这个应该小于一个−a值(即||f(A) − f(P)|| 2 − ||f(A) −<br>f(N)|| 2 ≤ −a),这里的a是另一个超参数,这个就可以阻止网络输出无用的结果。按照惯<br>例,我们习惯写+a(即||f(A) − f(P)|| 2 − ||f(A) − f(N)|| 2 + a ≤ 0),而不是把−a写在后<br>面,它也叫做间隔(margin)</p>
<p>总结：</p>
<p>三元组损失函数的定义基于三张图片,假如三张图片A、 P、 N,即 anchor 样本、 positive<br>样本和 negative 样本,其中 positive 图片和 anchor 图片是同一个人,但是 negative 图片和<br>anchor 不是同一个人。</p>
<h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L(A, P, N) = max(||f(A) − f(P)|| 2 − ||f(A) − f(N)|| 2 + a, 0)</span><br></pre></td></tr></table></figure>
<p>这是一个三元组定义的损失,整个网络的代价函数应该是训练集中这些单个三元组损失的总和。</p>
<h4 id="挑选数据集"><a href="#挑选数据集" class="headerlink" title="挑选数据集"></a>挑选数据集</h4><p>现在我们来看,你如何选择这些三元组来形成训练集。一个问题是如果你从训练集中,<br>随机地选择A、 P和N,遵守A和P是同一个人,而A和N是不同的人这一原则。有个问题就是,<br>如果随机的选择它们,那么这个约束条件(d(A, P) + a ≤ d(A, N))很容易达到,因为随机<br>选择的图片,A和N比A和P差别很大的概率很大。我希望你还记得这个符号d(A, P)就是前几<br>个幻灯片里写的||f(A) − f(P)|| 2 ,d(A, N)就是||f(A) − f(N)|| 2 ,d(A, P) + a ≤ d(A, N)即<br>||f(A) − f(P)|| 2 + a ≤ ||f(A) − f(N)|| 2 。但是如果A和N是随机选择的不同的人,有很大的<br>可能性||f(A) − f(N)|| 2 会比左边这项||f(A) − f(P)|| 2 大,而且差距远大于a,这样网络并不<br>能从中学到什么。</p>
<p>因此要挑选最难学习的，就是要挑选 d(A,P)约等于 d(A,N)的三元组，<br>只有这样梯度下降法才有用，才能学到有意义的参数。</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p><img src="/home/alex/图片/waisting.png" alt="upload succesl"></p>
<h3 id="人脸验证与二分类"><a href="#人脸验证与二分类" class="headerlink" title="人脸验证与二分类"></a>人脸验证与二分类</h3><p><img src="/home/alex/图片/learning.png" alt="upload ul"></p>
<p><img src="/home/alex/图片/hundred.png" alt="uploadccessful"></p>
<h4 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h4><p><img src="/home/alex/图片/预处理.png" alt="upload successful"></p>
<p>如果这是一张新图片(编号 1),<br>当员工走进门时,希望门可以自动为他们打开,这个(编号 2)是在数据库中的图片,不需<br>要每次都计算这些特征(编号 6),不需要每次都计算这个嵌入,你可以提前计算好,那么<br>当一个新员工走近时,你可以使用上方的卷积网络来计算这些编码(编号 5),然后使用它,<br>^ 。<br>和预先计算好的编码进行比较,然后输出预测值</p>
<h3 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h3><p><img src="/home/alex/图片/2018-162.png" alt="filename aeady exists, renamed"></p>
<p>怎么判断生成图像的好坏呢?我们把这个代价函数定义为两个部分。<br>J content (C, G)<br>第一部分被称作内容代价,这是一个关于内容图片和生成图片的函数,它是用来度量生<br>成图片G的内容与内容图片C的内容有多相似。<br>J style (S, G)<br>然后我们会把结果加上一个风格代价函数,也就是关于S和G的函数,用来度量图片G的<br>风格和图片S的风格的相似度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">J(G) = aJ content (C, G) + βJ style (S, G）</span><br></pre></td></tr></table></figure></p>
<h4 id="梗概"><a href="#梗概" class="headerlink" title="梗概"></a>梗概</h4><p><img src="/home/alex/图片/梗概.png" alt="uplosuccessful"></p>
<h4 id="content-cost-function"><a href="#content-cost-function" class="headerlink" title="content cost function"></a>content cost function</h4><p><img src="/home/alex/图片/2018-163.png" alt="filename eady exiss, renamed"></p>
<p>现在你需要衡量假如有一个内容图片和一个生成图片他们在内容上的相似度,我们令这<br>个a [l][C] 和a [l][G] ,代表这两个图片C和G的l层的激活函数值。如果这两个激活值相似,那么<br>就意味着两个图片的内容相似。<br>1<br>我们定义这个: J content (C, G) = 2 ||a [l][C] − a [l][G] || 2 ,为两个激活值不同或者相似的程度,<br>我们取l层的隐含单元的激活值,按元素相减,内容图片的激活值与生成图片相比较,然后<br>510第四门课 卷积神经网络(Convolutional Neural Networks)-第四周 特殊应用:人脸识别和神经风格转换<br>(Special applications: Face recognition &amp;Neural style transfer)<br>1<br>取平方,也可以在前面加上归一化或者不加,比如 或者其他的,都影响不大,因为这都可以<br>2<br>由这个超参数 α 来调整(J(G) = aJ content (C, G) + βJ style (S, G))。</p>
<h3 id="style-cost-function"><a href="#style-cost-function" class="headerlink" title="style cost function"></a>style cost function</h3><p><img src="/home/alex/图片/style.png" alt="upload successf"></p>
<p><img src="/home/alex/图片/feifeili.png" alt="upload success"></p>
<p><img src="/home/alex/图片/2018-164.png" alt="filenlready exists, renamed"></p>
<p>对于这个风格矩阵,你要做的就是计算这个矩阵也就是G [l] 矩阵,它是个n c × n c 的矩阵,<br>也就是一个方阵。记住,因为这里有n c 个通道,所以矩阵的大小是n c × n c 。以便计算每一对<br>[l]<br>激活项的相关系数,所以G kk ′ 可以用来测量k通道与k′通道中的激活项之间的相关系数,k和<br>k′会在 1 到n c 之间取值,n c 就是l层中通道的总数量。</p>
<p>可以看做这是两个通道间的协方差。</p>
<h3 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a>implementation</h3><h4 id="using-an-ConvNet-to-compute-encodings"><a href="#using-an-ConvNet-to-compute-encodings" class="headerlink" title="using an ConvNet to compute encodings"></a>using an ConvNet to compute encodings</h4><p><img src="/home/alex/图片/2018-165.png" alt="filename already ets, renamed"></p>
<p>这里使用的是Inception Network</p>
<p>把两张图片分别转换成2个128维的向量，然后计算这两个向量的距离。</p>
<p><img src="/home/alex/图片/howcan.png" alt="upload cessful"></p>
<h4 id="compute-triplet-loss"><a href="#compute-triplet-loss" class="headerlink" title="compute triplet loss"></a>compute triplet loss</h4><p><img src="/home/alex/图片/thisss.png" alt="uploadcessful"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: triplet_loss</span><br><span class="line"></span><br><span class="line">def triplet_loss(y_true, y_pred, alpha = 0.2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implementation of the triplet loss as defined by formula (3)</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    y_true -- true labels, required when you define a loss in Keras, you don&apos;t need it in this function.</span><br><span class="line">    y_pred -- python list containing three objects:</span><br><span class="line">            anchor -- the encodings for the anchor images, of shape (None, 128)</span><br><span class="line">            positive -- the encodings for the positive images, of shape (None, 128)</span><br><span class="line">            negative -- the encodings for the negative images, of shape (None, 128)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    loss -- real number, value of the loss</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (≈ 4 lines)</span><br><span class="line">    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1</span><br><span class="line">    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)</span><br><span class="line">    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1</span><br><span class="line">    neg_dist =  tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)</span><br><span class="line">    # Step 3: subtract the two previous distances and add alpha.</span><br><span class="line">    basic_loss = tf.add(alpha,tf.subtract(pos_dist,neg_dist))</span><br><span class="line">    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.</span><br><span class="line">    loss = tf.reduce_sum(tf.maximum(basic_loss,0))</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>
<h4 id="verify"><a href="#verify" class="headerlink" title="verify"></a>verify</h4><p>这里设定阈值！与数据库中已有的照片进行比较</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: verify</span><br><span class="line"></span><br><span class="line">def verify(image_path, identity, database, model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Function that verifies if the person on the &quot;image_path&quot; image is &quot;identity&quot;.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    image_path -- path to an image</span><br><span class="line">    identity -- string, name of the person you&apos;d like to verify the identity. Has to be a resident of the Happy house.</span><br><span class="line">    database -- python dictionary mapping names of allowed people&apos;s names (strings) to their encodings (vectors).</span><br><span class="line">    model -- your Inception model instance in Keras</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    dist -- distance between the image_path and the image of &quot;identity&quot; in the database.</span><br><span class="line">    door_open -- True, if the door should open. False otherwise.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)</span><br><span class="line">    encoding = img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    # Step 2: Compute distance with identity&apos;s image (≈ 1 line)</span><br><span class="line">    dist = np.linalg.norm(database[identity]-encoding)</span><br><span class="line">    </span><br><span class="line">    # Step 3: Open the door if dist &lt; 0.7, else don&apos;t open (≈ 3 lines)</span><br><span class="line">    if dist&lt;0.7:</span><br><span class="line">        print(&quot;It&apos;s &quot; + str(identity) + &quot;, welcome home!&quot;)</span><br><span class="line">        door_open = True</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;It&apos;s not &quot; + str(identity) + &quot;, please go away&quot;)</span><br><span class="line">        door_open = False</span><br><span class="line">        </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    return dist, door_open</span><br></pre></td></tr></table></figure>
<h3 id="face-recognition"><a href="#face-recognition" class="headerlink" title="face recognition"></a>face recognition</h3><p>在数据库中寻找dist与输入图片最小的，结果就是识别出来的人。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"> GRADED FUNCTION: who_is_it</span><br><span class="line"></span><br><span class="line">def who_is_it(image_path, database, model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements face recognition for the happy house by finding who is the person on the image_path image.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    image_path -- path to an image</span><br><span class="line">    database -- database containing image encodings along with the name of the person on the image</span><br><span class="line">    model -- your Inception model instance in Keras</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    min_dist -- the minimum distance between image_path encoding and the encodings from the database</span><br><span class="line">    identity -- string, the name prediction for the person on image_path</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### </span><br><span class="line">    </span><br><span class="line">    ## Step 1: Compute the target &quot;encoding&quot; for the image. Use img_to_encoding() see example above. ## (≈ 1 line)</span><br><span class="line">    encoding = img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    ## Step 2: Find the closest encoding ##</span><br><span class="line">    </span><br><span class="line">    # Initialize &quot;min_dist&quot; to a large value, say 100 (≈1 line)</span><br><span class="line">    min_dist = 100</span><br><span class="line">    </span><br><span class="line">    # Loop over the database dictionary&apos;s names and encodings.</span><br><span class="line">    for (name, db_enc) in database.items():</span><br><span class="line">        </span><br><span class="line">        # Compute L2 distance between the target &quot;encoding&quot; and the current &quot;emb&quot; from the database. (≈ 1 line)</span><br><span class="line">        dist = np.linalg.norm(db_enc-encoding)</span><br><span class="line"></span><br><span class="line">        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)</span><br><span class="line">        if dist&lt;min_dist:</span><br><span class="line">            min_dist = dist</span><br><span class="line">            identity = name</span><br><span class="line"></span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    if min_dist &gt; 0.7:</span><br><span class="line">        print(&quot;Not in the database.&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print (&quot;it&apos;s &quot; + str(identity) + &quot;, the distance is &quot; + str(min_dist))</span><br><span class="line">        </span><br><span class="line">    return min_dist, identity</span><br></pre></td></tr></table></figure>
<h3 id="Neural-Style-Transfer"><a href="#Neural-Style-Transfer" class="headerlink" title="Neural Style Transfer"></a>Neural Style Transfer</h3><h4 id="compute-the-content-cost"><a href="#compute-the-content-cost" class="headerlink" title="compute the content cost"></a>compute the content cost</h4><p>在网络里面，浅层的卷积网络倾向于检测一些如边缘与简单内容的低层次特征，深层的卷积网络倾向于检测一些高层次特征例如目标的类别等。</p>
<p>我们目标是生成的图片G与输入图片C有相同的内容。假设选定了某层的激活层来代表图片的内容，在实践中，选择那些不太深也不太浅——即中间的网络。</p>
<p>所以假定你选择一个隐含层，设定图片C作为预训练的VGG网络的输入，然后前向反馈，记a^[c]为你选择的隐含层的激活。 这将是一个维度为nHxnWxnC的张量。<br>对输入的图片G重复上述操作。</p>
<p>我们定义损失函数如下</p>
<p><img src="/home/alex/图片/2018-166.png" alt="filena already exists, renamed"></p>
<p>这里的nH,nW,nC分别为你所选择的隐含层的高度，宽度与通道数目。</p>
<p>为了方便计算J_content(C,G) ，需要把三维的volumns unrolled成二维的矩阵。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: compute_content_cost</span><br><span class="line"></span><br><span class="line">def compute_content_cost(a_C, a_G):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the content cost</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C </span><br><span class="line">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    J_content -- scalar that you compute using equation 1 above.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # Retrieve dimensions from a_G (≈1 line)</span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    new_shape = [m,(n_H*n_W),n_C]</span><br><span class="line">    </span><br><span class="line">    # Reshape a_C and a_G (≈2 lines)</span><br><span class="line">    a_C_unrolled = tf.reshape(a_C,shape=new_shape)</span><br><span class="line">    a_G_unrolled = tf.reshape(a_G,shape=new_shape)</span><br><span class="line">    </span><br><span class="line">    # compute the cost with tensorflow (≈1 line)</span><br><span class="line">    J_content = 1/(4*n_H*n_W*n_C)*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return J_content</span><br></pre></td></tr></table></figure>
<h4 id="STYLE-MATRIX"><a href="#STYLE-MATRIX" class="headerlink" title="STYLE MATRIX"></a>STYLE MATRIX</h4><p>在线性代数中，风格矩阵也叫作Gram矩阵.</p>
<p>matrix G of set of vectors(v1,v2…vn)</p>
<p>是任意两个向量vi,vj的点乘的矩阵，看作是来衡量vi<br>与vj是有多相似的。如果vi与vj很相似，得到的点乘也会越大。</p>
<p>Gij=vTivj=np.dot(vi,vj)Gij=viTvj=np.dot(vi,vj). In other words, GijGij compares how similar vivi is to vjvj: </p>
<p><img src="/home/alex/图片/af.png" alt="upload sucssful"></p>
<p>这个矩阵维度是(nc,nc),nc是filter的数量，Gij的值衡量了filter_i的激活与filter_j的激活的相似程度。</p>
<p>而矩阵的对角元素G_ii衡量了filter_i的活跃程度。举个例子，如果filter_i是用来检测竖直特征的，那么Gii就衡量了整张图片出现竖直特征的总体情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># GRADED FUNCTION: gram_matrix</span><br><span class="line"></span><br><span class="line">def gram_matrix(A):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Argument:</span><br><span class="line">    A -- matrix of shape (n_C, n_H*n_W)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    GA -- Gram matrix of A, of shape (n_C, n_C)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (≈1 line)</span><br><span class="line">    GA =  tf.matmul(A,tf.transpose(A))</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return GA</span><br></pre></td></tr></table></figure>
<h3 id="Style-cost"><a href="#Style-cost" class="headerlink" title="Style cost"></a>Style cost</h3><p>生成了风格矩阵之后，你的目标就是最小化风格图片的Gram矩阵与生成图片的Gram矩阵的距离。这就可以写出一个损失函数，然后转化为一个最优化问题就行。</p>
<p><img src="/home/alex/图片/2018-167.png" alt="filename already ests, renamed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: compute_layer_style_cost</span><br><span class="line"></span><br><span class="line">def compute_layer_style_cost(a_S, a_G):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S </span><br><span class="line">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # Retrieve dimensions from a_G (≈1 line)</span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    new_shape = [n_H*n_W,n_C]</span><br><span class="line">    # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)</span><br><span class="line">    a_S = tf.reshape(a_S,new_shape)</span><br><span class="line">    a_S = tf.transpose(a_S)</span><br><span class="line">    a_G = tf.reshape(a_G,new_shape)</span><br><span class="line">    a_G = tf.transpose(a_G)</span><br><span class="line">    # Computing gram_matrices for both images S and G (≈2 lines)</span><br><span class="line">    GS = gram_matrix(a_S)</span><br><span class="line">    GG = gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">    # Computing the loss (≈1 line)</span><br><span class="line">    J_style_layer = 1/(4*(n_H*n_W*n_C)**2)*tf.reduce_sum(tf.square(tf.subtract(GS,GG)))</span><br><span class="line">    </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return J_style_layer</span><br></pre></td></tr></table></figure>
<p>定义J_style</p>
<p><img src="/home/alex/图片/J_style.png" alt="upload suessful"></p>
<p>定义最终的cost</p>
<p><img src="/home/alex/图片/cosst.png" alt="upload ccessful"></p>
<p>再对这个损失函数求优化问题就行了。</p>
<p>result：<br><img src="/home/alex/图片/2018-168.png" alt="filenamlready exists, renamed"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/31/DL-C4W3-YOLO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/31/DL-C4W3-YOLO/" itemprop="url">DL-C4W3(YOLO)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-31T14:24:36+08:00">
                2018-12-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="intro-边框的定义"><a href="#intro-边框的定义" class="headerlink" title="intro 边框的定义"></a>intro 边框的定义</h3><p><img src="/home/alex/图片/2018-149.png" alt="filename alreay exists, renamed"></p>
<p>加入我们要分类80个物品，那么就有80个c（c0,c1…c80）</p>
<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>you only look once，只做一次前馈传播，并使用非最大化抑制之后就可以输出目标框。</p>
<h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4><p><img src="/home/alex/图片/编码.png" alt="uplouccessful"></p>
<p><img src="/home/alex/图片/sleep.png" alt="upload succful"></p>
<p>所谓anchor box，就是用来使得一个格子能够检测出多个对象。需要预先定义好anchor box的形状，当每找到一个对象的中点的时候，不仅仅把中点分配给对应的grid，而且还会分配到对应的anchor box</p>
<p>对于每个anchor box，找出该框包含某一类的概率</p>
<p><img src="/home/alex/图片/计算.png" alt="upload ccessful"></p>
<h4 id="可视化预测"><a href="#可视化预测" class="headerlink" title="可视化预测"></a>可视化预测</h4><p><img src="/home/alex/图片/取最大值.png" alt="upload sful"></p>
<p><img src="/home/alex/图片/2018-150.png" alt="filename already exists, ramed"></p>
<p>当框框太多的时候，使用<em>非最大化抑制</em>的方法剔除一些重叠的框框。</p>
<p>code见下</p>
<h4 id="filter-box"><a href="#filter-box" class="headerlink" title="filter_box"></a>filter_box</h4><p><img src="/home/alex/图片/filter.png" alt="upload sucsful"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">    通过阈值来过滤对象和分类的置信度。</span><br><span class="line"></span><br><span class="line">    参数：</span><br><span class="line">        box_confidence  - tensor类型，维度为（19,19,5,1）,包含19x19单元格中每个单元格预测的5个锚框中的所有的锚框的pc （一些对象的置信概率）。</span><br><span class="line">        boxes - tensor类型，维度为(19,19,5,4)，包含了所有的锚框的（px,py,ph,pw ）。</span><br><span class="line">        box_class_probs - tensor类型，维度为(19,19,5,80)，包含了所有单元格中所有锚框的所有对象( c1,c2,c3，···，c80 )检测的概率。</span><br><span class="line">        threshold - 实数，阈值，如果分类预测的概率高于它，那么这个分类预测的概率就会被保留。</span><br><span class="line"></span><br><span class="line">    返回：</span><br><span class="line">        scores - tensor 类型，维度为(None,)，包含了保留了的锚框的分类概率。</span><br><span class="line">        boxes - tensor 类型，维度为(None,4)，包含了保留了的锚框的(b_x, b_y, b_h, b_w)</span><br><span class="line">        classess - tensor 类型，维度为(None,)，包含了保留了的锚框的索引</span><br><span class="line"></span><br><span class="line">    注意：&quot;None&quot;是因为你不知道所选框的确切数量，因为它取决于阈值。</span><br><span class="line">          比如：如果有10个锚框，scores的实际输出大小将是（10,）</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Step 1: Compute box scores</span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    box_scores = box_confidence * box_class_probs</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines)</span><br><span class="line">    box_classes = K.argmax(box_scores, axis = -1)</span><br><span class="line">    box_class_scores = K.max(box_scores, axis = -1)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Step 3: Create a filtering mask based on &quot;box_class_scores&quot; by using &quot;threshold&quot;. The mask should have the</span><br><span class="line">    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability &gt;= threshold)</span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    filtering_mask = (box_class_scores &gt;= threshold)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Step 4: Apply the mask to scores, boxes and classes</span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines)</span><br><span class="line">    scores = tf.boolean_mask(box_class_scores,filtering_mask)</span><br><span class="line">    boxes = tf.boolean_mask(boxes,filtering_mask)</span><br><span class="line">    classes= tf.boolean_mask(box_classes,filtering_mask)</span><br><span class="line">    </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return scores, boxes, classes</span><br></pre></td></tr></table></figure>
<h4 id="非最大化抑制"><a href="#非最大化抑制" class="headerlink" title="非最大化抑制"></a>非最大化抑制</h4><p><img src="/home/alex/图片/2018-151.png" alt="filename already exists, renamed"></p>
<p>1.假设首先设定阈值为0.6，抛弃所有pc&lt;=0.6可能性的框框，这一步先剔除了所有可能性很低的框框。</p>
<p>2.选中一个pc最大的框框，作为输出，然后抛弃所有其他的与输出的交并比&gt;=0.5的框框</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Applies Non-max suppression (NMS) to set of boxes</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    scores -- tensor of shape (None,), output of yolo_filter_boxes()</span><br><span class="line">    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)</span><br><span class="line">    classes -- tensor of shape (None,), output of yolo_filter_boxes()</span><br><span class="line">    max_boxes -- integer, maximum number of predicted boxes you&apos;d like</span><br><span class="line">    iou_threshold -- real value, &quot;intersection over union&quot; threshold used for NMS filtering</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    scores -- tensor of shape (, None), predicted score for each box</span><br><span class="line">    boxes -- tensor of shape (4, None), predicted box coordinates</span><br><span class="line">    classes -- tensor of shape (, None), predicted class for each box</span><br><span class="line">    </span><br><span class="line">    Note: The &quot;None&quot; dimension of the output tensors has obviously to be less than max_boxes. Note also that this</span><br><span class="line">    function will transpose the shapes of scores, boxes, classes. This is made for convenience.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    max_boxes_tensor = K.variable(max_boxes, dtype=&apos;int32&apos;)     # tensor to be used in tf.image.non_max_suppression()</span><br><span class="line">    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor</span><br><span class="line">    </span><br><span class="line">    # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep</span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    indicesList = tf.image.non_max_suppression(</span><br><span class="line">    boxes,</span><br><span class="line">    scores,</span><br><span class="line">    max_boxes,</span><br><span class="line">    iou_threshold,</span><br><span class="line">    score_threshold=float(&apos;-inf&apos;),</span><br><span class="line">    name=None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    # Use K.gather() to select only nms_indices from scores, boxes and classes</span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines)</span><br><span class="line">    #把在indicesList的scores gather起来</span><br><span class="line">    scores = K.gather(scores,indicesList)</span><br><span class="line">    boxes = K.gather(boxes,indicesList)</span><br><span class="line">    classes = K.gather(classes,indicesList)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return scores, boxes, classes</span><br></pre></td></tr></table></figure>
<h3 id="所有框进行过滤"><a href="#所有框进行过滤" class="headerlink" title="所有框进行过滤"></a>所有框进行过滤</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:</span><br><span class="line">                    box_confidence: tensor of shape (None, 19, 19, 5, 1)</span><br><span class="line">                    box_xy: tensor of shape (None, 19, 19, 5, 2)</span><br><span class="line">                    box_wh: tensor of shape (None, 19, 19, 5, 2)</span><br><span class="line">                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)</span><br><span class="line">    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)</span><br><span class="line">    max_boxes -- integer, maximum number of predicted boxes you&apos;d like</span><br><span class="line">    score_threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box</span><br><span class="line">    iou_threshold -- real value, &quot;intersection over union&quot; threshold used for NMS filtering</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    scores -- tensor of shape (None, ), predicted score for each box</span><br><span class="line">    boxes -- tensor of shape (None, 4), predicted box coordinates</span><br><span class="line">    classes -- tensor of shape (None,), predicted class for each box</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### </span><br><span class="line">    </span><br><span class="line">    # Retrieve outputs of the YOLO model (≈1 line)</span><br><span class="line">    box_confidence,box_xy,box_wh,box_class_probs = yolo_outputs</span><br><span class="line"></span><br><span class="line">    # Convert boxes to be ready for filtering functions </span><br><span class="line">    boxes = yolo_boxes_to_corners(box_xy, box_wh) </span><br><span class="line">    # Use one of the functions you&apos;ve implemented to perform Score-filtering with a threshold of score_threshold (≈1 line)</span><br><span class="line">    scores,boxes,classes = yolo_filter_boxes(box_confidence,boxes,box_class_probs,score_threshold)</span><br><span class="line">    # Scale boxes back to original image shape.</span><br><span class="line">    boxes = scale_boxes(boxes, image_shape)</span><br><span class="line">    # Use one of the functions you&apos;ve implemented to perform Non-max suppression with a threshold of iou_threshold (≈1 line)</span><br><span class="line">    scores,boxes,classes = yolo_non_max_suppression(scores, boxes, classes,max_boxes,iou_threshold)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return scores, boxes, classes</span><br></pre></td></tr></table></figure>
<h3 id="YOLO总结"><a href="#YOLO总结" class="headerlink" title="YOLO总结"></a>YOLO总结</h3><p><img src="/home/alex/图片/2018-152.png" alt="filename eady exists, renamed"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/29/DL-C4W2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/29/DL-C4W2/" itemprop="url">DL-C4W2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-29T13:26:56+08:00">
                2018-12-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet -5"></a>LeNet -5</h3><p><img src="/home/alex/图片/alex.png" alt="upload ful"></p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="/home/alex/图片/local.png" alt="upload ssful"></p>
<p>conv-&gt;max pool-&gt;conv-&gt;max pool-&gt;conv-&gt;conv-&gt;conv-&gt;conv-&gt;max pool-&gt;fc-&gt;fc-&gt;fc</p>
<h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p><img src="/home/alex/图片/VGG16.png" alt="upload ssful"></p>
<p>16指的是只有１６层网络有需要学习的参数，ｐｏｏｌｉｎｇ层是没有要学习的参数的。</p>
<p>箭头下方的[CONV 64]指有６４个ｆｉｌｔｅｒ，x2值对两层作卷积</p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><h4 id="residual-block"><a href="#residual-block" class="headerlink" title="residual block"></a>residual block</h4><p><img src="/home/alex/图片/bl.png" alt="uload successful"></p>
<p><img src="/home/alex/图片/tiaodong.png" alt="uplosuccessful"></p>
<p> 增加short cut之后成为残差块的网络结构</p>
<p><img src="/home/alex/图片/2018-140.png" alt="filename alreay exists, renamed"></p>
<p> 可以构建更为深层的网络</p>
<h3 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h3><p><img src="/home/alex/图片/residual Network.png" alt="ccessful"></p>
<p> 没有residual block的网络叫做plain network</p>
<h4 id="why-it-works"><a href="#why-it-works" class="headerlink" title="why it works?"></a>why it works?</h4><p><img src="/home/alex/图片/2018-141.png" alt="filenamelready exists, renamed"></p>
<p>对于越深层的神经网络来说，参数越来越多，越来越难选择，将会导致连学习identity function（f(x)=x）都很困难，而如果用residual block，a^[l+2] = g(a^[l]) = a^[l],对于残差块来学习identity function 其实是很简单的，所以不影响性能。</p>
<p><img src="/home/alex/图片/哪里.png" alt="upload successfl"></p>
<p>在经历了相同的conv之后增加一层pooling</p>
<h3 id="1x1-convolution"><a href="#1x1-convolution" class="headerlink" title="1x1 convolution"></a>1x1 convolution</h3><p>又叫做网中网</p>
<p><img src="/home/alex/图片/2018-142.png" alt="filenamey exists, renamed"></p>
<p>对一层 nxnxc 的图片，应用1x1xc的卷积核，得到新一层</p>
<p>nxnx1，f个filter处理后，就得到一个nxnxf的新层。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><h5 id="降维或升维"><a href="#降维或升维" class="headerlink" title="降维或升维"></a>降维或升维</h5><p>压缩channel个数，当卷积核个数小于输入channel数量的时候</p>
<p><img src="/home/alex/图片/convu.png" alt="upload sful"></p>
<h5 id="增加非线性"><a href="#增加非线性" class="headerlink" title="增加非线性"></a>增加非线性</h5><p>why？</p>
<p><img src="/home/alex/图片/2018-143.png" alt="filename already exists,med"></p>
<h5 id="跨通道信息交互"><a href="#跨通道信息交互" class="headerlink" title="跨通道信息交互"></a>跨通道信息交互</h5><p><img src="/home/alex/图片/sun.png" alt="ud successful"></p>
<h3 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h3><p>ref:</p>
<p><a href="https://zhuanlan.zhihu.com/p/40050371" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40050371</a></p>
<p><a href="https://blog.csdn.net/a1154761720/article/details/53411365" target="_blank" rel="noopener">https://blog.csdn.net/a1154761720/article/details/53411365</a></p>
<p><img src="/home/alex/图片/inception.png" alt="upload sssful"></p>
<h3 id="compute-cost"><a href="#compute-cost" class="headerlink" title="compute cost"></a>compute cost</h3><p><img src="/home/alex/图片/2018-145.png" alt="filename alr exists, renamed"></p>
<p>filter的第三个通道数目 == input feature map的第三个通道数目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">结果参数 = 输入层 HxWx卷积核个数</span><br><span class="line"></span><br><span class="line">总计算成本 = (结果参数28x28x32) × （卷积核大小5x5x192）</span><br></pre></td></tr></table></figure>
<h4 id="bottleneck-layer"><a href="#bottleneck-layer" class="headerlink" title="bottleneck layer"></a>bottleneck layer</h4><p><img src="/home/alex/图片/2018-146.png" alt="filename already exists,med"></p>
<h3 id="inception-network"><a href="#inception-network" class="headerlink" title="inception network"></a>inception network</h3><h4 id="google-net"><a href="#google-net" class="headerlink" title="google net"></a>google net</h4><p><img src="/home/alex/图片/googleNet.png" alt="upload suc"></p>
<p>1x1convolution 能够有效减少参数数量，加快训练</p>
<p><img src="/home/alex/图片/2018-147.png" alt="filename already exi, renamed"></p>
<p>可以观察到有一些旁路也输入到softmax中，因为hidden layer有时候的预测效果也不错，这么做可以防止过拟合</p>
<h3 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h3><p>小数据集<br><img src="/home/alex/图片/xiaoshujuji.png" alt="up successful"></p>
<p>大数据集<br><img src="/home/alex/图片/dashujuji.png" alt="upload ssful"></p>
<h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p><img src="/home/alex/图片/2018-148.png" alt="filename already erenamed"></p>
<p>多CPU多线程实现</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.jpg"
                alt="Alex Chiu" />
            
              <p class="site-author-name" itemprop="name">Alex Chiu</p>
              <p class="site-description motion-element" itemprop="description">Alex's personal blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alex Chiu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
