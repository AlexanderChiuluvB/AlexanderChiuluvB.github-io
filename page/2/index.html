<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Alex&apos;s personal blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Alex Chiu">
<meta property="og:url" content="http://AlexanderChiuluvB.github.io/page/2/index.html">
<meta property="og:site_name" content="Alex Chiu">
<meta property="og:description" content="Alex&apos;s personal blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Alex Chiu">
<meta name="twitter:description" content="Alex&apos;s personal blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://AlexanderChiuluvB.github.io/page/2/"/>





  <title>Alex Chiu</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Alex Chiu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/28/Ubuntu-耳机-无线网驱动/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/28/Ubuntu-耳机-无线网驱动/" itemprop="url">Ubuntu 耳机/无线网驱动</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-28T20:23:40+08:00">
                2018-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无线网</p>
<p><a href="https://blog.csdn.net/fljhm/article/details/79281655" target="_blank" rel="noopener">https://blog.csdn.net/fljhm/article/details/79281655</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. make</span><br><span class="line">2. sudo make install</span><br><span class="line">3. sudo modprobe -a 8821ce</span><br></pre></td></tr></table></figure>
<p>耳机</p>
<p>1.alsactl restore</p>
<p>2.</p>
<p>sudo gedit /etc/modprobe.d/alsa-base.conf</p>
<p>添加</p>
<p>options snd-pcsp index=-2<br>alias snd-card-0 snd-hda-intel<br>alias sound-slot-0 snd-hda-intel<br>options snd-hda-intel model=pch position_fix=1</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/28/DL-C4W1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/28/DL-C4W1/" itemprop="url">DL-C4W1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-28T16:04:51+08:00">
                2018-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="为什么要卷积？"><a href="#为什么要卷积？" class="headerlink" title="为什么要卷积？"></a>为什么要卷积？</h3><p>for big size picture, the input scale would be very large. eg. an 1000<em>1000 size picture,<br>after flattening its features , you can get a vector as (3</em>1000*1000,1) = (3million ,1)</p>
<p><img src="/home/alex/图片/large.png" alt="upload ssful"></p>
<p>如果hidden　layer只有1000层，那么W1 的输入大小是(1000,3m)</p>
<p>因为z(1000,1) = W1(1000,3M)*X(3M,1)+b </p>
<h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3><p><img src="/home/alex/图片/边缘检测.png" alt="uad successful"></p>
<p>用一个3<em>3大小的卷积核对一张6</em>6大小的图片进行卷积运算，最终得到一个4x4的图片</p>
<p>python:conv_forward </p>
<p>tf.nn.conv2d</p>
<h4 id="边缘检测原理"><a href="#边缘检测原理" class="headerlink" title="边缘检测原理"></a>边缘检测原理</h4><p>用垂直边缘filter，可以明显吧边缘和非边缘区区分出来。</p>
<p><img src="/home/alex/图片/vertical.png" alt="upload sucssful"></p>
<h4 id="多种边缘检测"><a href="#多种边缘检测" class="headerlink" title="多种边缘检测"></a>多种边缘检测</h4><p><img src="/home/alex/图片/dark.png" alt="upload succful"></p>
<p><strong><em>我们可以直接把filter中的数字直接看作是需要学习的参数</em></strong></p>
<p>在nn中通过反向传播算法，学习到相应于目标结果的filter，然后把其应用在整个图片上，输出其提取到的所有有用的特征。</p>
<h3 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h3><p>从上面注意到每次卷积操作，图片会缩小。</p>
<p><img src="/home/alex/图片/2018-130.png" alt="filename alrea exists, renamed"></p>
<p>所以我们要在卷积之前，为图片加padding，包围角落和边缘的像素，使得通过filter的卷积运算后，图片大小不变，也不会丢失角落。</p>
<p><img src="/home/alex/图片/2018-131.png" alt="filename y exists, renamed"></p>
<h3 id="valid-Some-卷积"><a href="#valid-Some-卷积" class="headerlink" title="valid/Some 卷积"></a>valid/Some 卷积</h3><p>Valid: no padding </p>
<p>nxn –&gt;(n-f+1)x(n-f+1)</p>
<p>Same: padding</p>
<p>输出和输入图片的大小相同</p>
<p>p = (f-1)/2，在CV中，一般来说padding的值位奇数</p>
<p>N+2P-F+1 = N ,SO p = (F-1)/2</p>
<h3 id="卷积步长（stride）"><a href="#卷积步长（stride）" class="headerlink" title="卷积步长（stride）"></a>卷积步长（stride）</h3><p>stride=1,表示每次卷积运算以一个步长进行移动。</p>
<p><img src="/home/alex/图片/stride.png" alt="upload ful"></p>
<h3 id="立体卷积"><a href="#立体卷积" class="headerlink" title="立体卷积"></a>立体卷积</h3><p><img src="/home/alex/图片/立体卷积.png" alt="upd successful"></p>
<p><img src="/home/alex/图片/2018-132.png" alt="filename alrea exists, renamed"></p>
<p>第一行表示只检测红色通道的垂直边缘</p>
<p>第二行表示检测所有通道垂直边缘</p>
<p>卷积核第三个维度大小等于图片通道大小</p>
<h3 id="多卷积"><a href="#多卷积" class="headerlink" title="多卷积"></a>多卷积</h3><p><img src="/home/alex/图片/nminus.png" alt="upcessful"></p>
<p>上图意思是把检测垂直和水平边缘的两个图片叠成两层。</p>
<p><img src="/home/alex/图片/nextsee.png" alt="upload ful"></p>
<h3 id="单层卷积网络"><a href="#单层卷积网络" class="headerlink" title="单层卷积网络"></a>单层卷积网络</h3><p><img src="/home/alex/图片/danceng.png" alt="upload cessful"></p>
<p>与普通神经网络单层前向传播类似，卷机神经网络也是先由权重和bias做线性运算，然后得到结果在输入到一个激活函数中。</p>
<p><img src="/home/alex/图片/az.png" alt="upload succsful"></p>
<p>对应上图a[0]表示图片层（n<em>n</em>3）</p>
<p>w[1]对应卷积核（f<em>f</em>3）</p>
<p>a[1] 对应下一层（4x4x2）</p>
<h4 id="单层卷积参数个数"><a href="#单层卷积参数个数" class="headerlink" title="单层卷积参数个数"></a>单层卷积参数个数</h4><p><img src="/home/alex/图片/2018-134.png" alt="filename alreay exists, renamed"></p>
<p>不受图片大小影响</p>
<h3 id="标记"><a href="#标记" class="headerlink" title="标记"></a>标记</h3><p><img src="/home/alex/图片/2018-135.png" alt="filename alread exists, renamed"></p>
<p>f[l] 卷积核大小</p>
<p>卷积核第三个维度大小等于输入图片通道数</p>
<p>而权重就是卷积核大小×卷积核个数，卷积核个数就是输出层的通道数目</p>
<p>激活值大小就是下一层输出层的大小： nH X nW X nC</p>
<h4 id="简单卷积网络"><a href="#简单卷积网络" class="headerlink" title="简单卷积网络"></a>简单卷积网络</h4><p><img src="/home/alex/图片/2018-136.png" alt="filename y exists, renamed"></p>
<p>最后得到的7x7x40，一共1960个参数，就是最后输入激活函数的所有参数</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>最大池化(max pooling)</p>
<p>把前一层得到的特征图进行池化减小，仅由当前小区域内的最大值来代表最终池化后的值。</p>
<p><img src="/home/alex/图片/调参.png" alt="uploaduccessful"></p>
<p>平均池化</p>
<p><img src="/home/alex/图片/池化.png" alt="upload suessful"></p>
<p>池化只需要设置好超参数，没有要学习的参数</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>CNN的最大特点在于卷积的权值共享结构，可以大幅减少神经网络参数量，防止过拟合的同时又降低了神经网络模型的复杂度。</p>
<p>CNN通过卷积的方式实现局部链接，得到图片的参数量只跟卷积核的大小有关，一个卷积核对应一个图片特征，每一个卷积核滤波得到的图像就是一类特征的映射。</p>
<p>也就是说训练的权值数量只与卷积核大小与数量有关，但注意的是隐含层节点数量没有下降，隐含节点的数量只与卷积的步长有关，如果步长为1，那么隐含节点数量与输入图像像素数量一致。如果步长为5，那么每5x5个像素才需要一个隐含节点。</p>
<p>再总结，CNN的要点就是</p>
<p>1.局部连接 </p>
<p>2.权值共享</p>
<p>3.池化层的降采样</p>
<p>其中1与2降低了参数量，训练复杂度下降并减轻过拟合。</p>
<p>同时权值共享赋予了卷积网络对平移的容忍性。</p>
<p><img src="/home/alex/图片/总结.png" alt="upload successl"></p>
<p>随着nn层数增加，提取的特征图片大小将会减小，但是同时间通道数量会增加</p>
<p>为什么使用CNN？</p>
<p>1.参数少<br><img src="/home/alex/图片/cnn.png" alt="upload succeul"></p>
<p>2.参数共享&amp;链接的稀疏性</p>
<p>参数共享指一个卷积核可以有多个不同的卷积核，而每一个卷积核对应一个滤波后映射出的新图像，同一个新图像的每一个像素都来自完全相同的卷积核。</p>
<p><img src="/home/alex/图片/2018-137.png" alt="filename already exists, renad"></p>
<h3 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a>implementation</h3><h4 id="zero-padding"><a href="#zero-padding" class="headerlink" title="zero padding"></a>zero padding</h4><p><img src="/home/alex/图片/2018-138.png" alt="filename alreay exists, renamed"></p>
<h5 id="benefits"><a href="#benefits" class="headerlink" title="benefits"></a>benefits</h5><p><img src="/home/alex/图片/2018-139.png" alt="finame already exists, renamed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def zero_pad(X, pad):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, </span><br><span class="line">    as illustrated in Figure 1.</span><br><span class="line">    </span><br><span class="line">    Argument:</span><br><span class="line">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images</span><br><span class="line">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), &apos;constant&apos;, constant_values=0)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return X_pad</span><br></pre></td></tr></table></figure>
<h4 id="forward-convolution"><a href="#forward-convolution" class="headerlink" title="forward convolution"></a>forward convolution</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def conv_single_step(a_slice_prev, W, b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation </span><br><span class="line">    of the previous layer.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)</span><br><span class="line">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)</span><br><span class="line">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    # Element-wise product between a_slice and W. Add bias.</span><br><span class="line">    s = np.multiply(a_slice_prev, W) + b</span><br><span class="line">    # Sum over all entries of the volume s</span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    return Z</span><br></pre></td></tr></table></figure>
<h4 id="define-a-slice"><a href="#define-a-slice" class="headerlink" title="define a slice"></a>define a slice</h4><p><img src="/home/alex/图片/slice.png" alt="upload succeful"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def conv_forward(A_prev, W, b, hparameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the forward propagation for a convolution function</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span><br><span class="line">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)</span><br><span class="line">    b -- Biases, numpy array of shape (1, 1, 1, n_C)</span><br><span class="line">    hparameters -- python dictionary containing &quot;stride&quot; and &quot;pad&quot;</span><br><span class="line">        </span><br><span class="line">    Returns:</span><br><span class="line">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)</span><br><span class="line">    cache -- cache of values needed for the conv_backward() function</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # Retrieve dimensions from A_prev&apos;s shape (≈1 line)  </span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    # Retrieve dimensions from W&apos;s shape (≈1 line)</span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line"></span><br><span class="line">    # Retrieve information from &quot;hparameters&quot; (≈2 lines)</span><br><span class="line">    stride = hparameters[&apos;stride&apos;]</span><br><span class="line">    pad = hparameters[&apos;pad&apos;]</span><br><span class="line">    </span><br><span class="line">    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)</span><br><span class="line">    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1</span><br><span class="line">    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1</span><br><span class="line">    </span><br><span class="line">    # Initialize the output volume Z with zeros. (≈1 line)</span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    # Create A_prev_pad by padding A_prev</span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    for i in range(m):                                 # loop over the batch of training examples</span><br><span class="line">        a_prev_pad = A_prev_pad[i]                     # Select ith training example&apos;s padded activation</span><br><span class="line">        for h in range(n_H):                           # loop over vertical axis of the output volume</span><br><span class="line">            for w in range(n_W):                       # loop over horizontal axis of the output volume</span><br><span class="line">                for c in range(n_C):                   # loop over channels (= #filters) of the output volume</span><br><span class="line">                    # Find the corners of the current &quot;slice&quot; (≈4 lines)</span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line">                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)</span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])</span><br><span class="line">                                        </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    # Making sure your output shape is correct</span><br><span class="line">    assert(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    # Save information in &quot;cache&quot; for the backprop</span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    return Z, cache</span><br></pre></td></tr></table></figure>
<h3 id="对应的notebook"><a href="#对应的notebook" class="headerlink" title="对应的notebook"></a>对应的notebook</h3><p><a href="https://github.com/AlexanderChiuluvB/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb" target="_blank" rel="noopener">https://github.com/AlexanderChiuluvB/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/27/DL-C2W3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/27/DL-C2W3/" itemprop="url">DL-C2W3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-27T18:35:34+08:00">
                2018-12-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Hyperparameter-tuning"><a href="#Hyperparameter-tuning" class="headerlink" title="Hyperparameter tuning"></a>Hyperparameter tuning</h3><p>don’t use grid search</p>
<p>because you do not know which hyperparameters<br>matters most!</p>
<p><img src="/home/alex/图片/2018-123.png" alt="flename already exists, renamed"></p>
<p>instead,use randomly chosen hyperparameter</p>
<h4 id="coarse-to-fine"><a href="#coarse-to-fine" class="headerlink" title="coarse to fine"></a>coarse to fine</h4><p><img src="/home/alex/图片/coarse.png" alt="upload successl"></p>
<p>如果某些临近的参数效果不错，那么把选择的范围缩小。</p>
<h3 id="scale-for-params"><a href="#scale-for-params" class="headerlink" title="scale for params"></a>scale for params</h3><p>对数scale</p>
<p><img src="/home/alex/图片/2018-124.png" alt="filename already exists,named"></p>
<p><img src="/home/alex/图片/2018-125.png" alt="filename alreadysts, renamed"></p>
<p>如果beta 为0.9，当beta从0.9变成0.9005的时候，变化很小</p>
<p>但是如果beta很接近于1,当beta从0.999-&gt;0.9995的时候，就会变化很大</p>
<h4 id="panda-caviar-strategy"><a href="#panda-caviar-strategy" class="headerlink" title="panda / caviar strategy"></a>panda / caviar strategy</h4><p>given the computational resources you have,if limited,then you can only watch over one model,so check its cost 每隔一段时间，if any problems occurs, stop and return to previous state.</p>
<p>while if you have enough computational resources,then you can watch out various models<br>at one time,and choose the best one.</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>实质是对hidden units做normalization</p>
<p><img src="/home/alex/图片/youluve.png" alt="upload cessful"></p>
<p><a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" target="_blank" rel="noopener">https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c</a></p>
<p><img src="/home/alex/图片/2018-126.png" alt="filename alry exists, renamed"></p>
<h3 id="fitting-batch-norm-to-nn"><a href="#fitting-batch-norm-to-nn" class="headerlink" title="fitting batch norm to nn"></a>fitting batch norm to nn</h3><p>每个ｅｐｏｃｈ对隐藏层做一次batch normalization</p>
<p>back propagate 的时候不用考虑db，因为在normalize的时候Z_TILDA = (Z-mean)/(sqrt of variance) 会把常数项减掉<br><img src="/home/alex/图片/minzhi.png" alt="upload succeul"></p>
<h3 id="why-batch-norm-work"><a href="#why-batch-norm-work" class="headerlink" title="why batch norm work?"></a>why batch norm work?</h3><p><img src="/home/alex/图片/2018-127.png" alt="filename already exi renamed"></p>
<p>当你训练一个模型，得到一个x-&gt;y的映射之后，如果预测数据分布发生了变化，你需要重新训练你的模型。这叫做 covariate shift</p>
<p>例如你训练的都是黑猫图片，如果你用这个模型来测试彩色猫的图片，那么效果肯定会不好。</p>
<p>而对hidden units 做normalization （batch norm），即使前面输入数据x发生变化，那么对后面层的影响将会变小很多，因为每一层都’constrained to have the same mean and variance’ 提升了神经网络应付不同输入的健壮性。</p>
<p><img src="/home/alex/图片/2018-128.png" alt="filene already exists, renamed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Consequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a “standard deviation” parameter (gamma) and add a “mean” parameter (beta). In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights.</span><br></pre></td></tr></table></figure>
<h3 id="softmax-layer"><a href="#softmax-layer" class="headerlink" title="softmax layer"></a>softmax layer</h3><p>In mathematics, the softmax function takes an un-normalized vector, and normalizes it into a probability distribution. </p>
<p>我们已经知道，逻辑回归可生成介于 0 和 1.0 之间的小数。例如，某电子邮件分类器的逻辑回归输出值为 0.8，表明电子邮件是垃圾邮件的概率为 80%，不是垃圾邮件的概率为 20%。很明显，一封电子邮件是垃圾邮件或非垃圾邮件的概率之和为 1.0。</p>
<p>Softmax 将这一想法延伸到多类别领域。也就是说，在多类别问题中，Softmax 会为每个类别分配一个用小数表示的概率。这些用小数表示的概率相加之和必须是 1.0。与其他方式相比，这种附加限制有助于让训练过程更快速地收敛。</p>
<h4 id="softmax-function"><a href="#softmax-function" class="headerlink" title="softmax function"></a>softmax function</h4><p><img src="/home/alex/图片/soft.png" alt="upload sucsful"></p>
<p><img src="/home/alex/图片/softmax.png" alt="upload essful"></p>
<h4 id="understanding-softmax"><a href="#understanding-softmax" class="headerlink" title="understanding softmax"></a>understanding softmax</h4><p><img src="/home/alex/图片/2018-129.png" alt="filename  exists, renamed"></p>
<h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/26/JAVA并发编程（一）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/26/JAVA并发编程（一）/" itemprop="url">JAVA并发编程（一）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-26T20:38:02+08:00">
                2018-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h3><p>现代操作系统调度的最小单元是线程，在一个进程当中可以创建多个线程，这些线程拥有各自的计数器，堆栈以及局部变量，并且能够访问共享的内存变量。</p>
<p>处理器在这些线程中高速切换，让使用者感觉到这些线程在同时执行。</p>
<h3 id="线程状态的转换"><a href="#线程状态的转换" class="headerlink" title="线程状态的转换"></a>线程状态的转换</h3><p><img src="/home/alex/图片/karmar.png" alt="uoad successful"></p>
<p>Runnable: 线程对象创建之后，其他线程调用了该对象的start方法，该状态的线程位于可运行线程池中，等待获取CPU使用权。</p>
<p>运行状态：获取了CPU，执行程序代码</p>
<p>阻塞状态：线程由于某种原因放弃CPU使用权，暂时停止运行，直到线程进入就绪状态。</p>
<p>分为三种：</p>
<ul>
<li><p>等待阻塞，运行的线程执行wait()方法，JVM会把该线程放入等待池中。该过程会释放持有的锁</p>
</li>
<li><p>同步阻塞，运行的线程在获取对象的同步锁时，如果同步锁被别的线程占用，JVM会把该线程放入锁池当中</p>
</li>
<li><p>其他阻塞，运行的线程执行sleep()或者join()方法，或者发出了I/O请求的时候，JVM会把线程设置为阻塞状态。当sleep()超时，join()等待线程终止或者超时，或者I/O处理完毕的时候，线程重新进入runnable状态，sleep不会释放持有的锁。</p>
</li>
</ul>
<h3 id="为什么？"><a href="#为什么？" class="headerlink" title="为什么？"></a>为什么？</h3><h4 id="更多的处理器核心"><a href="#更多的处理器核心" class="headerlink" title="更多的处理器核心"></a>更多的处理器核心</h4><p>程序运行当中能够创建多个线程，但是一个线程只能在一个处理核心中运行，单线程程序运行的时候只能利用一个处理器核心。</p>
<p>而多线程把计算逻辑分配到多个处理器核心上面，让多个处理器核心加入到程序运行。</p>
<h4 id="更快的响应时间"><a href="#更快的响应时间" class="headerlink" title="更快的响应时间"></a>更快的响应时间</h4><p>一个面向用户的业务操作，可以把一些数据一致性不强的操作派发给其他线程处理，使得响应用户请求的线程能够尽快处理完成。</p>
<h3 id="线程调度"><a href="#线程调度" class="headerlink" title="线程调度"></a>线程调度</h3><h4 id="线程优先级"><a href="#线程优先级" class="headerlink" title="线程优先级"></a>线程优先级</h4><p>现代操作系统采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度。时间片多少决定了线程使用处理器资源的多少，而线程优先级决定线程得到分配的处理器资源多少。</p>
<p>线程默认优先级位是5，优先级高的线程分配时间片数量要多于优先级低的线程。设置线程优先级的时候，针对频繁阻塞的线程需要设置较高优先级，而偏重计算的线程设置较低的优先级，确保处理器不会被独占。</p>
<p><img src="/home/alex/图片/2018-153.png" alt="filenameready exists, renamed"></p>
<p><img src="/home/alex/图片/2018-157.png" alt="filename ready exists, renamed"></p>
<p>线程的优先级有继承关系</p>
<h4 id="线程睡眠"><a href="#线程睡眠" class="headerlink" title="线程睡眠"></a>线程睡眠</h4><p>Thread.sleep(long mills)<br>使得线程转到阻塞状态，结束后转为runnable状态</p>
<p> sleep()和yield()的区别):sleep()使当前线程进入停滞状态，所以执行sleep()的线程在指定的时间内肯定不会被执行；yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。</p>
<p>  sleep 方法使当前运行中的线程睡眼一段时间，进入不可运行状态，这段时间的长短是由程序设定的，yield 方法使当前线程让出 CPU 占有权，但让出的时间是不可设定的。实际上，yield()方法对应了如下操作：先检测当前是否有相同优先级的线程处于同可运行状态，如有，则把 CPU  的占有权交给此线程，否则，继续运行原来的线程。所以yield()方法称为“退让”，它把运行机会让给了同等优先级的其他线程</p>
<p>   另外，sleep 方法允许较低优先级的线程获得运行机会，但 yield()  方法执行时，当前线程仍处在可运行状态，所以，不可能让出较低优先级的线程些时获得 CPU 占有权。在一个运行系统中，如果较高优先级的线程没有调用 sleep 方法，又没有受到 I\O 阻塞，那么，较低优先级线程只能等待所有较高优先级的线程运行结束，才有机会运行。</p>
<h4 id="wait与sleep区别"><a href="#wait与sleep区别" class="headerlink" title="wait与sleep区别"></a>wait与sleep区别</h4><p><img src="/home/alex/图片/2018-158.png" alt="filenameeady exists, renamed"></p>
<h4 id="线程等待"><a href="#线程等待" class="headerlink" title="线程等待"></a>线程等待</h4><p>Object类的wait()方法，导致当前的线程等待，直到其他线程调用此对象的notify（）方法或者notifyAll()方法。</p>
<p>wait()与notify()方法必须要与synchronized一起使用，也就是也就是wait，与notify是针对已经获取了的锁进行操作。</p>
<p>wait就是说线程在获取对象锁后，主动释放对象锁，同时本线程休眠。直到有其他线程调用对象的notify()唤醒该线程，这样才能继续获取对象锁并继续执行。响应的notify()就是对象锁的唤醒操作。sleep()与wait()二者都可以暂停当前线程，释放CPU的控制权，主要区别是Object.wait()在释放CPU同时，释放了对锁的控制。</p>
<p>该问题为三线程间的同步唤醒操作，主要目的就是ThreadA-&gt;ThreadB-&gt;ThreadC,每一个线程必须同时持有两个对象锁才能继续执行。一个对象锁是prev，是前一个线程所持有的对象锁。还有一个是自身的对象锁。为了控制执行顺序，先持有prev锁，也就是前一个线程要释放自身对象锁，再去申请自身对象锁，两者兼备的时候打印。之后首先调用self.notify()释放自身对象锁，唤醒下一个等待线程，再用prev.wait()释放prev对象锁，终止当前线程。等待循环结束之后再次被唤醒。</p>
<h4 id="线程让步"><a href="#线程让步" class="headerlink" title="线程让步"></a>线程让步</h4><p>Thread.yield()方法，暂停当前执行的线程对象，把执行机会让给相同或者更高优先级的线程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public class ThreadYield extends Thread&#123;</span><br><span class="line">  //private String name;</span><br><span class="line">  public ThreadYield(String name)&#123;</span><br><span class="line">      super(name);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void run()&#123;</span><br><span class="line">    for(int i=1;i&lt;=10;i++)&#123;</span><br><span class="line">      System.out.println(this.getName()+&quot;----&quot;+i);</span><br><span class="line">      if(i==3)&#123;</span><br><span class="line">        this.yield();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static void main(String[]args)&#123;</span><br><span class="line"></span><br><span class="line">    ThreadYield thread1 = new ThreadYield(&quot;ALex&quot;);</span><br><span class="line">    ThreadYield thread2 = new ThreadYield(&quot;Brecher&quot;);</span><br><span class="line"></span><br><span class="line">    thread1.start();</span><br><span class="line">    thread2.start();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里结果表示ALEX执行到i=3的时候，会把CPU让出来，这时候BRECHER抢到线程。<br><img src="/home/alex/图片/2018-155.png" alt="filename already exists,med"></p>
<p>也有可能是BRECHER执行到i=3的时候，会把CPU让出来，这时候还是BRECHER抢到线程。<br><img src="/home/alex/图片/2018-156.png" alt="filename aady exists, renamed"></p>
<h4 id="线程加入"><a href="#线程加入" class="headerlink" title="线程加入"></a>线程加入</h4><p>join（），等待其他线程终止。在当前线程中调用另一个线程的join()方法，当前线程进入阻塞。知道另外一个线程结束，这个线程回到runnable</p>
<p>join():等待t线程终止<br>join是Thread类的一个方法，启动线程后直接调用，join(）作用是等待该线程终止。该线程是指主线程等待子线程的终止<br>也就是在子线程调用了join()方法后面的代码，只有等到子线程结束了才能执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Thread t = new Athread();</span><br><span class="line">t.start();</span><br><span class="line">t.join();</span><br></pre></td></tr></table></figure>
<p>为什么要用join？</p>
<p>如果子线程很耗时，主线程往往提前与子线程结束，万一主线程需要用子线程的处理结果，就是主线程需要等待子线程执行完成之后再结束。</p>
<p>如果不用join函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class Thread1 extends Thread&#123;</span><br><span class="line">  private String name;</span><br><span class="line">  public Thread1(String name)&#123;</span><br><span class="line">    super(name);</span><br><span class="line">    this.name = name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void run()&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;线程开始运行&quot;);</span><br><span class="line">    for(int i=0;i&lt;5;i++)&#123;</span><br><span class="line">      System.out.println(&quot;子线程&quot;+name+&quot;运行&quot;+i);</span><br><span class="line">      try&#123;</span><br><span class="line">        sleep((int)Math.random()*10);</span><br><span class="line">      &#125;catch(InterruptedException e)&#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;线程结束运行&quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  public static void main(String[] args)&#123;</span><br><span class="line"></span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程开始运行&quot;);</span><br><span class="line">    Thread1 Athread = new Thread1(&quot;A&quot;);</span><br><span class="line">    Thread1 Bthread = new Thread1(&quot;B&quot;);</span><br><span class="line">    Athread.start();</span><br><span class="line">    Bthread.start();</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程结束运行&quot;);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>结果：</p>
<p><img src="/home/alex/图片/结果.png" alt="upload succeful"></p>
<p>而在main函数中添加join方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args)&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程开始运行&quot;);</span><br><span class="line">    Thread1 Athread = new Thread1(&quot;A&quot;);</span><br><span class="line">    Thread1 Bthread = new Thread1(&quot;B&quot;);</span><br><span class="line">    Athread.start();</span><br><span class="line">    Bthread.start();</span><br><span class="line">    try&#123;</span><br><span class="line">      Athread.join();</span><br><span class="line">    &#125;catch(InterruptedException e)&#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    try&#123;</span><br><span class="line">      Bthread.join();</span><br><span class="line">    &#125;catch(InterruptedException e)&#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程结束运行&quot;);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<p><img src="/home/alex/图片/2018-154.png" alt="filename amed"></p>
<h4 id="线程唤醒"><a href="#线程唤醒" class="headerlink" title="线程唤醒"></a>线程唤醒</h4><p>object类的notify()方法，唤醒在此对象监视器上等待的单个线程。如果所有线程都在此对象中等待，则会选择唤醒其中一个线程。线程调用其中一个wait方法，在对象的监视器上等待。知道当前线程放弃此对象上的锁定，才能继续执行被唤醒的线程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class ThreadState &#123;</span><br><span class="line"></span><br><span class="line">  public static void main(String[] args)&#123;</span><br><span class="line"></span><br><span class="line">      new Thread(new TimeWaiting(),&quot;TimeWaitingThread&quot;).start();</span><br><span class="line">      new Thread(new Waiting(),&quot;WaitingThread&quot;).start();</span><br><span class="line"></span><br><span class="line">      //使用两个Blocked线程，一个获取锁成功，另外一个被阻塞</span><br><span class="line">      new Thread(new Blocked(),&quot;BlockedThread-1&quot;).start();</span><br><span class="line">      new Thread(new Blocked(),&quot;BlockedTHread-2&quot;).start();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  static class SleepUtils&#123;</span><br><span class="line">      public static void second(long seconds) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">          TimeUnit.SECONDS.sleep(seconds);</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //该线程不断进行睡眠</span><br><span class="line">  static class TimeWaiting implements Runnable&#123;</span><br><span class="line">      public void run()&#123;</span><br><span class="line">        while(true)&#123;</span><br><span class="line">          SleepUtils.second(100);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //在Waiting.class实例上等待</span><br><span class="line">  static class Waiting implements Runnable&#123;</span><br><span class="line">    public void run()&#123;</span><br><span class="line">        while(true)&#123;</span><br><span class="line">          synchronized (Waiting.class)&#123;</span><br><span class="line">              try&#123;</span><br><span class="line">                  Waiting.class.wait();</span><br><span class="line">              &#125;catch(InterruptedException e)&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //该线程在Blocked.class 实例上加锁后，不会释放该锁</span><br><span class="line">  static class Blocked implements Runnable&#123;</span><br><span class="line">    public void run()&#123;</span><br><span class="line">        synchronized (Blocked.class)&#123;</span><br><span class="line">          while(true)&#123;</span><br><span class="line">              SleepUtils.second(100);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行代码，打开shell，输入jps，得到ThreadState的进程ID，然后 输入 jstack ID,得到线程消息</p>
<p><img src="/home/alex/图片/thread.png" alt="upload sucessful"></p>
<p><img src="/home/alex/图片/2018-118.png" alt="filename alrey exists, renamed"></p>
<p>当线程创建之后，调用start方法开始运行，当执行wait()的时候进入等待状态，需要依靠其他线程的通知才能够返回到运行状态。</p>
<p>而超时等待状态相当于在等待状态基础上增加了超时限制，一旦过时，返回到运行状态。</p>
<p>如果线程调用同步方法但没有获得锁，线程会进入到阻塞状态。</p>
<h3 id="常见线程名词解释"><a href="#常见线程名词解释" class="headerlink" title="常见线程名词解释"></a>常见线程名词解释</h3><p>主线程：main()产生的线程</p>
<p>当前线程：Thread.currentThread()获取的进程</p>
<p>后台线程：为其他线程提供服务的线程，称为守护线程</p>
<p>前台线程：接收后台线程服务的线程</p>
<h3 id="Daemon-线程"><a href="#Daemon-线程" class="headerlink" title="Daemon 线程"></a>Daemon 线程</h3><p>这是一种支持型线程，用作程序中后台调度以及支持性工作。</p>
<h3 id="启动和终止线程"><a href="#启动和终止线程" class="headerlink" title="启动和终止线程"></a>启动和终止线程</h3><h4 id="构造线程"><a href="#构造线程" class="headerlink" title="构造线程"></a>构造线程</h4><p>确定线程所属的线程组，线程优先级，是否是Daemon线程等，线程将在堆内存中等待运行。</p>
<h4 id="启动线程"><a href="#启动线程" class="headerlink" title="启动线程"></a>启动线程</h4><p>start()方法将告知JVM，只要线程规划器空闲，应该立即启动start（）方法的线程。</p>
<h3 id="安全地终止线程"><a href="#安全地终止线程" class="headerlink" title="安全地终止线程"></a>安全地终止线程</h3><p>中断状态是线程的一个标识位，而中断操作是一种简便的线程间交互方式。</p>
<p>同时还可以利用一个boolean变量来控制是否需要停止任务并终止该线程。</p>
<p>main线程通过中断操作和cancel（）方法使得countThread得以终止。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class Shutdown &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line"></span><br><span class="line">        Runner one = new Runner();</span><br><span class="line">        Thread countThread = new Thread(one,&quot;CountThread&quot;);</span><br><span class="line">        countThread.start();</span><br><span class="line">        //睡眠1s,main线程对CountThread进行中断，使得CountThread能够感知中断而结束</span><br><span class="line">        TimeUnit.SECONDS.sleep(1);</span><br><span class="line">        countThread.interrupt();</span><br><span class="line">        Runner two = new Runner();</span><br><span class="line">        countThread = new Thread(two,&quot;CountThread&quot;);</span><br><span class="line">        countThread.start();</span><br><span class="line">        //睡眠1s,main线程对Runner Two进行取消，使CountThread 能够感知on 为false</span><br><span class="line">        TimeUnit.SECONDS.sleep(1);</span><br><span class="line">        two.cancel();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static class Runner implements Runnable&#123;</span><br><span class="line">        private long i;</span><br><span class="line">        private volatile boolean on = true;</span><br><span class="line">        public void run()&#123;</span><br><span class="line">          while(on&amp;&amp;!Thread.currentThread().isInterrupted())&#123;</span><br><span class="line">            i++;</span><br><span class="line">          &#125;</span><br><span class="line">          System.out.println(&quot;Count i = &quot;+i);</span><br><span class="line">        &#125;</span><br><span class="line">        public void cancel()&#123;</span><br><span class="line">            on = false;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="线程间通信"><a href="#线程间通信" class="headerlink" title="线程间通信"></a>线程间通信</h3><p>线程开始运行的时候，拥有自己栈空间。java支持多个线程同时访问一个对象或者对象的成员变量，由于每个线程可以拥有这个变量的拷贝（对象以及成员变量分配的内存是在共享内存中，但每个线程可以拥有一份拷贝，可以加速程序的执行）</p>
<h4 id="volatile关键字"><a href="#volatile关键字" class="headerlink" title="volatile关键字"></a>volatile关键字</h4><p>其他线程对该变量进行改变的时候，可以让所有线程感知到变化。保证所有线程对变量访问的可见性。</p>
<h4 id="synchronized-关键字"><a href="#synchronized-关键字" class="headerlink" title="synchronized 关键字"></a>synchronized 关键字</h4><p>主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，保证了线程对变量访问的可见性与排他性，</p>
<p>任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用时，执行方法的线程必须先获取该对象的监视器才能进入同步块和同步方法，如果没有获取到监视器的线程将会被阻塞在同步块和同步方法的入口处，进入到BLOCKED状态</p>
<p><img src="/home/alex/图片/2018-119.png" alt="filename already exists, rend"></p>
<h3 id="线程同步"><a href="#线程同步" class="headerlink" title="线程同步"></a>线程同步</h3><p>1、synchronized关键字的作用域有二种：</p>
<p>1）是某个对象实例内，synchronized aMethod(){}可以防止多个线程同时访问这个对象的synchronized方法（如果一个对象有多个synchronized方法，只要一个线程访问了其中的一个synchronized方法，其它线程不能同时访问这个对象中任何一个synchronized方法）。这时，不同的对象实例的synchronized方法是不相干扰的。也就是说，其它线程照样可以同时访问相同类的另一个对象实例中的synchronized方法；</p>
<p>假设P1，P2是同一个类的不同对象，这个类中定义了以下几种情况的同步块或者同步方法，P1，P2都可以调用他们</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public synchronized void methodAAA()</span><br></pre></td></tr></table></figure>
<p>上面这个函数当对象P1的不同线程执行这个同步方法的时候，会形成互斥，达到同步的效果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public void methodAAA()&#123;</span><br><span class="line">synchronized(this)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>this指调用这个方法的对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public void method3(SomeObject so)&#123;</span><br><span class="line"></span><br><span class="line">	synchronized(so)</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2）是某个类的范围，synchronized static aStaticMethod{}防止多个线程同时访问这个类中的synchronized static 方法。它可以对类的所有对象实例起作用。</p>
<p>2、除了方法前用synchronized关键字，synchronized关键字还可以用于方法中的某个区块中，表示只对这个区块的资源实行互斥访问。用法是: synchronized(this){/<em>区块</em>/}，它的作用域是当前对象；</p>
<p>1.线程同步的目的是为了保护多个线程访问一个资源的时候破坏资源</p>
<p>2.线程同步方法通过锁来实现，每个对象有且仅有一个锁，这个锁与一个特定的对象关联，线程一旦获取了对象的锁，其他这个对象的线程就无法再访问该对象的其他非同步方法。</p>
<p>3.对于静态同步方法，锁是针对这个类的，锁对象是该类的Class对象。静态和非静态方法的锁互不干预。一个线程获得锁，当在一个同步方法中访问另外对象上的同步方法时，会获取这两个对象锁。</p>
<h3 id="线程数据传递"><a href="#线程数据传递" class="headerlink" title="线程数据传递"></a>线程数据传递</h3><h4 id="通过构造方法"><a href="#通过构造方法" class="headerlink" title="通过构造方法"></a>通过构造方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class MyThreadPrint extends Thread&#123;</span><br><span class="line"></span><br><span class="line">    private String name;</span><br><span class="line">    public MyThreadPrint(String name)&#123;</span><br><span class="line">      this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void run()&#123;</span><br><span class="line">        System.out.println(&quot;hello&quot;+name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String [] args)&#123;</span><br><span class="line"></span><br><span class="line">      Thread thread = new MyThreadPrint(&quot;world&quot;);</span><br><span class="line">      thread.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="通过变量与方法"><a href="#通过变量与方法" class="headerlink" title="通过变量与方法"></a>通过变量与方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public class MyThreadPrint implements Runnable&#123;</span><br><span class="line"></span><br><span class="line">    private String name;</span><br><span class="line"></span><br><span class="line">    public void setName(String name)&#123;</span><br><span class="line">      this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void run()&#123;</span><br><span class="line">        System.out.println(&quot;hello&quot;+name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String [] args)&#123;</span><br><span class="line"></span><br><span class="line">      MyThreadPrint mythread = new MyThreadPrint();</span><br><span class="line">      mythread.setName(&quot;world&quot;);</span><br><span class="line">      Thread thread = new Thread(mythread);</span><br><span class="line">      thread.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="waiting-and-notifying"><a href="#waiting-and-notifying" class="headerlink" title="waiting and notifying"></a>waiting and notifying</h3><p>等待与通知机制</p>
<p>notify() 通知一个在对象上等待的线程，使其从wait()方法返回，返回的前提是该线程获得了对象的锁。</p>
<p>wait()调用该方法线程进入WAITING状态，只有等待另外线程的通知或者被中断才会返回，调用wait()后会释放对象的锁。</p>
<h3 id="Thread-join"><a href="#Thread-join" class="headerlink" title="Thread.join()"></a>Thread.join()</h3><p>每个线程终止的前提是前驱线程的终止，每个线程等待前驱线程终止后，才从join()方法返回</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class Join &#123;</span><br><span class="line">  public static void main(String[]args)throws Exception&#123;</span><br><span class="line"></span><br><span class="line">      Thread previous = Thread.currentThread();</span><br><span class="line">      for(int i=0;i&lt;10;i++) &#123;</span><br><span class="line">        Thread thread = new Thread(new Domino(previous), String.valueOf(i));</span><br><span class="line">        thread.start();</span><br><span class="line">        previous = thread;</span><br><span class="line">      &#125;</span><br><span class="line">      TimeUnit.SECONDS.sleep(5);</span><br><span class="line">      System.out.println(Thread.currentThread().getName()+&quot; terminate.&quot;);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  static class Domino implements Runnable&#123;</span><br><span class="line">    private Thread thread;</span><br><span class="line">    public Domino(Thread thread)&#123;</span><br><span class="line">        this.thread = thread;</span><br><span class="line">    &#125;</span><br><span class="line">    public void run()&#123;</span><br><span class="line">        try&#123;</span><br><span class="line">          thread.join();</span><br><span class="line">        &#125;catch (InterruptedException e)&#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(Thread.currentThread().getName()+&quot; terminate.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/26/DL-C2wk2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/26/DL-C2wk2/" itemprop="url">DL C2wk2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-26T20:20:00+08:00">
                2018-12-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="mini-batch-gradient-descent"><a href="#mini-batch-gradient-descent" class="headerlink" title="mini batch gradient descent"></a>mini batch gradient descent</h3><h4 id="mini-batch-size"><a href="#mini-batch-size" class="headerlink" title="mini-batch size"></a>mini-batch size</h4><ul>
<li>if mini-batch size = m （m=sample size）</li>
</ul>
<p>(X^[i],Y^[i]) = (X,Y)</p>
<p>收敛速度会很快（步长很大）</p>
<p>end up with batch gradient descent,which has to process the whole training set before making progress</p>
<ul>
<li>mini-batch size = 1 又叫做stocastic gradient descent</li>
</ul>
<p>收敛步长会很小，很有可能会不收敛，而且vectorization会很慢</p>
<p>lose the benefits of vectorization</p>
<p>如下图</p>
<p><img src="/home/alex/图片/2018-120.png" alt="filename alrea exists, renamed"></p>
<p>紫色表示batch size=1的收敛，而蓝色表示batch size=m的收敛。</p>
<h4 id="how-to-choose-mini-batch-size"><a href="#how-to-choose-mini-batch-size" class="headerlink" title="how to choose mini-batch size?"></a>how to choose mini-batch size?</h4><p>1.通常用 64,128,256,512大小(power of two)</p>
<p><img src="/home/alex/图片/sto.png" alt="upload sucsful"></p>
<p><img src="/home/alex/图片/update.png" alt="upload sessful"></p>
<p><img src="/home/alex/图片/partition.png" alt="upload ssful"></p>
<h3 id="learning-rate-decay"><a href="#learning-rate-decay" class="headerlink" title="learning rate decay"></a>learning rate decay</h3><p><img src="/home/alex/图片/learning rate decay.png" alt="upload essful"></p>
<p>当梯度下降的时候，由于学习率是固定的，因此可能会在最低点附近徘徊而最终不能收敛。</p>
<h4 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a>implementation</h4><p><img src="/home/alex/图片/suppose.png" alt="upload essful"></p>
<p>alpha = 1/（1+decay_rate x epoch_num） *alpha</p>
<p>其他方法也可：</p>
<p><img src="/home/alex/图片/qita.png" alt="upload succeful"></p>
<h3 id="mini-Batch-gradient-descent"><a href="#mini-Batch-gradient-descent" class="headerlink" title="mini-Batch gradient descent"></a>mini-Batch gradient descent</h3><h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h4><p><img src="/home/alex/图片/miniBatch.png" alt="upload sful"></p>
<h4 id="parition"><a href="#parition" class="headerlink" title="parition"></a>parition</h4><p><img src="/home/alex/图片/par.png" alt="load successful"></p>
<h4 id="code"><a href="#code" class="headerlink" title="code"></a>code</h4><p>注意最后一个batch_size有可能和前面的size不同，因为样本总数可能不等于batch_size的倍数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a list of random minibatches from (X, Y)</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input data, of shape (input size, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span><br><span class="line">    mini_batch_size -- size of the mini-batches, integer</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(seed)            # To make your &quot;random&quot; minibatches the same as ours</span><br><span class="line">    m = X.shape[1]                  # number of training examples</span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    # Step 1: Shuffle (X, Y)</span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((1,m))</span><br><span class="line"></span><br><span class="line">    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning</span><br><span class="line">    for k in range(0, num_complete_minibatches):</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        mini_batch_X = shuffled_X[:,mini_batch_size*(k):mini_batch_size*(k+1)]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,mini_batch_size*(k):mini_batch_size*(k+1)]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    # Handling the end case (last mini-batch &lt; mini_batch_size)</span><br><span class="line">    if m % mini_batch_size != 0:</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        mini_batch_X = shuffled_X[:,mini_batch_size*(num_complete_minibatches):]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,mini_batch_size*(num_complete_minibatches):]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    return mini_batches</span><br></pre></td></tr></table></figure>
<h3 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h3><p><img src="/home/alex/图片/2018-121.png" alt="filename already exists, named"></p>
<p>蓝色是gradient的方向，而红色是实际velocity的方向，我们让gradient影响velocty下降的方向</p>
<h4 id="code-1"><a href="#code-1" class="headerlink" title="code"></a>code</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def initialize_velocity(parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Initializes the velocity as a python dictionary with:</span><br><span class="line">                - keys: &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span><br><span class="line">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters.</span><br><span class="line">                    parameters[&apos;W&apos; + str(l)] = Wl</span><br><span class="line">                    parameters[&apos;b&apos; + str(l)] = bl</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    v -- python dictionary containing the current velocity.</span><br><span class="line">                    v[&apos;dW&apos; + str(l)] = velocity of dWl</span><br><span class="line">                    v[&apos;db&apos; + str(l)] = velocity of dbl</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize velocity</span><br><span class="line">    for l in range(L):</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = np.zeros(parameters[&apos;W&apos;+str(l+1)].shape)</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = np.zeros(parameters[&apos;b&apos;+str(l+1)].shape)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    return v</span><br></pre></td></tr></table></figure>
<h4 id="update-parameters"><a href="#update-parameters" class="headerlink" title="update parameters"></a>update parameters</h4><p><img src="/home/alex/图片/2018-122.png" alt="filename already exists, remed"></p>
<p><img src="/home/alex/图片/zhanlang.png" alt="upload succesul"></p>
<h3 id="Adam-optimization"><a href="#Adam-optimization" class="headerlink" title="Adam optimization"></a>Adam optimization</h3><p><img src="/home/alex/图片/adam.png" alt="upload succeul"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def initialize_adam(parameters) :</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Initializes v and s as two python dictionaries with:</span><br><span class="line">                - keys: &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span><br><span class="line">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters.</span><br><span class="line">                    parameters[&quot;W&quot; + str(l)] = Wl</span><br><span class="line">                    parameters[&quot;b&quot; + str(l)] = bl</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span><br><span class="line">                    v[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">                    v[&quot;db&quot; + str(l)] = ...</span><br><span class="line">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span><br><span class="line">                    s[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">                    s[&quot;db&quot; + str(l)] = ...</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize v, s. Input: &quot;parameters&quot;. Outputs: &quot;v, s&quot;.</span><br><span class="line">    for l in range(L):</span><br><span class="line">    ### START CODE HERE ### (approx. 4 lines)</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = np.zeros_like(parameters[&apos;W&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;W&quot; + str(l+1)])</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = np.zeros_like(parameters[&apos;b&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;b&quot; + str(l+1)])</span><br><span class="line">        s[&quot;dW&quot; + str(l+1)] = np.zeros_like(parameters[&apos;W&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;W&quot; + str(l+1)])</span><br><span class="line">        s[&quot;db&quot; + str(l+1)] = np.zeros_like(parameters[&apos;b&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;b&quot; + str(l+1)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return v, s</span><br></pre></td></tr></table></figure>
<h4 id="update-parameters-1"><a href="#update-parameters-1" class="headerlink" title="update parameters"></a>update parameters</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,</span><br><span class="line">                                beta1=0.9, beta2=0.999, epsilon=1e-8):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Update parameters using Adam</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters:</span><br><span class="line">                    parameters[&apos;W&apos; + str(l)] = Wl</span><br><span class="line">                    parameters[&apos;b&apos; + str(l)] = bl</span><br><span class="line">    grads -- python dictionary containing your gradients for each parameters:</span><br><span class="line">                    grads[&apos;dW&apos; + str(l)] = dWl</span><br><span class="line">                    grads[&apos;db&apos; + str(l)] = dbl</span><br><span class="line">    v -- Adam variable, moving average of the first gradient, python dictionary</span><br><span class="line">    s -- Adam variable, moving average of the squared gradient, python dictionary</span><br><span class="line">    learning_rate -- the learning rate, scalar.</span><br><span class="line">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span><br><span class="line">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span><br><span class="line">    epsilon -- hyperparameter preventing division by zero in Adam updates</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">    v -- Adam variable, moving average of the first gradient, python dictionary</span><br><span class="line">    s -- Adam variable, moving average of the squared gradient, python dictionary</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2                 # number of layers in the neural networks</span><br><span class="line">    v_corrected = &#123;&#125;                         # Initializing first moment estimate, python dictionary</span><br><span class="line">    s_corrected = &#123;&#125;                         # Initializing second moment estimate, python dictionary</span><br><span class="line">    </span><br><span class="line">    # Perform Adam update on all parameters</span><br><span class="line">    for l in range(L):</span><br><span class="line">        # Moving average of the gradients. Inputs: &quot;v, grads, beta1&quot;. Output: &quot;v&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        v[&apos;dW&apos;+str(l+1)] = beta1*v[&apos;dW&apos;+str(l+1)]+(1-beta1)*grads[&apos;dW&apos;+str(l+1)]</span><br><span class="line">        v[&apos;db&apos;+str(l+1)] = beta1*v[&apos;db&apos;+str(l+1)]+(1-beta1)*grads[&apos;db&apos;+str(l+1)]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected first moment estimate. Inputs: &quot;v, beta1, t&quot;. Output: &quot;v_corrected&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        v_corrected[&apos;dW&apos;+str(l+1)] = v[&apos;dW&apos;+str(l+1)]/(1-np.power(beta1,t))</span><br><span class="line">        v_corrected[&apos;db&apos;+str(l+1)] = v[&apos;db&apos;+str(l+1)]/(1-np.power(beta1,t))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Moving average of the squared gradients. Inputs: &quot;s, grads, beta2&quot;. Output: &quot;s&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        s[&apos;dW&apos;+str(l+1)] = beta2*s[&apos;dW&apos;+str(l+1)]+(1-beta2)*np.power(grads[&apos;dW&apos;+str(l+1)],2)</span><br><span class="line">        s[&apos;db&apos;+str(l+1)] = beta2*s[&apos;db&apos;+str(l+1)]+(1-beta2)*np.power(grads[&apos;db&apos;+str(l+1)],2)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected second raw moment estimate. Inputs: &quot;s, beta2, t&quot;. Output: &quot;s_corrected&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        s_corrected[&apos;dW&apos;+str(l+1)] = s[&apos;dW&apos;+str(l+1)]/(1-np.power(beta2,t))</span><br><span class="line">        s_corrected[&apos;db&apos;+str(l+1)] = s[&apos;db&apos;+str(l+1)]/(1-np.power(beta2,t))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Update parameters. Inputs: &quot;parameters, learning_rate, v_corrected, s_corrected, epsilon&quot;. Output: &quot;parameters&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        parameters[&apos;W&apos;+str(l+1)] -= learning_rate * v_corrected[&apos;dW&apos;+str(l+1)] /(epsilon+np.sqrt(s_corrected[&apos;dW&apos;+str(l+1)]))</span><br><span class="line">        parameters[&apos;b&apos;+str(l+1)] -= learning_rate * v_corrected[&apos;db&apos;+str(l+1)] /(epsilon+np.sqrt(s_corrected[&apos;db&apos;+str(l+1)]))</span><br><span class="line">        </span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    return parameters, v, s</span><br></pre></td></tr></table></figure>
<h3 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h3><p>注意每次epoch的时候，分为多个batch学习参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,</span><br><span class="line">          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    3-layer neural network model which can be run in different optimizer modes.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input data, of shape (2, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span><br><span class="line">    layers_dims -- python list, containing the size of each layer</span><br><span class="line">    learning_rate -- the learning rate, scalar.</span><br><span class="line">    mini_batch_size -- the size of a mini batch</span><br><span class="line">    beta -- Momentum hyperparameter</span><br><span class="line">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span><br><span class="line">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span><br><span class="line">    epsilon -- hyperparameter preventing division by zero in Adam updates</span><br><span class="line">    num_epochs -- number of epochs</span><br><span class="line">    print_cost -- True to print the cost every 1000 epochs</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             # number of layers in the neural networks</span><br><span class="line">    costs = []                       # to keep track of the cost</span><br><span class="line">    t = 0                            # initializing the counter required for Adam update</span><br><span class="line">    seed = 10                        # For grading purposes, so that your &quot;random&quot; minibatches are the same as ours</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters</span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    # Initialize the optimizer</span><br><span class="line">    if optimizer == &quot;gd&quot;:</span><br><span class="line">        pass # no initialization required for gradient descent</span><br><span class="line">    elif optimizer == &quot;momentum&quot;:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    elif optimizer == &quot;adam&quot;:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line">    </span><br><span class="line">    # Optimization loop</span><br><span class="line">    for i in range(num_epochs):</span><br><span class="line">        </span><br><span class="line">        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span><br><span class="line">        seed = seed + 1</span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        for minibatch in minibatches:</span><br><span class="line"></span><br><span class="line">            # Select a minibatch</span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            # Forward propagation</span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            # Compute cost</span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            # Backward propagation</span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            # Update parameters</span><br><span class="line">            if optimizer == &quot;gd&quot;:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            elif optimizer == &quot;momentum&quot;:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            elif optimizer == &quot;adam&quot;:</span><br><span class="line">                t = t + 1 # Adam counter</span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line">                                                               t, learning_rate, beta1, beta2,  epsilon)</span><br><span class="line">        </span><br><span class="line">        # Print the cost every 1000 epoch</span><br><span class="line">        if print_cost and i % 1000 == 0:</span><br><span class="line">            print(&quot;Cost after epoch %i: %f&quot; % (i, cost))</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;epochs (per 100)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate = &quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/26/DL-wk2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/26/DL-wk2/" itemprop="url">DL C2wk1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-26T13:57:43+08:00">
                2018-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Train-dev-test-sets"><a href="#Train-dev-test-sets" class="headerlink" title="Train/dev/test sets"></a>Train/dev/test sets</h3><p>dev sets,also called hold-out sets, are used to decide the model’s performance.(give you an unbiased estimate of your model’s performance)</p>
<h4 id="example"><a href="#example" class="headerlink" title="example:"></a>example:</h4><p>If you have 10,000,000 examples, how would you split the train/dev/test set?</p>
<pre><code>98% train . 1% dev . 1% test
</code></pre><p>The dev and test set should:</p>
<pre><code>Come from the same distribution
</code></pre><h3 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h3><p><img src="/home/alex/图片/biad.png" alt="upload succesul"></p>
<p>underfitting -&gt; high bias</p>
<p>overfitting -&gt; high variance</p>
<h4 id="example-1"><a href="#example-1" class="headerlink" title="example"></a>example</h4><p><img src="/home/alex/图片/2018-105.png" alt="filename aady exists, renamed"></p>
<p>1.如果train error =1%,dev set error = 11%</p>
<p>则overfitting，说明是high variance</p>
<p>2.如果 train error都很大的话，说明是high bias</p>
<p>3.总的来说，如果dev set error 比train set error<br>大很多，可以说明是overfitting</p>
<h4 id="high-bias-and-high-variance"><a href="#high-bias-and-high-variance" class="headerlink" title="high bias and high variance"></a>high bias and high variance</h4><p><img src="/home/alex/图片/high.png" alt="upload essful"></p>
<p>部分数据过拟合，部分欠拟合</p>
<h3 id="basic-recipe-for-ML"><a href="#basic-recipe-for-ML" class="headerlink" title="basic recipe for ML"></a>basic recipe for ML</h3><p>如果high bias咋办</p>
<p>High bias （欠拟合）<br>(training data performance)</p>
<ul>
<li>1.bigger network</li>
<li>2.optimized neural network architecture</li>
<li>3.other optimizing techniques</li>
</ul>
<p>High Variance? （过拟合）<br>(dev data performance)</p>
<ul>
<li>1.more data</li>
<li>2.regularization</li>
</ul>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p><img src="/home/alex/图片/2018-106.png" alt="filename alrea exists, renamed"></p>
<p>在L1正则化中,w会是一个很sparse的向量，通常在实际应用中L2正则化会应用的更为广泛。</p>
<p>λ 又叫正则化参数</p>
<h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p><img src="/home/alex/图片/2018-107.png" alt="filename alrea exists, renamed"></p>
<h4 id="how-does-it-work"><a href="#how-does-it-work" class="headerlink" title="how does it work?"></a>how does it work?</h4><p>由上面推导我们可知，如果λ越大，那么w会越接近于0，那么以下图的激活函数为例，如果z很小的时候，tanh结果会接近于线性的，，神经网络每一层都将近似于一个线性神经元，那么就可以有效解决过拟合问题，往”欠拟合”或者刚好的方向。</p>
<p><img src="/home/alex/图片/linear.png" alt="upload successful"></p>
<h3 id="drop-out"><a href="#drop-out" class="headerlink" title="drop out"></a>drop out</h3><p>let’s say a nn with layer l = 3,keep_prob = 0.8</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d3= np.random.rand(a3.shape[0],a3.shape[1])&lt;keep_prob</span><br></pre></td></tr></table></figure>
<p>80%的unit会被保留，20%会被drop out</p>
<p>上面语句作用是d380%元素为1,20%元素为0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 = np.multiply(a3,d3)</span><br></pre></td></tr></table></figure>
<p>然后再scale up</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3/= keep_prob</span><br></pre></td></tr></table></figure>
<h4 id="why-does-it-work"><a href="#why-does-it-work" class="headerlink" title="why does it work?"></a>why does it work?</h4><p><img src="/home/alex/图片/why.png" alt="upload succeful"></p>
<p>1.在一些神经元很多的层，设置keep_probs低一点，可以有效减少过拟合，实际上是减弱regularization 的作用，在一些神经元很少的层，设置为1.0就好</p>
<p>2.在CV领域，由于输入数据的维数通常很大，一般都需要drop out</p>
<p>3.但是drop out会导致不能够通过画出cost function曲线来debug，解决方法是最开始先把所有keep_prob set to 1,then if no bug, turn on drop out</p>
<h3 id="other-technique-of-reducing-overfitting"><a href="#other-technique-of-reducing-overfitting" class="headerlink" title="other technique of reducing overfitting"></a>other technique of reducing overfitting</h3><h4 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h4><p><img src="/home/alex/图片/2018-108.png" alt="filena already exists, renamed"></p>
<p>防止dev set error增加，采取early stopping,最后会得到一个middle-size的||w||^2</p>
<h4 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h4><p><img src="/home/alex/图片/2018-109.png" alt="filename alre exists, renamed"></p>
<p>通过变换现有的数据集，来获得更多的数据集</p>
<h3 id="normalizing-inputs"><a href="#normalizing-inputs" class="headerlink" title="normalizing inputs"></a>normalizing inputs</h3><p><img src="/home/alex/图片/2018-110.png" alt="filename alrea exists, renamed"></p>
<p>by subtract the mean and scalling the variance</p>
<p><img src="/home/alex/图片/2018-111.png" alt="filename exists, renamed"></p>
<p>效果就是会加快optimizing 的速度</p>
<h3 id="Vanishing-gradients"><a href="#Vanishing-gradients" class="headerlink" title="Vanishing gradients"></a>Vanishing gradients</h3><p>很深层的神经网络，权重相乘累积起来的话后果很严重</p>
<p><img src="/home/alex/图片/2018-113.png" alt="filename already exists, renmed"></p>
<p>参数初始化方法：</p>
<p>这样可以使得w的值接近于1，不会导致梯度消失和梯度爆炸</p>
<p><img src="/home/alex/图片/single.png" alt="upload cessful"></p>
<p>其中的Xavior initialization</p>
<p>可以用来保证输入输出数据的分布相近，加快收敛速度（方差与均值大概相同）</p>
<p><a href="https://blog.csdn.net/shuzfan/article/details/51338178" target="_blank" rel="noopener">https://blog.csdn.net/shuzfan/article/details/51338178</a></p>
<h3 id="gradient-checking"><a href="#gradient-checking" class="headerlink" title="gradient checking"></a>gradient checking</h3><p><img src="/home/alex/图片/gradient.png" alt="upload ccessful"></p>
<h4 id="grad-check"><a href="#grad-check" class="headerlink" title="grad check"></a>grad check</h4><p><img src="/home/alex/图片/gradcheck.png" alt="upload essful"></p>
<h4 id="notes"><a href="#notes" class="headerlink" title="notes"></a>notes</h4><p>1.no use in training ,only to debug</p>
<p>2.if fails grad check,look at components to try to identify bug.</p>
<p>3.remember regularization</p>
<p>4.doesn’t work with dropout</p>
<p>5.run at random initialization </p>
<h3 id="initialization"><a href="#initialization" class="headerlink" title="initialization"></a>initialization</h3><h4 id="zero-initialization"><a href="#zero-initialization" class="headerlink" title="zero initialization"></a>zero initialization</h4><p>如果把W矩阵初始化为0的话，相当于在训练一个各层只有一个神经元的神经网络，因为每一层的每个神经元其实都在学习相同的参数。</p>
<p>这时候神经网络只相当于一个线性分类器。</p>
<p>但是bias可以设置处值为0</p>
<h4 id="large-random-initialization"><a href="#large-random-initialization" class="headerlink" title="large random initialization"></a>large random initialization</h4><p>1.poor initialization can lead to vanishing/exploding gradients</p>
<p>2.如果一开始w初值非常大，梯度下降所花时间会很长，（需要更多迭代次数）</p>
<h4 id="He-random-initialization"><a href="#He-random-initialization" class="headerlink" title="He random initialization"></a>He random initialization</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def initialize_parameters_he(layers_dims):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    layer_dims -- python array (list) containing the size of each layer.</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span><br><span class="line">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span><br><span class="line">                    b1 -- bias vector of shape (layers_dims[1], 1)</span><br><span class="line">                    ...</span><br><span class="line">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span><br><span class="line">                    bL -- bias vector of shape (layers_dims[L], 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - 1 # integer representing the number of layers</span><br><span class="line">     </span><br><span class="line">    for l in range(1, L + 1):</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        parameters[&apos;W&apos; + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * (np.sqrt(2. / layers_dims[l-1]))</span><br><span class="line">        parameters[&apos;b&apos; + str(l)] = np.zeros((layers_dims[l], 1))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>
<h4 id="xavier-random-initialization"><a href="#xavier-random-initialization" class="headerlink" title="xavier random initialization"></a>xavier random initialization</h4><p>只是把 sqrt（2./layers_dims[l-1]） 换作sqrt（1./layers_dims[l-1]）</p>
<h3 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h3><p><img src="/home/alex/图片/2018-114.png" alt="flename already exists, renamed"></p>
<p>同时code如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def compute_cost_with_regularization(A3, Y, parameters, lambd):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the cost function with L2 regularization. See formula (2) above.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span><br><span class="line">    parameters -- python dictionary containing parameters of the model</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    cost - value of the regularized loss function (formula (2))</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    m = Y.shape[1]</span><br><span class="line">    W1 = parameters[&quot;W1&quot;]</span><br><span class="line">    W2 = parameters[&quot;W2&quot;]</span><br><span class="line">    W3 = parameters[&quot;W3&quot;]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    sumW1 = np.sum(np.square(W1))</span><br><span class="line">    sumW2 = np.sum(np.square(W2))</span><br><span class="line">    sumW3 = np.sum(np.square(W3))</span><br><span class="line">    L2_regularization_cost = (0.5/m*lambd)*(sumW1+sumW2+sumW3)</span><br><span class="line">    ### END CODER HERE ###</span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure>
<p>同时引入正则化的话，在backword propa的时候，要加上正则项</p>
<p><img src="/home/alex/图片/www.png" alt="upload sussful"></p>
<p>code<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def backward_propagation_with_regularization(X, Y, cache, lambd):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (input size, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span><br><span class="line">    cache -- cache output from forward_propagation()</span><br><span class="line">    lambd -- regularization hyperparameter, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd/m)*W3</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd/m)*W2</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    dW1 = 1./m * np.dot(dZ1, X.T) + (lambd/m)*W1</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dZ3&quot;: dZ3, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3,&quot;dA2&quot;: dA2,</span><br><span class="line">                 &quot;dZ2&quot;: dZ2, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dA1&quot;: dA1, </span><br><span class="line">                 &quot;dZ1&quot;: dZ1, &quot;dW1&quot;: dW1, &quot;db1&quot;: db1&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure></p>
<p>lambd = 0.6</p>
<p><img src="/home/alex/图片/2018-115.png" alt="filename al exists, renamed"></p>
<p>lambd = 0.7</p>
<p><img src="/home/alex/图片/0.8.png" alt="upload succsful"></p>
<p>lambd = 0.8</p>
<p><img src="/home/alex/图片/2018-116.png" alt="filename already exists,amed"></p>
<p>λ增大，能够减少过拟合现象，但是training set error 也会随之减少</p>
<h3 id="drop-out-1"><a href="#drop-out-1" class="headerlink" title="drop out"></a>drop out</h3><h4 id="what-is-inverted-dropout"><a href="#what-is-inverted-dropout" class="headerlink" title="what is inverted dropout?"></a>what is inverted dropout?</h4><p><img src="/home/alex/图片/dropout.png" alt="upload cessful"></p>
<p><img src="/home/alex/图片/inve.png" alt="upload ful"></p>
<h4 id="drop-out-2"><a href="#drop-out-2" class="headerlink" title="drop out"></a>drop out</h4><p><img src="/home/alex/图片/drop.png" alt="upload successul"></p>
<p>1.创建一个np.array D1 which has the same size as A1（np.random.randn(A.shape[0],A.shape[1])）</p>
<p>2.当A的元素&lt;D[keep_prob]的时候为1，大于的时候为0</p>
<p>3.A = np.multiply(A,D)</p>
<p>4.scale A,i.e. A/=keep_prob (inverted dropout)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">def forward_propagation_with_dropout(X, parameters, keep_prob=0.5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (2, number of examples)</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:</span><br><span class="line">                    W1 -- weight matrix of shape (20, 2)</span><br><span class="line">                    b1 -- bias vector of shape (20, 1)</span><br><span class="line">                    W2 -- weight matrix of shape (3, 20)</span><br><span class="line">                    b2 -- bias vector of shape (3, 1)</span><br><span class="line">                    W3 -- weight matrix of shape (1, 3)</span><br><span class="line">                    b3 -- bias vector of shape (1, 1)</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span><br><span class="line">    cache -- tuple, information stored for computing the backward propagation</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(1)</span><br><span class="line">    </span><br><span class="line">    # retrieve parameters</span><br><span class="line">    W1 = parameters[&quot;W1&quot;]</span><br><span class="line">    b1 = parameters[&quot;b1&quot;]</span><br><span class="line">    W2 = parameters[&quot;W2&quot;]</span><br><span class="line">    b2 = parameters[&quot;b2&quot;]</span><br><span class="line">    W3 = parameters[&quot;W3&quot;]</span><br><span class="line">    b3 = parameters[&quot;b3&quot;]</span><br><span class="line">    </span><br><span class="line">    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span><br><span class="line">    D1 = np.random.rand(A1.shape[0], A1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)</span><br><span class="line">    D1 = D1 &lt; keep_prob                            # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span><br><span class="line">    A1 = A1 * D1                                      # Step 3: shut down some neurons of A1</span><br><span class="line">    A1 = A1 / keep_prob                               # Step 4: scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    ### START CODE HERE ### (approx. 4 lines)</span><br><span class="line">    D2 = np.random.rand(A2.shape[0], A2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)</span><br><span class="line">    D2 = D2 &lt; keep_prob                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)                           </span><br><span class="line">    A2 = A2 * D2                                      # Step 3: shut down some neurons of A2</span><br><span class="line">    A2 = A2 / keep_prob                               # Step 4: scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    return A3, cache</span><br></pre></td></tr></table></figure>
<h4 id="drop-out-in-backward"><a href="#drop-out-in-backward" class="headerlink" title="drop out in backward"></a>drop out in backward</h4><p>just perform dA2*=D2 and scaling</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def backward_propagation_with_dropout(X, Y, cache, keep_prob):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the backward propagation of our baseline model to which we added dropout.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (2, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span><br><span class="line">    cache -- cache output from forward_propagation_with_dropout()</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = 1. / m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA2 = dA2 / keep_prob              # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    dW2 = 1. / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA1 = dA1 / keep_prob              # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    dW1 = 1. / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dZ3&quot;: dZ3, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3,&quot;dA2&quot;: dA2,</span><br><span class="line">                 &quot;dZ2&quot;: dZ2, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dA1&quot;: dA1, </span><br><span class="line">                 &quot;dZ1&quot;: dZ1, &quot;dW1&quot;: dW1, &quot;db1&quot;: db1&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure>
<h3 id="gradient-checking-1"><a href="#gradient-checking-1" class="headerlink" title="gradient checking"></a>gradient checking</h3><p>以J = x*theta 为例</p>
<p><img src="/home/alex/图片/theta.png" alt="upload succesul"></p>
<p>implementation</p>
<p><img src="/home/alex/图片/imple.png" alt="upload succeful"></p>
<p>注意<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.norm 是求范数的函数，默认是求二范数</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def gradient_check(x, theta, epsilon = 1e-7):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation presented in Figure 1.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    x -- a real-valued input</span><br><span class="line">    theta -- our parameter, a real number as well</span><br><span class="line">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don&apos;t need to worry about the limit.</span><br><span class="line">    ### START CODE HERE ### (approx. 5 lines)</span><br><span class="line">    thetaPlus = theta+epsilon</span><br><span class="line">    thetaMinus = theta-epsilon</span><br><span class="line">    JPlus = forward_propagation(x,thetaPlus)</span><br><span class="line">    JMinus = forward_propagation(x,thetaMinus)</span><br><span class="line">    gradapprox = (JPlus-JMinus)/(2*epsilon)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Check if gradapprox is close enough to the output of backward_propagation()</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    grad = backward_propagation(x, gradapprox)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox)</span><br><span class="line">    demurator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)</span><br><span class="line">    difference = numerator/demurator</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    if difference &lt; 1e-7:</span><br><span class="line">        print (&quot;The gradient is correct!&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print (&quot;The gradient is wrong!&quot;)</span><br><span class="line">    </span><br><span class="line">    return difference</span><br></pre></td></tr></table></figure>
<p>对于多维的情况</p>
<p><img src="/home/alex/图片/dimension.png" alt="upload success"></p>
<p>把所有params压缩到一个向量，然后一个for循环，计算每个参数的grad，gradapprox，并加入到一个向量当中，分别得到一个gradapprox与grad向量，再利用这两个向量求范数，求difference</p>
<p><img src="/home/alex/图片/2018-117.png" alt="filename already exists, reamed"></p>
<p>code<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:</span><br><span class="line">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. </span><br><span class="line">    x -- input datapoint, of shape (input size, 1)</span><br><span class="line">    y -- true &quot;label&quot;</span><br><span class="line">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Set-up variables</span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[0]</span><br><span class="line">    J_plus = np.zeros((num_parameters, 1))</span><br><span class="line">    J_minus = np.zeros((num_parameters, 1))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, 1))</span><br><span class="line">    </span><br><span class="line">    # Compute gradapprox</span><br><span class="line">    for i in range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        # Compute J_plus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_plus[i]&quot;.</span><br><span class="line">        # &quot;_&quot; is used because the function you have to outputs two parameters but we only care about the first one</span><br><span class="line">        ### START CODE HERE ### (approx. 3 lines)</span><br><span class="line">        thetaplus =  np.copy(parameters_values)                                       # Step 1</span><br><span class="line">        thetaplus[i][0] = thetaplus[i][0] + epsilon                                   # Step 2</span><br><span class="line">        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  # Step 3</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Compute J_minus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_minus[i]&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 3 lines)</span><br><span class="line">        thetaminus = np.copy(parameters_values)                                       # Step 1</span><br><span class="line">        thetaminus[i][0] = thetaminus[i][0] - epsilon                                 # Step 2        </span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) # Step 3</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Compute gradapprox[i]</span><br><span class="line">        ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Compare gradapprox to backward propagation gradients by computing difference.</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                                     # Step 1&apos;</span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   # Step 2&apos;</span><br><span class="line">    difference = numerator / denominator                                              # Step 3&apos;</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    if difference &gt; 1e-7:</span><br><span class="line">        print(&quot;\033[93m&quot; + &quot;There is a mistake in the backward propagation! difference = &quot; + str(difference) + &quot;\033[0m&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;\033[92m&quot; + &quot;Your backward propagation works perfectly fine! difference = &quot; + str(difference) + &quot;\033[0m&quot;)</span><br><span class="line">    </span><br><span class="line">    return difference</span><br></pre></td></tr></table></figure></p>
<h3 id="concolusion"><a href="#concolusion" class="headerlink" title="concolusion"></a>concolusion</h3><p>1.L2正则化和drop out都可以帮你解决overfitting</p>
<p>2.regularization 会使得weight变得非常小</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/24/CS230-深层神经网络图像分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/24/CS230-深层神经网络图像分类/" itemprop="url">CS230-深层神经网络图像分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-24T16:58:35+08:00">
                2018-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p><img src="/home/alex/图片/2018-102.png" alt="filename eady exists, renamed"></p>
<p>每一张图片都可以把它转化为一个m*1的向量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Reshape the training and test examples </span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The &quot;-1&quot; makes reshape flatten the remaining dimensions</span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T</span><br><span class="line"></span><br><span class="line"># Standardize data to have feature values between 0 and 1.</span><br><span class="line">train_x = train_x_flatten/255.</span><br><span class="line">test_x = test_x_flatten/255.</span><br><span class="line"></span><br><span class="line">print (&quot;train_x&apos;s shape: &quot; + str(train_x.shape))</span><br><span class="line">print (&quot;test_x&apos;s shape: &quot; + str(test_x.shape))</span><br></pre></td></tr></table></figure>
<p><img src="/home/alex/图片/what.png" alt="upload sussful"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span><br><span class="line">    Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span><br><span class="line">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span><br><span class="line">    learning_rate -- learning rate of the gradient descent update rule</span><br><span class="line">    num_iterations -- number of iterations of the optimization loop</span><br><span class="line">    print_cost -- if True, it prints the cost every 100 steps</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- parameters learnt by the model. They can then be used to predict.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    np.random.seed(1)</span><br><span class="line">    costs = []                         # keep track of cost</span><br><span class="line">    </span><br><span class="line">    # Parameters initialization.</span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line">    for i in range(0, num_iterations):</span><br><span class="line"></span><br><span class="line">        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Compute cost.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">        # Backward propagation.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"> </span><br><span class="line">        # Update parameters.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">                </span><br><span class="line">        # Print the cost every 100 training example</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;iterations (per tens)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate =&quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/24/CS230-搭建深层的神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/24/CS230-搭建深层的神经网络/" itemprop="url">CS230-搭建深层的神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-24T15:18:11+08:00">
                2018-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="参数的初始化"><a href="#参数的初始化" class="headerlink" title="参数的初始化"></a>参数的初始化</h3><p><img src="/home/alex/图片/搭建.png" alt="upload succeful"></p>
<p>搭建深层的神经网络，各层W,X,B的维度一定要搞清楚</p>
<p>如上图所示，第i层W的维度为（n^[i]，n^[i-1]）</p>
<p>其中12288是特征数量，209是样本数，n^[i]是第i层神经元的数量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">代码如下</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def initialize_parameters_deep(layer_dims):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span><br><span class="line">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span><br><span class="line">                    bl -- bias vector of shape (layer_dims[l], 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            # number of layers in the network</span><br><span class="line"></span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        parameters[&apos;W&apos;+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01</span><br><span class="line">        parameters[&apos;b&apos;+str(l)] = np.zeros((layer_dims[l],1))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        assert(parameters[&apos;W&apos; + str(l)].shape == (layer_dims[l], layer_dims[l-1]))</span><br><span class="line">        assert(parameters[&apos;b&apos; + str(l)].shape == (layer_dims[l], 1))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>
<h3 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h3><p><img src="/home/alex/图片/实现.png" alt="upload successl"></p>
<p>实现Relu或者sigmoid激活函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid(Z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the sigmoid activation in numpy</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    Z -- numpy array of any shape</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    A -- output of sigmoid(z), same shape as Z</span><br><span class="line">    cache -- returns Z as well, useful during backpropagation</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    A = 1/(1+np.exp(-Z))</span><br><span class="line">    cache = Z</span><br><span class="line">    </span><br><span class="line">    return A, cache</span><br><span class="line"></span><br><span class="line">def relu(Z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the RELU function.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    Z -- Output of the linear layer, of any shape</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    A -- Post-activation parameter, of the same shape as Z</span><br><span class="line">    cache -- a python dictionary containing &quot;A&quot; ; stored for computing the backward pass efficiently</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    A = np.maximum(0,Z)</span><br><span class="line">    </span><br><span class="line">    assert(A.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    cache = Z </span><br><span class="line">    return A, cache</span><br></pre></td></tr></table></figure>
<p>以下是linear_activation_forward的函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def linear_activation_forward(A_prev, W, b, activation):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span><br><span class="line">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span><br><span class="line">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span><br><span class="line">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    A -- the output of the activation function, also called the post-activation value </span><br><span class="line">    cache -- a python dictionary containing &quot;linear_cache&quot; and &quot;activation_cache&quot;;</span><br><span class="line">             stored for computing the backward pass efficiently</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    if activation == &quot;sigmoid&quot;:</span><br><span class="line">        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        Z = np.dot(W,A_prev)+b</span><br><span class="line">        A,activation_cache = sigmoid(Z)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    elif activation == &quot;relu&quot;:</span><br><span class="line">        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        Z = np.dot(W,A_prev)+b</span><br><span class="line">        A,activation_cache = relu(Z)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    assert (A.shape == (W.shape[0], A_prev.shape[1]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    return A, cache</span><br></pre></td></tr></table></figure>
<h4 id="对于L层网络的forwarding"><a href="#对于L层网络的forwarding" class="headerlink" title="对于L层网络的forwarding"></a>对于L层网络的forwarding</h4><p><img src="/home/alex/图片/forwarding.png" alt="upload succul"></p>
<p>前L-1层作Relu变换，最后一层做sigmoid变换（由于是二分类）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def L_model_forward(X, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- data, numpy array of shape (input size, number of examples)</span><br><span class="line">    parameters -- output of initialize_parameters_deep()</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    AL -- last post-activation value</span><br><span class="line">    caches -- list of caches containing:</span><br><span class="line">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span><br><span class="line">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // 2                  # number of layers in the neural network</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.</span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        A,cache = linear_activation_forward(A_prev, parameters[&apos;W&apos;+str(l)], parameters[&apos;b&apos;+str(l)], activation=&apos;relu&apos;)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    AL,cache = linear_activation_forward(A, parameters[&apos;W&apos;+str(L)], parameters[&apos;b&apos;+str(L)], activation=&apos;sigmoid&apos;)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    assert(AL.shape == (1,X.shape[1]))</span><br><span class="line">            </span><br><span class="line">    return AL, caches</span><br></pre></td></tr></table></figure>
<p>在这里你得到的AL就是经过L曾训练后的parameters，可以用来测试结果，同时所有中间结果都保存在了caches中。</p>
<h3 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h3><p><img src="/home/alex/图片/computeTheCost.png" alt="upload succel"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def compute_cost(AL, Y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the cost function defined by equation (7).</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    cost -- cross-entropy cost</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m = Y.shape[1]</span><br><span class="line"></span><br><span class="line">    # Compute loss from aL and y.</span><br><span class="line">    ### START CODE HERE ### (≈ 1 lines of code)</span><br><span class="line">    ##attention! here Y,and np.log(AL) is scalar so use multiply rather than dot</span><br><span class="line">    cost = (-1./m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL)))</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      # To make sure your cost&apos;s shape is what we expect (e.g. this turns [[17]] into 17).</span><br><span class="line">    assert(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure>
<h3 id="backward-propagation"><a href="#backward-propagation" class="headerlink" title="backward propagation"></a>backward propagation</h3><p><img src="/home/alex/图片/如图.png" alt="upload succeful"></p>
<p>要通过计算dL/dz 来算出dw,dAprev,db 来进行梯度下降</p>
<p><img src="/home/alex/图片/怎么计算呢.png" alt="upload succesful"></p>
<p>假设已知第l层的dZ，同时我们在做forward propagation的时候把对应该层的A_PREV,W已经存起来了</p>
<h4 id="两个辅助函数"><a href="#两个辅助函数" class="headerlink" title="两个辅助函数"></a>两个辅助函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def relu_backward(dA, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for a single RELU unit.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient, of any shape</span><br><span class="line">    cache -- &apos;Z&apos; where we store for computing backward propagation efficiently</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dZ -- Gradient of the cost with respect to Z</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    dZ = np.array(dA, copy=True) # just converting dz to a correct object.</span><br><span class="line">    </span><br><span class="line">    # When z &lt;= 0, you should set dz to 0 as well. </span><br><span class="line">    dZ[Z &lt;= 0] = 0</span><br><span class="line">    </span><br><span class="line">    assert (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    return dZ</span><br><span class="line"></span><br><span class="line">def sigmoid_backward(dA, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for a single SIGMOID unit.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient, of any shape</span><br><span class="line">    cache -- &apos;Z&apos; where we store for computing backward propagation efficiently</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dZ -- Gradient of the cost with respect to Z</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = 1/(1+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (1-s)</span><br><span class="line">    </span><br><span class="line">    assert (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    return dZ</span><br></pre></td></tr></table></figure>
<h4 id="linear-backward"><a href="#linear-backward" class="headerlink" title="linear backward"></a>linear backward</h4><p>计算公式如上图所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def linear_backward(dZ, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the linear portion of backward propagation for a single layer (layer l)</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span><br><span class="line">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span><br><span class="line">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span><br><span class="line">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[1]</span><br><span class="line"></span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines of code)</span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    dW = (1/m)*np.dot(dZ,A_prev.T)</span><br><span class="line">    db = (1/m)*np.sum(dZ,axis=1,keepdims = True)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    assert (dA_prev.shape == A_prev.shape)</span><br><span class="line">    assert (dW.shape == W.shape)</span><br><span class="line">    assert (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    return dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h4 id="linear-activation-backward"><a href="#linear-activation-backward" class="headerlink" title="linear_activation_backward"></a>linear_activation_backward</h4><p>那么上一个函数的dZ又是怎么计算？</p>
<p><img src="/home/alex/图片/at.png" alt="upload succsful"></p>
<p>我们运用最开始定义的两个辅助函数来计算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def linear_activation_backward(dA, cache, activation):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient for current layer l </span><br><span class="line">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span><br><span class="line">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span><br><span class="line">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span><br><span class="line">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    if activation == &quot;relu&quot;:</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev,dW,db = linear_backward(dZ,linear_cache)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    elif activation == &quot;sigmoid&quot;:</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev,dW,db = linear_backward(dZ,linear_cache)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h3 id="对L层的神经网络的backward"><a href="#对L层的神经网络的backward" class="headerlink" title="对L层的神经网络的backward"></a>对L层的神经网络的backward</h3><p>对最后一层是sigmoid_backward,剩下的所有层是linear_backward</p>
<p><img src="/home/alex/图片/2018-101.png" alt="filename already exists, renaed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: L_model_backward</span><br><span class="line"></span><br><span class="line">def L_model_backward(AL, Y, caches):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    AL -- probability vector, output of the forward propagation (L_model_forward())</span><br><span class="line">    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat)</span><br><span class="line">    caches -- list of caches containing:</span><br><span class="line">                every cache of linear_activation_forward() with &quot;relu&quot; (it&apos;s caches[l], for l in range(L-1) i.e l = 0...L-2)</span><br><span class="line">                the cache of linear_activation_forward() with &quot;sigmoid&quot; (it&apos;s caches[L-1])</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    grads -- A dictionary with the gradients</span><br><span class="line">             grads[&quot;dA&quot; + str(l)] = ... </span><br><span class="line">             grads[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">             grads[&quot;db&quot; + str(l)] = ... </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) # the number of layers</span><br><span class="line">    m = AL.shape[1]</span><br><span class="line">    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL</span><br><span class="line">    </span><br><span class="line">    # Initializing the backpropagation</span><br><span class="line">    ### START CODE HERE ### (1 line of code)</span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;AL, Y, caches&quot;. Outputs: &quot;grads[&quot;dAL&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]</span><br><span class="line">    ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">    current_cache = caches[-1]</span><br><span class="line">    grads[&quot;dA&quot;+str(L)],grads[&quot;dW&quot;+str(L)],grads[&quot;db&quot;+str(L)] = linear_activation_backward(dAL, current_cache, &apos;sigmoid&apos;)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    for l in reversed(range(L-1)):</span><br><span class="line">        # lth layer: (RELU -&gt; LINEAR) gradients.</span><br><span class="line">        # Inputs: &quot;grads[&quot;dA&quot; + str(l + 2)], caches&quot;. Outputs: &quot;grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span><br><span class="line">        ### START CODE HERE ### (approx. 5 lines)</span><br><span class="line">        current_cache = caches[l];</span><br><span class="line">        current_A = grads[&quot;dA&quot;+str(l+2)]</span><br><span class="line">        grads[&quot;dA&quot; + str(l + 1)],grads[&quot;dW&quot; + str(l + 1)],grads[&quot;db&quot; + str(l + 1)] = linear_activation_backward(current_A, current_cache, &apos;relu&apos;)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    return grads</span><br></pre></td></tr></table></figure>
<h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def update_parameters(parameters, grads, learning_rate):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Update parameters using gradient descent</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters </span><br><span class="line">    grads -- python dictionary containing your gradients, output of L_model_backward</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">                  parameters[&quot;W&quot; + str(l)] = ... </span><br><span class="line">                  parameters[&quot;b&quot; + str(l)] = ...</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural network</span><br><span class="line"></span><br><span class="line">    # Update rule for each parameter. Use a for loop.</span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines of code)</span><br><span class="line">    for l in range(L):</span><br><span class="line">        parameters[&quot;W&quot;+str(l+1)]-=learning_rate*grads[&quot;dW&quot;+str(l+1)]</span><br><span class="line">        parameters[&quot;b&quot;+str(l+1)]-=learning_rate*grads[&quot;db&quot;+str(l+1)]</span><br><span class="line">        </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/22/线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/22/线性模型/" itemprop="url">PRML-线性模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-22T13:55:54+08:00">
                2018-12-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="线性基函数模型"><a href="#线性基函数模型" class="headerlink" title="线性基函数模型"></a>线性基函数模型</h3><p>线性回归模型 y(x,w) = w0+w1x1+w2x2+…wDxD</p>
<p>而为了改进模型，把xi替换为非线性表达式</p>
<p><img src="/home/alex/图片/six.png" alt="uploaccessful"></p>
<p>本实验使用了多项式基函数，高斯基函数和sigmoid基函数，以及混合型（多项式+sin正弦）而损失函数有两种选择，<em>交叉熵</em>函数以及<em>均方误差</em>，这里选择均方误差，求导比较方便。</p>
<h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><h4 id="多项式基函数"><a href="#多项式基函数" class="headerlink" title="多项式基函数"></a>多项式基函数</h4><p>y = w0+w1<em>x1+w2</em>x2^2 + b</p>
<p>λ2是L2正则化系数，可以解决过拟合问题</p>
<p>正则化意义：<a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">https://blog.csdn.net/jinping_shi/article/details/52433975</a></p>
<h4 id="推导-1"><a href="#推导-1" class="headerlink" title="推导"></a>推导</h4><p><img src="/home/alex/图片/2018-98.png" alt="filename alrey exists, renamed"></p>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def propogate(w,b,X,Y):</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    :param w: (n,1)</span><br><span class="line">    :param b: scalar</span><br><span class="line">    :param X: (n,m)</span><br><span class="line">    :param Y: (1,n)</span><br><span class="line">    :return: grads a dict that saves the params dw,db</span><br><span class="line">            cost saves the cost after each iteration</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    #np.squeeze</span><br><span class="line">    #squeeze 函数：从数组的形状中删除单维度条目，即把shape中为1的维度去掉</span><br><span class="line">    m = X.shape[1]</span><br><span class="line"></span><br><span class="line">    Yba = np.dot(w.T,X) + b</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze((0.5/m *np.dot((Yba-Y),(Yba-Y).T)))</span><br><span class="line"></span><br><span class="line">    dw = (1./m) * np.dot(X,(Yba-Y).T)</span><br><span class="line"></span><br><span class="line">    db = (1./m) * np.squeeze(np.sum(Yba-Y))</span><br><span class="line">    #save the updated result to the grads dict</span><br><span class="line">    grads = &#123;&quot;dw&quot;:dw,&quot;db&quot;:db&#125;</span><br><span class="line"></span><br><span class="line">    return grads,cost</span><br><span class="line"></span><br><span class="line">def optimize(w,b,X,Y,epochs,learning_rate,l2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    :param w: (n,1) </span><br><span class="line">    :param b: scalar</span><br><span class="line">    :param X: (n,m)</span><br><span class="line">    :param Y: (1,n)</span><br><span class="line">    :param epochs: 迭代次数 </span><br><span class="line">    :param learning_rate: </span><br><span class="line">    :param l2: 正则化系数 </span><br><span class="line">    :return: params</span><br><span class="line">            grads save the final params ,used for test</span><br><span class="line">            costs save the cost</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    for i in range(epochs):</span><br><span class="line">        grads,cost = propogate(w,b,X,Y)</span><br><span class="line">        dw = grads[&quot;dw&quot;]</span><br><span class="line">        db = grads[&quot;db&quot;]</span><br><span class="line"></span><br><span class="line">        w-=learning_rate*dw+l2*w</span><br><span class="line">        b-=learning_rate*db</span><br><span class="line"></span><br><span class="line">    params = &#123;&quot;w&quot;:w,&quot;b&quot;:b&#125;</span><br><span class="line">    grads = &#123;&quot;dw&quot;:dw,&quot;db&quot;:db&#125;</span><br><span class="line"></span><br><span class="line">    return params,grads,costs</span><br><span class="line"></span><br><span class="line">def main(x_train, y_train,n,epoches,learning_rate,l2):</span><br><span class="line">    &quot;&quot;&quot;训练模型，并返回从x到y的映射。&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # 使用线性回归训练模型，根据训练集计算最优化参数</span><br><span class="line">    ## 请补全此处代码，替换以下示例</span><br><span class="line">    m = x_train.shape[0]</span><br><span class="line">    w = (np.random.randn(n,1))</span><br><span class="line">    X = np.zeros((n,m),dtype = np.float64)</span><br><span class="line">    b = np.float(0)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        X[i,:] = x_train**(i+1)</span><br><span class="line">    Y = np.float64(np.reshape(y_train,newshape=(1,m)))</span><br><span class="line">    params,grads,costs = optimize(w,b,X,Y,epoches,learning_rate,l2)</span><br><span class="line">    w = params[&apos;w&apos;]</span><br><span class="line">    b = params[&apos;b&apos;]</span><br><span class="line">    def f(x):</span><br><span class="line">        ## 请补全此处代码，替换以下示例</span><br><span class="line">        m = x.shape[0]</span><br><span class="line"></span><br><span class="line">        X = np.zeros((n,m))</span><br><span class="line">        for i in range(n):</span><br><span class="line">            X[i,:] = x**(i+1)</span><br><span class="line">        y = b+np.dot(w.T,X)</span><br><span class="line">        return y.squeeze()</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    return f</span><br></pre></td></tr></table></figure>
<h4 id="result"><a href="#result" class="headerlink" title="result"></a>result</h4><p>参数：n=2 epochs = 100000 lr 1e-7</p>
<p><img src="/home/alex/图片/2018-97.png" alt="filename alrey exists, renamed"></p>
<h4 id="高斯基函数"><a href="#高斯基函数" class="headerlink" title="高斯基函数"></a>高斯基函数</h4><p><img src="/home/alex/图片/gaosiji.png" alt="upload succeful"></p>
<h4 id="推导-2"><a href="#推导-2" class="headerlink" title="推导"></a>推导</h4><p><img src="/home/alex/图片/推导.png" alt="upload ssful"></p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def propogate(w,b,X,Y,mu,s):</span><br><span class="line"></span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    Z = (X-mu)/s</span><br><span class="line">    A = np.exp(-(Z*Z)/2)</span><br><span class="line">    Yba = np.dot(w.T,A) + b</span><br><span class="line">    cost = np.squeeze((0.5/m *np.dot((Yba-Y),(Yba-Y).T)))</span><br><span class="line"></span><br><span class="line">    dY = 1./m*(Yba-Y)</span><br><span class="line">    dw = np.dot(A,dY.T)</span><br><span class="line">    db = np.squeeze(np.sum(dY))</span><br><span class="line">    dA = w*dY</span><br><span class="line">    dZ = dA*A*(-Z)</span><br><span class="line">    dmu = 1./m*np.sum(dZ*(-1/s),axis=1,keepdims=True)</span><br><span class="line">    ds = 1./m*np.sum(dZ*(-(X-mu)/(s*s)),axis=1,keepdims=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    grads = &#123;&quot;dw&quot;:dw,&quot;db&quot;:db,&quot;dmu&quot;:dmu,&quot;ds&quot;:ds&#125;</span><br><span class="line"></span><br><span class="line">    return grads,cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def optimize(w,b,X,Y,mu,s,epochs,learning_rate,l2):</span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    for i in range(epochs):</span><br><span class="line">        grads,cost = propogate(w,b,X,Y,mu,s)</span><br><span class="line">        dw = grads[&quot;dw&quot;]</span><br><span class="line">        db = grads[&quot;db&quot;]</span><br><span class="line">        dmu = grads[&quot;dmu&quot;]</span><br><span class="line">        ds = grads[&quot;ds&quot;]</span><br><span class="line"></span><br><span class="line">        w-=learning_rate*dw+l2*w</span><br><span class="line">        b-=learning_rate*db+l2*b</span><br><span class="line">        mu-=learning_rate*dmu+l2*mu</span><br><span class="line">        s -=learning_rate*ds+l2*s</span><br><span class="line"></span><br><span class="line">        costs.append(cost)</span><br><span class="line"></span><br><span class="line">    params = &#123;&quot;w&quot;:w,&quot;b&quot;:b,&quot;mu&quot;:mu,&quot;s&quot;:s&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    return params,costs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(x_train, y_train,n,epoches,learning_rate,l2):</span><br><span class="line">    &quot;&quot;&quot;训练模型，并返回从x到y的映射。&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # 使用线性回归训练模型，根据训练集计算最优化参数</span><br><span class="line">    ## 请补全此处代码，替换以下示例</span><br><span class="line">    m = x_train.shape[0]</span><br><span class="line">    w = (np.random.randn(n,1))</span><br><span class="line">    # means of gaussian</span><br><span class="line">    mu = np.random.randn(n,1)</span><br><span class="line">    #I still have no idea why set like this?</span><br><span class="line">    for i in range(n):</span><br><span class="line">        mu[i,0]+=i*40</span><br><span class="line">    X = np.zeros((n,m),dtype = np.float64)</span><br><span class="line">    b = np.float(0)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        X[i,:] = x_train</span><br><span class="line">    Y = np.float64(np.reshape(y_train,newshape=(1,m)))</span><br><span class="line"></span><br><span class="line">    #initalize s if s is too close to 0 then it will go wrong</span><br><span class="line">    while True:</span><br><span class="line">        flag = True</span><br><span class="line">        s = np.random.randn(n,1)</span><br><span class="line">        jump = np.int32(np.abs(s)&lt;1)</span><br><span class="line">        if(np.sum(jump))&gt;=1:</span><br><span class="line">            flag = False</span><br><span class="line">        if flag is True:</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    params,costs = optimize(w,b,X,Y,mu,s,epoches,learning_rate,l2)</span><br><span class="line">    w = params[&apos;w&apos;]</span><br><span class="line">    b = params[&apos;b&apos;]</span><br><span class="line">    mu = params[&apos;mu&apos;]</span><br><span class="line">    s = params[&apos;s&apos;]</span><br><span class="line">    def f(x):</span><br><span class="line">        ## 请补全此处代码，替换以下示例</span><br><span class="line">        m = x.shape[0]</span><br><span class="line"></span><br><span class="line">        X = np.zeros((n,m))</span><br><span class="line">        for i in range(n):</span><br><span class="line">            X[i,:] = x</span><br><span class="line">        Z = (X-mu)/s</span><br><span class="line">        A = np.exp(-(Z*Z)/2)</span><br><span class="line">        y = b+np.dot(w.T,A)</span><br><span class="line">        return y.squeeze()</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    return f</span><br></pre></td></tr></table></figure>
<p>参数：f = gaussian.main(x_train, y_train,3,500000,1e-2,0)</p>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p><img src="/home/alex/图片/2018-100.png" alt="fileme already exists, renamed"></p>
<h4 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h4><p><img src="/home/alex/图片/zhuanjiang.png" alt="upload succeful"></p>
<h4 id="推导-3"><a href="#推导-3" class="headerlink" title="推导"></a>推导</h4><p>结果与高斯类似</p>
<p><img src="/home/alex/图片/2018-99.png" alt="fiename already exists, renamed"></p>
<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def sigmoidFunction(z):</span><br><span class="line">    return 1./(1+np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def propogate(w,b,X,Y,mu,s):</span><br><span class="line"></span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    Z = (X-mu)/s</span><br><span class="line">    A = sigmoidFunction(Z)</span><br><span class="line">    Yba = np.dot(w.T,A) + b</span><br><span class="line">    cost = np.squeeze((0.5/m *np.dot((Yba-Y),(Yba-Y).T)))</span><br><span class="line"></span><br><span class="line">    dY = 1./m*(Yba-Y)</span><br><span class="line">    dw = np.dot(A,dY.T)</span><br><span class="line">    db = np.squeeze(np.sum(dY))</span><br><span class="line">    dA = w*dY</span><br><span class="line">    dZ = dA*A*(1-A)</span><br><span class="line">    dmu = 1./m*np.sum(dZ*(-1./s),axis=1,keepdims=True)</span><br><span class="line">    ds = 1./m*np.sum(dZ*(-(X-mu)/(s*s)),axis=1,keepdims=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    grads = &#123;&quot;dw&quot;:dw,&quot;db&quot;:db,&quot;dmu&quot;:dmu,&quot;ds&quot;:ds&#125;</span><br><span class="line"></span><br><span class="line">    return grads,cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def optimize(w,b,X,Y,mu,s,epochs,learning_rate,l2):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    for i in range(epochs):</span><br><span class="line">        grads,cost = propogate(w,b,X,Y,mu,s)</span><br><span class="line">        dw = grads[&quot;dw&quot;]</span><br><span class="line">        db = grads[&quot;db&quot;]</span><br><span class="line">        dmu = grads[&quot;dmu&quot;]</span><br><span class="line">        ds = grads[&quot;ds&quot;]</span><br><span class="line"></span><br><span class="line">        w-=learning_rate*dw+l2*w</span><br><span class="line">        b-=learning_rate*db+l2*b</span><br><span class="line">        mu-=learning_rate*dmu+l2*mu</span><br><span class="line">        s -=learning_rate*ds+l2*s</span><br><span class="line"></span><br><span class="line">        costs.append(cost)</span><br><span class="line"></span><br><span class="line">    params = &#123;&quot;w&quot;:w,&quot;b&quot;:b,&quot;mu&quot;:mu,&quot;s&quot;:s&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    return params,costs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(x_train, y_train,n,epoches,learning_rate,l2):</span><br><span class="line">    &quot;&quot;&quot;训练模型，并返回从x到y的映射。&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # 使用线性回归训练模型，根据训练集计算最优化参数</span><br><span class="line">    ## 请补全此处代码，替换以下示例</span><br><span class="line">    m = x_train.shape[0]</span><br><span class="line">    w = (np.random.randn(n,1))</span><br><span class="line">    # means of gaussian</span><br><span class="line">    mu = np.random.randn(n,1)</span><br><span class="line">    # mu is the mean of the sigmoid</span><br><span class="line">    for i in range(n):</span><br><span class="line">        mu[i,0]+= i*100/n</span><br><span class="line">    X = np.zeros((n,m),dtype = np.float64)</span><br><span class="line">    b = np.float(0)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        X[i,:] = x_train</span><br><span class="line">    Y = np.float64(np.reshape(y_train,newshape=(1,m)))</span><br><span class="line"></span><br><span class="line">    #initalize s if s is too close to 0 then it will go wrong</span><br><span class="line">    while True:</span><br><span class="line">        flag = True</span><br><span class="line">        s = np.random.randn(n,1)</span><br><span class="line">        jump = np.int32(np.abs(s)&lt;1)</span><br><span class="line">        if(np.sum(jump))&gt;=1:</span><br><span class="line">            flag = False</span><br><span class="line">        if flag is True:</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    params,costs = optimize(w,b,X,Y,mu,s,epoches,learning_rate,l2)</span><br><span class="line">    w = params[&apos;w&apos;]</span><br><span class="line">    b = params[&apos;b&apos;]</span><br><span class="line">    mu = params[&apos;mu&apos;]</span><br><span class="line">    s = params[&apos;s&apos;]</span><br><span class="line">    def f(x):</span><br><span class="line">        ## 请补全此处代码，替换以下示例</span><br><span class="line">        m = x.shape[0]</span><br><span class="line"></span><br><span class="line">        X = np.zeros((n,m))</span><br><span class="line">        for i in range(n):</span><br><span class="line">            X[i,:] = x</span><br><span class="line">        Z = (X-mu)/s</span><br><span class="line">        A = sigmoidFunction(Z)</span><br><span class="line">        y = b+np.dot(w.T,A)</span><br><span class="line">        return y.squeeze()</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    return f</span><br></pre></td></tr></table></figure>
<p>参数：  f = sigmoid.main(x_train, y_train,10,100000,1e-2,0)</p>
<h4 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h4><p><img src="/home/alex/图片/jieguo.png" alt="upload successful"></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><p><a href="https://github.com/Haicang/PRML/blob/master/lab1/Report.ipynb" target="_blank" rel="noopener">https://github.com/Haicang/PRML/blob/master/lab1/Report.ipynb</a></p>
<p><a href="https://blog.csdn.net/pipisorry/article/details/73770637" target="_blank" rel="noopener">https://blog.csdn.net/pipisorry/article/details/73770637</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/18/生产者与消费者模型-同步异步概念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/18/生产者与消费者模型-同步异步概念/" itemprop="url">生产者与消费者模型&同步异步概念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-18T13:01:48+08:00">
                2018-12-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Ref：</p>
<p><a href="https://blog.csdn.net/fuzhongmin05/article/details/54616344" target="_blank" rel="noopener">https://blog.csdn.net/fuzhongmin05/article/details/54616344</a></p>
<p><img src="/home/alex/图片/expla.png" alt="upload succsful"></p>
<p>code:</p>
<p><img src="/home/alex/图片/code.png" alt="upload successl"></p>
<p><strong><em>无法避免竞争</em></strong></p>
<p>对count的访问没有做限制。</p>
<p>这里有可能出现竞争条件，其原因是对count的访问未作限制。有可能出现以下情况：缓冲区为空，消费者刚刚读取count的值发现它为0，此时调度程序决定暂停消费者并启动运行生产者。生产者向缓冲区加入一个数据项，count加1。现在count的值变成了1，它推断认为count刚才为0，所以消费者此时一定在睡眠，于是生产者调用wakeup来唤醒消费者。</p>
<p> 但是消费者在逻辑上并未睡眠，所以wakeup信号丢失，当消费者下次运行时，它将测试先前读取的count值，发现它为0。于是睡眠，生产者迟早会填满整个缓冲区，然后睡眠，这样一来，两个进程将永远睡眠下去。</p>
<h3 id="引入信号量的操作"><a href="#引入信号量的操作" class="headerlink" title="引入信号量的操作"></a>引入信号量的操作</h3><p> Dijkstra建议设立两种操作：down和up（分别为一般化后的sleep和wakeup）。对一信号量执行down操作，则是检查其值是否大于0。若该值大于0，则将其减1（即用掉一个保存的唤醒信号）并继续；若该值为0，则进程将睡眠，而且此时down操作并未结束。检查数值、修改变量值以及可能发生的睡眠操作均作为一个单一的、不可分割的原子操作完成。保证一旦一个信号量操作开始，则在该操作完成或阻塞之前，其他进程均不允许访问该信号量。这种原子性对于解决同步问题和避免竞争条件是绝对必要的。所谓原子操作，是指一组相关联的操作要么都不间断地执行，要么不执行。</p>
<p> up操作对信号量的值增1。如果一个或多个进程在该信号量上睡眠，无法完成一个先前的down操作，则由系统选择其中的一个（如随机挑选）并允许该进程完成它的down操作。于是，对一个有进程在其上睡眠的信号量执行一次up操作后，该信号量的值仍旧是0，但在其上睡眠的进程却少了一个。信号量的值增加1和唤醒一个进程同样也是不可分割的，不会有某个进程因执行up而阻塞，正如前面的模型中不会有进程因执行wakeup而阻塞一样。</p>
<p> 在Dijkstra原来的论文中，他分别使用名称P和V而不是down和up，荷兰语中，Proberen的意思是尝试，Verhogen的含义是增加或升高。</p>
<p>从物理上说明信号量的P、V操作的含义。 P(S)表示申请一个资源，S.value&gt;0表示有资源可用,其值为资源的数目；S.value=0表示无资源可用；S.value&lt;0, 则|S.value|表示S等待队列中的进程个数。V(S)表示释放一个资源，信号量的初值应该大于等于0。P操作相当于“等待一个信号”，而V操作相当于“发送一个信号”，在实现同步过程中，V操作相当于发送一个信号说合作者已经完成了某项任务，在实现互斥过程中，V操作相当于发送一个信号说临界资源可用了。实际上，在实现互斥时，P、V操作相当于申请资源和释放资源。</p>
<p>该解决方案使用了三个信号量：一个称为full，用来记录充满缓冲槽数目，一个称为empty，记录空的缓冲槽总数；一个称为mutex，用来确保生产者和消费者不会同时访问缓冲区。full的初值为0，empty的初值为缓冲区中槽的数目，mutex的初值为1。供两个或多个进程使用的信号量，其初值为1，保证同时只有一个进程可以进入临界区，称作二元信号量。如果每个进程在进入临界区前都执行down操作，并在刚刚退出时执行一个up操作，就能够实现互斥。</p>
<p> 在下面的例子中，我们实际上是通过两种不同的方式来使用信号量，两者之间的区别是很重要的，信号量mutex用于互斥，它用于保证任一时刻只有一个进程读写缓冲区和相关的变量。互斥是避免混乱所必需的操作。</p>
<p>例题：</p>
<p><img src="/home/alex/图片/mancao.png" alt="uplouccessful"></p>
<p> items  就是full，记录满槽数目，初始值位0</p>
<p> slots 就是empty，就是缓冲区中槽的数目,初始值为10</p>
<p> mutex inital value  = 0</p>
<p>那么insert function</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">P(slots);       //empty slot-1</span><br><span class="line">P(mutex);		//进入临界区</span><br><span class="line">insert_item</span><br><span class="line">V(mutex);		//离开临界区</span><br><span class="line">V(items);		//full slot+1</span><br></pre></td></tr></table></figure>
<p>remove function</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">P(items);		//满槽数目-1</span><br><span class="line">P(mutex);		</span><br><span class="line">remove_item;</span><br><span class="line">V(mutex);</span><br><span class="line">V(slots);		//空槽数目+1</span><br></pre></td></tr></table></figure>
<p>本例中，信号量保证缓冲区满的时候生产者停止运行，缓冲区空的时候消费者停止运行。</p>
<h3 id="信号量用于同步"><a href="#信号量用于同步" class="headerlink" title="信号量用于同步"></a>信号量用于同步</h3><h4 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h4><p>一个进程执行某个请求，如果需要一段时间才能返回信息，那么进程会一直等待下去。也就是说直到收到返回信息才继续下去。</p>
<h4 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h4><p>进程不需要一直等待下去，继续执行下面的操作，不管其他进程的状态。</p>
<p>对于无界缓冲区问题，消费者只需关心缓冲区是否空即可</p>
<p><img src="/home/alex/图片/empty.png" alt="upload suessful">。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.jpg"
                alt="Alex Chiu" />
            
              <p class="site-author-name" itemprop="name">Alex Chiu</p>
              <p class="site-description motion-element" itemprop="description">Alex's personal blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alex Chiu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
