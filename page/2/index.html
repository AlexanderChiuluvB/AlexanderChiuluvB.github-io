<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Alex&apos;s personal blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Alex Chiu">
<meta property="og:url" content="http://AlexanderChiuluvB.github.io/page/2/index.html">
<meta property="og:site_name" content="Alex Chiu">
<meta property="og:description" content="Alex&apos;s personal blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Alex Chiu">
<meta name="twitter:description" content="Alex&apos;s personal blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://AlexanderChiuluvB.github.io/page/2/"/>





  <title>Alex Chiu</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Alex Chiu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/31/DL-C4W3-YOLO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/31/DL-C4W3-YOLO/" itemprop="url">DL-C4W3(YOLO)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-31T14:24:36+08:00">
                2018-12-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="intro-边框的定义"><a href="#intro-边框的定义" class="headerlink" title="intro 边框的定义"></a>intro 边框的定义</h3><p><img src="/home/alex/图片/2018-149.png" alt="filename alreay exists, renamed"></p>
<p>加入我们要分类80个物品，那么就有80个c（c0,c1…c80）</p>
<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>you only look once，只做一次前馈传播，并使用非最大化抑制之后就可以输出目标框。</p>
<h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4><p><img src="/home/alex/图片/编码.png" alt="uplouccessful"></p>
<p><img src="/home/alex/图片/sleep.png" alt="upload succful"></p>
<p>所谓anchor box，就是用来使得一个格子能够检测出多个对象。需要预先定义好anchor box的形状，当每找到一个对象的中点的时候，不仅仅把中点分配给对应的grid，而且还会分配到对应的anchor box</p>
<p>对于每个anchor box，找出该框包含某一类的概率</p>
<p><img src="/home/alex/图片/计算.png" alt="upload ccessful"></p>
<h4 id="可视化预测"><a href="#可视化预测" class="headerlink" title="可视化预测"></a>可视化预测</h4><p><img src="/home/alex/图片/取最大值.png" alt="upload sful"></p>
<p><img src="/home/alex/图片/2018-150.png" alt="filename already exists, ramed"></p>
<p>当框框太多的时候，使用<em>非最大化抑制</em>的方法剔除一些重叠的框框。</p>
<p>code见下</p>
<h4 id="filter-box"><a href="#filter-box" class="headerlink" title="filter_box"></a>filter_box</h4><p><img src="/home/alex/图片/filter.png" alt="upload sucsful"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">    通过阈值来过滤对象和分类的置信度。</span><br><span class="line"></span><br><span class="line">    参数：</span><br><span class="line">        box_confidence  - tensor类型，维度为（19,19,5,1）,包含19x19单元格中每个单元格预测的5个锚框中的所有的锚框的pc （一些对象的置信概率）。</span><br><span class="line">        boxes - tensor类型，维度为(19,19,5,4)，包含了所有的锚框的（px,py,ph,pw ）。</span><br><span class="line">        box_class_probs - tensor类型，维度为(19,19,5,80)，包含了所有单元格中所有锚框的所有对象( c1,c2,c3，···，c80 )检测的概率。</span><br><span class="line">        threshold - 实数，阈值，如果分类预测的概率高于它，那么这个分类预测的概率就会被保留。</span><br><span class="line"></span><br><span class="line">    返回：</span><br><span class="line">        scores - tensor 类型，维度为(None,)，包含了保留了的锚框的分类概率。</span><br><span class="line">        boxes - tensor 类型，维度为(None,4)，包含了保留了的锚框的(b_x, b_y, b_h, b_w)</span><br><span class="line">        classess - tensor 类型，维度为(None,)，包含了保留了的锚框的索引</span><br><span class="line"></span><br><span class="line">    注意：&quot;None&quot;是因为你不知道所选框的确切数量，因为它取决于阈值。</span><br><span class="line">          比如：如果有10个锚框，scores的实际输出大小将是（10,）</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Step 1: Compute box scores</span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    box_scores = box_confidence * box_class_probs</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines)</span><br><span class="line">    box_classes = K.argmax(box_scores, axis = -1)</span><br><span class="line">    box_class_scores = K.max(box_scores, axis = -1)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Step 3: Create a filtering mask based on &quot;box_class_scores&quot; by using &quot;threshold&quot;. The mask should have the</span><br><span class="line">    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability &gt;= threshold)</span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    filtering_mask = (box_class_scores &gt;= threshold)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Step 4: Apply the mask to scores, boxes and classes</span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines)</span><br><span class="line">    scores = tf.boolean_mask(box_class_scores,filtering_mask)</span><br><span class="line">    boxes = tf.boolean_mask(boxes,filtering_mask)</span><br><span class="line">    classes= tf.boolean_mask(box_classes,filtering_mask)</span><br><span class="line">    </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return scores, boxes, classes</span><br></pre></td></tr></table></figure>
<h4 id="非最大化抑制"><a href="#非最大化抑制" class="headerlink" title="非最大化抑制"></a>非最大化抑制</h4><p><img src="/home/alex/图片/2018-151.png" alt="filename already exists, renamed"></p>
<p>1.假设首先设定阈值为0.6，抛弃所有pc&lt;=0.6可能性的框框，这一步先剔除了所有可能性很低的框框。</p>
<p>2.选中一个pc最大的框框，作为输出，然后抛弃所有其他的与输出的交并比&gt;=0.5的框框</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Applies Non-max suppression (NMS) to set of boxes</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    scores -- tensor of shape (None,), output of yolo_filter_boxes()</span><br><span class="line">    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)</span><br><span class="line">    classes -- tensor of shape (None,), output of yolo_filter_boxes()</span><br><span class="line">    max_boxes -- integer, maximum number of predicted boxes you&apos;d like</span><br><span class="line">    iou_threshold -- real value, &quot;intersection over union&quot; threshold used for NMS filtering</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    scores -- tensor of shape (, None), predicted score for each box</span><br><span class="line">    boxes -- tensor of shape (4, None), predicted box coordinates</span><br><span class="line">    classes -- tensor of shape (, None), predicted class for each box</span><br><span class="line">    </span><br><span class="line">    Note: The &quot;None&quot; dimension of the output tensors has obviously to be less than max_boxes. Note also that this</span><br><span class="line">    function will transpose the shapes of scores, boxes, classes. This is made for convenience.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    max_boxes_tensor = K.variable(max_boxes, dtype=&apos;int32&apos;)     # tensor to be used in tf.image.non_max_suppression()</span><br><span class="line">    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor</span><br><span class="line">    </span><br><span class="line">    # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep</span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    indicesList = tf.image.non_max_suppression(</span><br><span class="line">    boxes,</span><br><span class="line">    scores,</span><br><span class="line">    max_boxes,</span><br><span class="line">    iou_threshold,</span><br><span class="line">    score_threshold=float(&apos;-inf&apos;),</span><br><span class="line">    name=None</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    # Use K.gather() to select only nms_indices from scores, boxes and classes</span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines)</span><br><span class="line">    #把在indicesList的scores gather起来</span><br><span class="line">    scores = K.gather(scores,indicesList)</span><br><span class="line">    boxes = K.gather(boxes,indicesList)</span><br><span class="line">    classes = K.gather(classes,indicesList)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return scores, boxes, classes</span><br></pre></td></tr></table></figure>
<h3 id="所有框进行过滤"><a href="#所有框进行过滤" class="headerlink" title="所有框进行过滤"></a>所有框进行过滤</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:</span><br><span class="line">                    box_confidence: tensor of shape (None, 19, 19, 5, 1)</span><br><span class="line">                    box_xy: tensor of shape (None, 19, 19, 5, 2)</span><br><span class="line">                    box_wh: tensor of shape (None, 19, 19, 5, 2)</span><br><span class="line">                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)</span><br><span class="line">    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)</span><br><span class="line">    max_boxes -- integer, maximum number of predicted boxes you&apos;d like</span><br><span class="line">    score_threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box</span><br><span class="line">    iou_threshold -- real value, &quot;intersection over union&quot; threshold used for NMS filtering</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    scores -- tensor of shape (None, ), predicted score for each box</span><br><span class="line">    boxes -- tensor of shape (None, 4), predicted box coordinates</span><br><span class="line">    classes -- tensor of shape (None,), predicted class for each box</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### </span><br><span class="line">    </span><br><span class="line">    # Retrieve outputs of the YOLO model (≈1 line)</span><br><span class="line">    box_confidence,box_xy,box_wh,box_class_probs = yolo_outputs</span><br><span class="line"></span><br><span class="line">    # Convert boxes to be ready for filtering functions </span><br><span class="line">    boxes = yolo_boxes_to_corners(box_xy, box_wh) </span><br><span class="line">    # Use one of the functions you&apos;ve implemented to perform Score-filtering with a threshold of score_threshold (≈1 line)</span><br><span class="line">    scores,boxes,classes = yolo_filter_boxes(box_confidence,boxes,box_class_probs,score_threshold)</span><br><span class="line">    # Scale boxes back to original image shape.</span><br><span class="line">    boxes = scale_boxes(boxes, image_shape)</span><br><span class="line">    # Use one of the functions you&apos;ve implemented to perform Non-max suppression with a threshold of iou_threshold (≈1 line)</span><br><span class="line">    scores,boxes,classes = yolo_non_max_suppression(scores, boxes, classes,max_boxes,iou_threshold)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return scores, boxes, classes</span><br></pre></td></tr></table></figure>
<h3 id="YOLO总结"><a href="#YOLO总结" class="headerlink" title="YOLO总结"></a>YOLO总结</h3><p><img src="/home/alex/图片/2018-152.png" alt="filename eady exists, renamed"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/29/DL-C4W2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/29/DL-C4W2/" itemprop="url">DL-C4W2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-29T13:26:56+08:00">
                2018-12-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet -5"></a>LeNet -5</h3><p><img src="/home/alex/图片/alex.png" alt="upload ful"></p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="/home/alex/图片/local.png" alt="upload ssful"></p>
<p>conv-&gt;max pool-&gt;conv-&gt;max pool-&gt;conv-&gt;conv-&gt;conv-&gt;conv-&gt;max pool-&gt;fc-&gt;fc-&gt;fc</p>
<h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p><img src="/home/alex/图片/VGG16.png" alt="upload ssful"></p>
<p>16指的是只有１６层网络有需要学习的参数，ｐｏｏｌｉｎｇ层是没有要学习的参数的。</p>
<p>箭头下方的[CONV 64]指有６４个ｆｉｌｔｅｒ，x2值对两层作卷积</p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><h4 id="residual-block"><a href="#residual-block" class="headerlink" title="residual block"></a>residual block</h4><p><img src="/home/alex/图片/bl.png" alt="uload successful"></p>
<p><img src="/home/alex/图片/tiaodong.png" alt="uplosuccessful"></p>
<p> 增加short cut之后成为残差块的网络结构</p>
<p><img src="/home/alex/图片/2018-140.png" alt="filename alreay exists, renamed"></p>
<p> 可以构建更为深层的网络</p>
<h3 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h3><p><img src="/home/alex/图片/residual Network.png" alt="ccessful"></p>
<p> 没有residual block的网络叫做plain network</p>
<h4 id="why-it-works"><a href="#why-it-works" class="headerlink" title="why it works?"></a>why it works?</h4><p><img src="/home/alex/图片/2018-141.png" alt="filenamelready exists, renamed"></p>
<p>对于越深层的神经网络来说，参数越来越多，越来越难选择，将会导致连学习identity function（f(x)=x）都很困难，而如果用residual block，a^[l+2] = g(a^[l]) = a^[l],对于残差块来学习identity function 其实是很简单的，所以不影响性能。</p>
<p><img src="/home/alex/图片/哪里.png" alt="upload successfl"></p>
<p>在经历了相同的conv之后增加一层pooling</p>
<h3 id="1x1-convolution"><a href="#1x1-convolution" class="headerlink" title="1x1 convolution"></a>1x1 convolution</h3><p>又叫做网中网</p>
<p><img src="/home/alex/图片/2018-142.png" alt="filenamey exists, renamed"></p>
<p>对一层 nxnxc 的图片，应用1x1xc的卷积核，得到新一层</p>
<p>nxnx1，f个filter处理后，就得到一个nxnxf的新层。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><h5 id="降维或升维"><a href="#降维或升维" class="headerlink" title="降维或升维"></a>降维或升维</h5><p>压缩channel个数，当卷积核个数小于输入channel数量的时候</p>
<p><img src="/home/alex/图片/convu.png" alt="upload sful"></p>
<h5 id="增加非线性"><a href="#增加非线性" class="headerlink" title="增加非线性"></a>增加非线性</h5><p>why？</p>
<p><img src="/home/alex/图片/2018-143.png" alt="filename already exists,med"></p>
<h5 id="跨通道信息交互"><a href="#跨通道信息交互" class="headerlink" title="跨通道信息交互"></a>跨通道信息交互</h5><p><img src="/home/alex/图片/sun.png" alt="ud successful"></p>
<h3 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h3><p>ref:</p>
<p><a href="https://zhuanlan.zhihu.com/p/40050371" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40050371</a></p>
<p><a href="https://blog.csdn.net/a1154761720/article/details/53411365" target="_blank" rel="noopener">https://blog.csdn.net/a1154761720/article/details/53411365</a></p>
<p><img src="/home/alex/图片/inception.png" alt="upload sssful"></p>
<h3 id="compute-cost"><a href="#compute-cost" class="headerlink" title="compute cost"></a>compute cost</h3><p><img src="/home/alex/图片/2018-145.png" alt="filename alr exists, renamed"></p>
<p>filter的第三个通道数目 == input feature map的第三个通道数目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">结果参数 = 输入层 HxWx卷积核个数</span><br><span class="line"></span><br><span class="line">总计算成本 = (结果参数28x28x32) × （卷积核大小5x5x192）</span><br></pre></td></tr></table></figure>
<h4 id="bottleneck-layer"><a href="#bottleneck-layer" class="headerlink" title="bottleneck layer"></a>bottleneck layer</h4><p><img src="/home/alex/图片/2018-146.png" alt="filename already exists,med"></p>
<h3 id="inception-network"><a href="#inception-network" class="headerlink" title="inception network"></a>inception network</h3><h4 id="google-net"><a href="#google-net" class="headerlink" title="google net"></a>google net</h4><p><img src="/home/alex/图片/googleNet.png" alt="upload suc"></p>
<p>1x1convolution 能够有效减少参数数量，加快训练</p>
<p><img src="/home/alex/图片/2018-147.png" alt="filename already exi, renamed"></p>
<p>可以观察到有一些旁路也输入到softmax中，因为hidden layer有时候的预测效果也不错，这么做可以防止过拟合</p>
<h3 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h3><p>小数据集<br><img src="/home/alex/图片/xiaoshujuji.png" alt="up successful"></p>
<p>大数据集<br><img src="/home/alex/图片/dashujuji.png" alt="upload ssful"></p>
<h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p><img src="/home/alex/图片/2018-148.png" alt="filename already erenamed"></p>
<p>多CPU多线程实现</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/28/Ubuntu-耳机-无线网驱动/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/28/Ubuntu-耳机-无线网驱动/" itemprop="url">Ubuntu 耳机/无线网驱动</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-28T20:23:40+08:00">
                2018-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无线网</p>
<p><a href="https://blog.csdn.net/fljhm/article/details/79281655" target="_blank" rel="noopener">https://blog.csdn.net/fljhm/article/details/79281655</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. make</span><br><span class="line">2. sudo make install</span><br><span class="line">3. sudo modprobe -a 8821ce</span><br></pre></td></tr></table></figure>
<p>耳机</p>
<p>1.alsactl restore</p>
<p>2.</p>
<p>sudo gedit /etc/modprobe.d/alsa-base.conf</p>
<p>添加</p>
<p>options snd-pcsp index=-2<br>alias snd-card-0 snd-hda-intel<br>alias sound-slot-0 snd-hda-intel<br>options snd-hda-intel model=pch position_fix=1</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/28/DL-C4W1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/28/DL-C4W1/" itemprop="url">DL-C4W1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-28T16:04:51+08:00">
                2018-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="为什么要卷积？"><a href="#为什么要卷积？" class="headerlink" title="为什么要卷积？"></a>为什么要卷积？</h3><p>for big size picture, the input scale would be very large. eg. an 1000<em>1000 size picture,<br>after flattening its features , you can get a vector as (3</em>1000*1000,1) = (3million ,1)</p>
<p><img src="/home/alex/图片/large.png" alt="upload ssful"></p>
<p>如果hidden　layer只有1000层，那么W1 的输入大小是(1000,3m)</p>
<p>因为z(1000,1) = W1(1000,3M)*X(3M,1)+b </p>
<h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3><p><img src="/home/alex/图片/边缘检测.png" alt="uad successful"></p>
<p>用一个3<em>3大小的卷积核对一张6</em>6大小的图片进行卷积运算，最终得到一个4x4的图片</p>
<p>python:conv_forward </p>
<p>tf.nn.conv2d</p>
<h4 id="边缘检测原理"><a href="#边缘检测原理" class="headerlink" title="边缘检测原理"></a>边缘检测原理</h4><p>用垂直边缘filter，可以明显吧边缘和非边缘区区分出来。</p>
<p><img src="/home/alex/图片/vertical.png" alt="upload sucssful"></p>
<h4 id="多种边缘检测"><a href="#多种边缘检测" class="headerlink" title="多种边缘检测"></a>多种边缘检测</h4><p><img src="/home/alex/图片/dark.png" alt="upload succful"></p>
<p><strong><em>我们可以直接把filter中的数字直接看作是需要学习的参数</em></strong></p>
<p>在nn中通过反向传播算法，学习到相应于目标结果的filter，然后把其应用在整个图片上，输出其提取到的所有有用的特征。</p>
<h3 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h3><p>从上面注意到每次卷积操作，图片会缩小。</p>
<p><img src="/home/alex/图片/2018-130.png" alt="filename alrea exists, renamed"></p>
<p>所以我们要在卷积之前，为图片加padding，包围角落和边缘的像素，使得通过filter的卷积运算后，图片大小不变，也不会丢失角落。</p>
<p><img src="/home/alex/图片/2018-131.png" alt="filename y exists, renamed"></p>
<h3 id="valid-Some-卷积"><a href="#valid-Some-卷积" class="headerlink" title="valid/Some 卷积"></a>valid/Some 卷积</h3><p>Valid: no padding </p>
<p>nxn –&gt;(n-f+1)x(n-f+1)</p>
<p>Same: padding</p>
<p>输出和输入图片的大小相同</p>
<p>p = (f-1)/2，在CV中，一般来说padding的值位奇数</p>
<p>N+2P-F+1 = N ,SO p = (F-1)/2</p>
<h3 id="卷积步长（stride）"><a href="#卷积步长（stride）" class="headerlink" title="卷积步长（stride）"></a>卷积步长（stride）</h3><p>stride=1,表示每次卷积运算以一个步长进行移动。</p>
<p><img src="/home/alex/图片/stride.png" alt="upload ful"></p>
<h3 id="立体卷积"><a href="#立体卷积" class="headerlink" title="立体卷积"></a>立体卷积</h3><p><img src="/home/alex/图片/立体卷积.png" alt="upd successful"></p>
<p><img src="/home/alex/图片/2018-132.png" alt="filename alrea exists, renamed"></p>
<p>第一行表示只检测红色通道的垂直边缘</p>
<p>第二行表示检测所有通道垂直边缘</p>
<p>卷积核第三个维度大小等于图片通道大小</p>
<h3 id="多卷积"><a href="#多卷积" class="headerlink" title="多卷积"></a>多卷积</h3><p><img src="/home/alex/图片/nminus.png" alt="upcessful"></p>
<p>上图意思是把检测垂直和水平边缘的两个图片叠成两层。</p>
<p><img src="/home/alex/图片/nextsee.png" alt="upload ful"></p>
<h3 id="单层卷积网络"><a href="#单层卷积网络" class="headerlink" title="单层卷积网络"></a>单层卷积网络</h3><p><img src="/home/alex/图片/danceng.png" alt="upload cessful"></p>
<p>与普通神经网络单层前向传播类似，卷机神经网络也是先由权重和bias做线性运算，然后得到结果在输入到一个激活函数中。</p>
<p><img src="/home/alex/图片/az.png" alt="upload succsful"></p>
<p>对应上图a[0]表示图片层（n<em>n</em>3）</p>
<p>w[1]对应卷积核（f<em>f</em>3）</p>
<p>a[1] 对应下一层（4x4x2）</p>
<h4 id="单层卷积参数个数"><a href="#单层卷积参数个数" class="headerlink" title="单层卷积参数个数"></a>单层卷积参数个数</h4><p><img src="/home/alex/图片/2018-134.png" alt="filename alreay exists, renamed"></p>
<p>不受图片大小影响</p>
<h3 id="标记"><a href="#标记" class="headerlink" title="标记"></a>标记</h3><p><img src="/home/alex/图片/2018-135.png" alt="filename alread exists, renamed"></p>
<p>f[l] 卷积核大小</p>
<p>卷积核第三个维度大小等于输入图片通道数</p>
<p>而权重就是卷积核大小×卷积核个数，卷积核个数就是输出层的通道数目</p>
<p>激活值大小就是下一层输出层的大小： nH X nW X nC</p>
<h4 id="简单卷积网络"><a href="#简单卷积网络" class="headerlink" title="简单卷积网络"></a>简单卷积网络</h4><p><img src="/home/alex/图片/2018-136.png" alt="filename y exists, renamed"></p>
<p>最后得到的7x7x40，一共1960个参数，就是最后输入激活函数的所有参数</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>最大池化(max pooling)</p>
<p>把前一层得到的特征图进行池化减小，仅由当前小区域内的最大值来代表最终池化后的值。</p>
<p><img src="/home/alex/图片/调参.png" alt="uploaduccessful"></p>
<p>平均池化</p>
<p><img src="/home/alex/图片/池化.png" alt="upload suessful"></p>
<p>池化只需要设置好超参数，没有要学习的参数</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>CNN的最大特点在于卷积的权值共享结构，可以大幅减少神经网络参数量，防止过拟合的同时又降低了神经网络模型的复杂度。</p>
<p>CNN通过卷积的方式实现局部链接，得到图片的参数量只跟卷积核的大小有关，一个卷积核对应一个图片特征，每一个卷积核滤波得到的图像就是一类特征的映射。</p>
<p>也就是说训练的权值数量只与卷积核大小与数量有关，但注意的是隐含层节点数量没有下降，隐含节点的数量只与卷积的步长有关，如果步长为1，那么隐含节点数量与输入图像像素数量一致。如果步长为5，那么每5x5个像素才需要一个隐含节点。</p>
<p>再总结，CNN的要点就是</p>
<p>1.局部连接 </p>
<p>2.权值共享</p>
<p>3.池化层的降采样</p>
<p>其中1与2降低了参数量，训练复杂度下降并减轻过拟合。</p>
<p>同时权值共享赋予了卷积网络对平移的容忍性。</p>
<p><img src="/home/alex/图片/总结.png" alt="upload successl"></p>
<p>随着nn层数增加，提取的特征图片大小将会减小，但是同时间通道数量会增加</p>
<p>为什么使用CNN？</p>
<p>1.参数少<br><img src="/home/alex/图片/cnn.png" alt="upload succeul"></p>
<p>2.参数共享&amp;链接的稀疏性</p>
<p>参数共享指一个卷积核可以有多个不同的卷积核，而每一个卷积核对应一个滤波后映射出的新图像，同一个新图像的每一个像素都来自完全相同的卷积核。</p>
<p><img src="/home/alex/图片/2018-137.png" alt="filename already exists, renad"></p>
<h3 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a>implementation</h3><h4 id="zero-padding"><a href="#zero-padding" class="headerlink" title="zero padding"></a>zero padding</h4><p><img src="/home/alex/图片/2018-138.png" alt="filename alreay exists, renamed"></p>
<h5 id="benefits"><a href="#benefits" class="headerlink" title="benefits"></a>benefits</h5><p><img src="/home/alex/图片/2018-139.png" alt="finame already exists, renamed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def zero_pad(X, pad):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, </span><br><span class="line">    as illustrated in Figure 1.</span><br><span class="line">    </span><br><span class="line">    Argument:</span><br><span class="line">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images</span><br><span class="line">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (≈ 1 line)</span><br><span class="line">    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), &apos;constant&apos;, constant_values=0)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return X_pad</span><br></pre></td></tr></table></figure>
<h4 id="forward-convolution"><a href="#forward-convolution" class="headerlink" title="forward convolution"></a>forward convolution</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def conv_single_step(a_slice_prev, W, b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation </span><br><span class="line">    of the previous layer.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)</span><br><span class="line">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)</span><br><span class="line">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    # Element-wise product between a_slice and W. Add bias.</span><br><span class="line">    s = np.multiply(a_slice_prev, W) + b</span><br><span class="line">    # Sum over all entries of the volume s</span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    return Z</span><br></pre></td></tr></table></figure>
<h4 id="define-a-slice"><a href="#define-a-slice" class="headerlink" title="define a slice"></a>define a slice</h4><p><img src="/home/alex/图片/slice.png" alt="upload succeful"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def conv_forward(A_prev, W, b, hparameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the forward propagation for a convolution function</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span><br><span class="line">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)</span><br><span class="line">    b -- Biases, numpy array of shape (1, 1, 1, n_C)</span><br><span class="line">    hparameters -- python dictionary containing &quot;stride&quot; and &quot;pad&quot;</span><br><span class="line">        </span><br><span class="line">    Returns:</span><br><span class="line">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)</span><br><span class="line">    cache -- cache of values needed for the conv_backward() function</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    # Retrieve dimensions from A_prev&apos;s shape (≈1 line)  </span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    # Retrieve dimensions from W&apos;s shape (≈1 line)</span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line"></span><br><span class="line">    # Retrieve information from &quot;hparameters&quot; (≈2 lines)</span><br><span class="line">    stride = hparameters[&apos;stride&apos;]</span><br><span class="line">    pad = hparameters[&apos;pad&apos;]</span><br><span class="line">    </span><br><span class="line">    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)</span><br><span class="line">    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1</span><br><span class="line">    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1</span><br><span class="line">    </span><br><span class="line">    # Initialize the output volume Z with zeros. (≈1 line)</span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    # Create A_prev_pad by padding A_prev</span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    for i in range(m):                                 # loop over the batch of training examples</span><br><span class="line">        a_prev_pad = A_prev_pad[i]                     # Select ith training example&apos;s padded activation</span><br><span class="line">        for h in range(n_H):                           # loop over vertical axis of the output volume</span><br><span class="line">            for w in range(n_W):                       # loop over horizontal axis of the output volume</span><br><span class="line">                for c in range(n_C):                   # loop over channels (= #filters) of the output volume</span><br><span class="line">                    # Find the corners of the current &quot;slice&quot; (≈4 lines)</span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line">                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)</span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])</span><br><span class="line">                                        </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    # Making sure your output shape is correct</span><br><span class="line">    assert(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    # Save information in &quot;cache&quot; for the backprop</span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    return Z, cache</span><br></pre></td></tr></table></figure>
<h3 id="对应的notebook"><a href="#对应的notebook" class="headerlink" title="对应的notebook"></a>对应的notebook</h3><p><a href="https://github.com/AlexanderChiuluvB/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb" target="_blank" rel="noopener">https://github.com/AlexanderChiuluvB/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/27/DL-C2W3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/27/DL-C2W3/" itemprop="url">DL-C2W3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-27T18:35:34+08:00">
                2018-12-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Hyperparameter-tuning"><a href="#Hyperparameter-tuning" class="headerlink" title="Hyperparameter tuning"></a>Hyperparameter tuning</h3><p>don’t use grid search</p>
<p>because you do not know which hyperparameters<br>matters most!</p>
<p><img src="/home/alex/图片/2018-123.png" alt="flename already exists, renamed"></p>
<p>instead,use randomly chosen hyperparameter</p>
<h4 id="coarse-to-fine"><a href="#coarse-to-fine" class="headerlink" title="coarse to fine"></a>coarse to fine</h4><p><img src="/home/alex/图片/coarse.png" alt="upload successl"></p>
<p>如果某些临近的参数效果不错，那么把选择的范围缩小。</p>
<h3 id="scale-for-params"><a href="#scale-for-params" class="headerlink" title="scale for params"></a>scale for params</h3><p>对数scale</p>
<p><img src="/home/alex/图片/2018-124.png" alt="filename already exists,named"></p>
<p><img src="/home/alex/图片/2018-125.png" alt="filename alreadysts, renamed"></p>
<p>如果beta 为0.9，当beta从0.9变成0.9005的时候，变化很小</p>
<p>但是如果beta很接近于1,当beta从0.999-&gt;0.9995的时候，就会变化很大</p>
<h4 id="panda-caviar-strategy"><a href="#panda-caviar-strategy" class="headerlink" title="panda / caviar strategy"></a>panda / caviar strategy</h4><p>given the computational resources you have,if limited,then you can only watch over one model,so check its cost 每隔一段时间，if any problems occurs, stop and return to previous state.</p>
<p>while if you have enough computational resources,then you can watch out various models<br>at one time,and choose the best one.</p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>实质是对hidden units做normalization</p>
<p><img src="/home/alex/图片/youluve.png" alt="upload cessful"></p>
<p><a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" target="_blank" rel="noopener">https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c</a></p>
<p><img src="/home/alex/图片/2018-126.png" alt="filename alry exists, renamed"></p>
<h3 id="fitting-batch-norm-to-nn"><a href="#fitting-batch-norm-to-nn" class="headerlink" title="fitting batch norm to nn"></a>fitting batch norm to nn</h3><p>每个ｅｐｏｃｈ对隐藏层做一次batch normalization</p>
<p>back propagate 的时候不用考虑db，因为在normalize的时候Z_TILDA = (Z-mean)/(sqrt of variance) 会把常数项减掉<br><img src="/home/alex/图片/minzhi.png" alt="upload succeul"></p>
<h3 id="why-batch-norm-work"><a href="#why-batch-norm-work" class="headerlink" title="why batch norm work?"></a>why batch norm work?</h3><p><img src="/home/alex/图片/2018-127.png" alt="filename already exi renamed"></p>
<p>当你训练一个模型，得到一个x-&gt;y的映射之后，如果预测数据分布发生了变化，你需要重新训练你的模型。这叫做 covariate shift</p>
<p>例如你训练的都是黑猫图片，如果你用这个模型来测试彩色猫的图片，那么效果肯定会不好。</p>
<p>而对hidden units 做normalization （batch norm），即使前面输入数据x发生变化，那么对后面层的影响将会变小很多，因为每一层都’constrained to have the same mean and variance’ 提升了神经网络应付不同输入的健壮性。</p>
<p><img src="/home/alex/图片/2018-128.png" alt="filene already exists, renamed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Consequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a “standard deviation” parameter (gamma) and add a “mean” parameter (beta). In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights.</span><br></pre></td></tr></table></figure>
<h3 id="softmax-layer"><a href="#softmax-layer" class="headerlink" title="softmax layer"></a>softmax layer</h3><p>In mathematics, the softmax function takes an un-normalized vector, and normalizes it into a probability distribution. </p>
<p>我们已经知道，逻辑回归可生成介于 0 和 1.0 之间的小数。例如，某电子邮件分类器的逻辑回归输出值为 0.8，表明电子邮件是垃圾邮件的概率为 80%，不是垃圾邮件的概率为 20%。很明显，一封电子邮件是垃圾邮件或非垃圾邮件的概率之和为 1.0。</p>
<p>Softmax 将这一想法延伸到多类别领域。也就是说，在多类别问题中，Softmax 会为每个类别分配一个用小数表示的概率。这些用小数表示的概率相加之和必须是 1.0。与其他方式相比，这种附加限制有助于让训练过程更快速地收敛。</p>
<h4 id="softmax-function"><a href="#softmax-function" class="headerlink" title="softmax function"></a>softmax function</h4><p><img src="/home/alex/图片/soft.png" alt="upload sucsful"></p>
<p><img src="/home/alex/图片/softmax.png" alt="upload essful"></p>
<h4 id="understanding-softmax"><a href="#understanding-softmax" class="headerlink" title="understanding softmax"></a>understanding softmax</h4><p><img src="/home/alex/图片/2018-129.png" alt="filename  exists, renamed"></p>
<h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/26/JAVA并发编程（一）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/26/JAVA并发编程（一）/" itemprop="url">JAVA并发编程（一）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-26T20:38:02+08:00">
                2018-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h3><p>现代操作系统调度的最小单元是线程，在一个进程当中可以创建多个线程，这些线程拥有各自的计数器，堆栈以及局部变量，并且能够访问共享的内存变量。</p>
<p>处理器在这些线程中高速切换，让使用者感觉到这些线程在同时执行。</p>
<h3 id="线程状态的转换"><a href="#线程状态的转换" class="headerlink" title="线程状态的转换"></a>线程状态的转换</h3><p><img src="/home/alex/图片/karmar.png" alt="uoad successful"></p>
<p>Runnable: 线程对象创建之后，其他线程调用了该对象的start方法，该状态的线程位于可运行线程池中，等待获取CPU使用权。</p>
<p>运行状态：获取了CPU，执行程序代码</p>
<p>阻塞状态：线程由于某种原因放弃CPU使用权，暂时停止运行，直到线程进入就绪状态。</p>
<p>分为三种：</p>
<ul>
<li><p>等待阻塞，运行的线程执行wait()方法，JVM会把该线程放入等待池中。该过程会释放持有的锁</p>
</li>
<li><p>同步阻塞，运行的线程在获取对象的同步锁时，如果同步锁被别的线程占用，JVM会把该线程放入锁池当中</p>
</li>
<li><p>其他阻塞，运行的线程执行sleep()或者join()方法，或者发出了I/O请求的时候，JVM会把线程设置为阻塞状态。当sleep()超时，join()等待线程终止或者超时，或者I/O处理完毕的时候，线程重新进入runnable状态，sleep不会释放持有的锁。</p>
</li>
</ul>
<h3 id="为什么？"><a href="#为什么？" class="headerlink" title="为什么？"></a>为什么？</h3><h4 id="更多的处理器核心"><a href="#更多的处理器核心" class="headerlink" title="更多的处理器核心"></a>更多的处理器核心</h4><p>程序运行当中能够创建多个线程，但是一个线程只能在一个处理核心中运行，单线程程序运行的时候只能利用一个处理器核心。</p>
<p>而多线程把计算逻辑分配到多个处理器核心上面，让多个处理器核心加入到程序运行。</p>
<h4 id="更快的响应时间"><a href="#更快的响应时间" class="headerlink" title="更快的响应时间"></a>更快的响应时间</h4><p>一个面向用户的业务操作，可以把一些数据一致性不强的操作派发给其他线程处理，使得响应用户请求的线程能够尽快处理完成。</p>
<h3 id="线程调度"><a href="#线程调度" class="headerlink" title="线程调度"></a>线程调度</h3><h4 id="线程优先级"><a href="#线程优先级" class="headerlink" title="线程优先级"></a>线程优先级</h4><p>现代操作系统采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度。时间片多少决定了线程使用处理器资源的多少，而线程优先级决定线程得到分配的处理器资源多少。</p>
<p>线程默认优先级位是5，优先级高的线程分配时间片数量要多于优先级低的线程。设置线程优先级的时候，针对频繁阻塞的线程需要设置较高优先级，而偏重计算的线程设置较低的优先级，确保处理器不会被独占。</p>
<p><img src="/home/alex/图片/2018-153.png" alt="filenameready exists, renamed"></p>
<p><img src="/home/alex/图片/2018-157.png" alt="filename ready exists, renamed"></p>
<p>线程的优先级有继承关系</p>
<h4 id="线程睡眠"><a href="#线程睡眠" class="headerlink" title="线程睡眠"></a>线程睡眠</h4><p>Thread.sleep(long mills)<br>使得线程转到阻塞状态，结束后转为runnable状态</p>
<p> sleep()和yield()的区别):sleep()使当前线程进入停滞状态，所以执行sleep()的线程在指定的时间内肯定不会被执行；yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。</p>
<p>  sleep 方法使当前运行中的线程睡眼一段时间，进入不可运行状态，这段时间的长短是由程序设定的，yield 方法使当前线程让出 CPU 占有权，但让出的时间是不可设定的。实际上，yield()方法对应了如下操作：先检测当前是否有相同优先级的线程处于同可运行状态，如有，则把 CPU  的占有权交给此线程，否则，继续运行原来的线程。所以yield()方法称为“退让”，它把运行机会让给了同等优先级的其他线程</p>
<p>   另外，sleep 方法允许较低优先级的线程获得运行机会，但 yield()  方法执行时，当前线程仍处在可运行状态，所以，不可能让出较低优先级的线程些时获得 CPU 占有权。在一个运行系统中，如果较高优先级的线程没有调用 sleep 方法，又没有受到 I\O 阻塞，那么，较低优先级线程只能等待所有较高优先级的线程运行结束，才有机会运行。</p>
<h4 id="wait与sleep区别"><a href="#wait与sleep区别" class="headerlink" title="wait与sleep区别"></a>wait与sleep区别</h4><p><img src="/home/alex/图片/2018-158.png" alt="filenameeady exists, renamed"></p>
<h4 id="线程等待"><a href="#线程等待" class="headerlink" title="线程等待"></a>线程等待</h4><p>Object类的wait()方法，导致当前的线程等待，直到其他线程调用此对象的notify（）方法或者notifyAll()方法。</p>
<p>wait()与notify()方法必须要与synchronized一起使用，也就是也就是wait，与notify是针对已经获取了的锁进行操作。</p>
<p>wait就是说线程在获取对象锁后，主动释放对象锁，同时本线程休眠。直到有其他线程调用对象的notify()唤醒该线程，这样才能继续获取对象锁并继续执行。响应的notify()就是对象锁的唤醒操作。sleep()与wait()二者都可以暂停当前线程，释放CPU的控制权，主要区别是Object.wait()在释放CPU同时，释放了对锁的控制。</p>
<p>该问题为三线程间的同步唤醒操作，主要目的就是ThreadA-&gt;ThreadB-&gt;ThreadC,每一个线程必须同时持有两个对象锁才能继续执行。一个对象锁是prev，是前一个线程所持有的对象锁。还有一个是自身的对象锁。为了控制执行顺序，先持有prev锁，也就是前一个线程要释放自身对象锁，再去申请自身对象锁，两者兼备的时候打印。之后首先调用self.notify()释放自身对象锁，唤醒下一个等待线程，再用prev.wait()释放prev对象锁，终止当前线程。等待循环结束之后再次被唤醒。</p>
<h4 id="线程让步"><a href="#线程让步" class="headerlink" title="线程让步"></a>线程让步</h4><p>Thread.yield()方法，暂停当前执行的线程对象，把执行机会让给相同或者更高优先级的线程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public class ThreadYield extends Thread&#123;</span><br><span class="line">  //private String name;</span><br><span class="line">  public ThreadYield(String name)&#123;</span><br><span class="line">      super(name);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void run()&#123;</span><br><span class="line">    for(int i=1;i&lt;=10;i++)&#123;</span><br><span class="line">      System.out.println(this.getName()+&quot;----&quot;+i);</span><br><span class="line">      if(i==3)&#123;</span><br><span class="line">        this.yield();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static void main(String[]args)&#123;</span><br><span class="line"></span><br><span class="line">    ThreadYield thread1 = new ThreadYield(&quot;ALex&quot;);</span><br><span class="line">    ThreadYield thread2 = new ThreadYield(&quot;Brecher&quot;);</span><br><span class="line"></span><br><span class="line">    thread1.start();</span><br><span class="line">    thread2.start();</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里结果表示ALEX执行到i=3的时候，会把CPU让出来，这时候BRECHER抢到线程。<br><img src="/home/alex/图片/2018-155.png" alt="filename already exists,med"></p>
<p>也有可能是BRECHER执行到i=3的时候，会把CPU让出来，这时候还是BRECHER抢到线程。<br><img src="/home/alex/图片/2018-156.png" alt="filename aady exists, renamed"></p>
<h4 id="线程加入"><a href="#线程加入" class="headerlink" title="线程加入"></a>线程加入</h4><p>join（），等待其他线程终止。在当前线程中调用另一个线程的join()方法，当前线程进入阻塞。知道另外一个线程结束，这个线程回到runnable</p>
<p>join():等待t线程终止<br>join是Thread类的一个方法，启动线程后直接调用，join(）作用是等待该线程终止。该线程是指主线程等待子线程的终止<br>也就是在子线程调用了join()方法后面的代码，只有等到子线程结束了才能执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Thread t = new Athread();</span><br><span class="line">t.start();</span><br><span class="line">t.join();</span><br></pre></td></tr></table></figure>
<p>为什么要用join？</p>
<p>如果子线程很耗时，主线程往往提前与子线程结束，万一主线程需要用子线程的处理结果，就是主线程需要等待子线程执行完成之后再结束。</p>
<p>如果不用join函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class Thread1 extends Thread&#123;</span><br><span class="line">  private String name;</span><br><span class="line">  public Thread1(String name)&#123;</span><br><span class="line">    super(name);</span><br><span class="line">    this.name = name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void run()&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;线程开始运行&quot;);</span><br><span class="line">    for(int i=0;i&lt;5;i++)&#123;</span><br><span class="line">      System.out.println(&quot;子线程&quot;+name+&quot;运行&quot;+i);</span><br><span class="line">      try&#123;</span><br><span class="line">        sleep((int)Math.random()*10);</span><br><span class="line">      &#125;catch(InterruptedException e)&#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;线程结束运行&quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  public static void main(String[] args)&#123;</span><br><span class="line"></span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程开始运行&quot;);</span><br><span class="line">    Thread1 Athread = new Thread1(&quot;A&quot;);</span><br><span class="line">    Thread1 Bthread = new Thread1(&quot;B&quot;);</span><br><span class="line">    Athread.start();</span><br><span class="line">    Bthread.start();</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程结束运行&quot;);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>结果：</p>
<p><img src="/home/alex/图片/结果.png" alt="upload succeful"></p>
<p>而在main函数中添加join方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args)&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程开始运行&quot;);</span><br><span class="line">    Thread1 Athread = new Thread1(&quot;A&quot;);</span><br><span class="line">    Thread1 Bthread = new Thread1(&quot;B&quot;);</span><br><span class="line">    Athread.start();</span><br><span class="line">    Bthread.start();</span><br><span class="line">    try&#123;</span><br><span class="line">      Athread.join();</span><br><span class="line">    &#125;catch(InterruptedException e)&#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    try&#123;</span><br><span class="line">      Bthread.join();</span><br><span class="line">    &#125;catch(InterruptedException e)&#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;主线程结束运行&quot;);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<p><img src="/home/alex/图片/2018-154.png" alt="filename amed"></p>
<h4 id="线程唤醒"><a href="#线程唤醒" class="headerlink" title="线程唤醒"></a>线程唤醒</h4><p>object类的notify()方法，唤醒在此对象监视器上等待的单个线程。如果所有线程都在此对象中等待，则会选择唤醒其中一个线程。线程调用其中一个wait方法，在对象的监视器上等待。知道当前线程放弃此对象上的锁定，才能继续执行被唤醒的线程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class ThreadState &#123;</span><br><span class="line"></span><br><span class="line">  public static void main(String[] args)&#123;</span><br><span class="line"></span><br><span class="line">      new Thread(new TimeWaiting(),&quot;TimeWaitingThread&quot;).start();</span><br><span class="line">      new Thread(new Waiting(),&quot;WaitingThread&quot;).start();</span><br><span class="line"></span><br><span class="line">      //使用两个Blocked线程，一个获取锁成功，另外一个被阻塞</span><br><span class="line">      new Thread(new Blocked(),&quot;BlockedThread-1&quot;).start();</span><br><span class="line">      new Thread(new Blocked(),&quot;BlockedTHread-2&quot;).start();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  static class SleepUtils&#123;</span><br><span class="line">      public static void second(long seconds) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">          TimeUnit.SECONDS.sleep(seconds);</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //该线程不断进行睡眠</span><br><span class="line">  static class TimeWaiting implements Runnable&#123;</span><br><span class="line">      public void run()&#123;</span><br><span class="line">        while(true)&#123;</span><br><span class="line">          SleepUtils.second(100);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //在Waiting.class实例上等待</span><br><span class="line">  static class Waiting implements Runnable&#123;</span><br><span class="line">    public void run()&#123;</span><br><span class="line">        while(true)&#123;</span><br><span class="line">          synchronized (Waiting.class)&#123;</span><br><span class="line">              try&#123;</span><br><span class="line">                  Waiting.class.wait();</span><br><span class="line">              &#125;catch(InterruptedException e)&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //该线程在Blocked.class 实例上加锁后，不会释放该锁</span><br><span class="line">  static class Blocked implements Runnable&#123;</span><br><span class="line">    public void run()&#123;</span><br><span class="line">        synchronized (Blocked.class)&#123;</span><br><span class="line">          while(true)&#123;</span><br><span class="line">              SleepUtils.second(100);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行代码，打开shell，输入jps，得到ThreadState的进程ID，然后 输入 jstack ID,得到线程消息</p>
<p><img src="/home/alex/图片/thread.png" alt="upload sucessful"></p>
<p><img src="/home/alex/图片/2018-118.png" alt="filename alrey exists, renamed"></p>
<p>当线程创建之后，调用start方法开始运行，当执行wait()的时候进入等待状态，需要依靠其他线程的通知才能够返回到运行状态。</p>
<p>而超时等待状态相当于在等待状态基础上增加了超时限制，一旦过时，返回到运行状态。</p>
<p>如果线程调用同步方法但没有获得锁，线程会进入到阻塞状态。</p>
<h3 id="常见线程名词解释"><a href="#常见线程名词解释" class="headerlink" title="常见线程名词解释"></a>常见线程名词解释</h3><p>主线程：main()产生的线程</p>
<p>当前线程：Thread.currentThread()获取的进程</p>
<p>后台线程：为其他线程提供服务的线程，称为守护线程</p>
<p>前台线程：接收后台线程服务的线程</p>
<h3 id="Daemon-线程"><a href="#Daemon-线程" class="headerlink" title="Daemon 线程"></a>Daemon 线程</h3><p>这是一种支持型线程，用作程序中后台调度以及支持性工作。</p>
<h3 id="启动和终止线程"><a href="#启动和终止线程" class="headerlink" title="启动和终止线程"></a>启动和终止线程</h3><h4 id="构造线程"><a href="#构造线程" class="headerlink" title="构造线程"></a>构造线程</h4><p>确定线程所属的线程组，线程优先级，是否是Daemon线程等，线程将在堆内存中等待运行。</p>
<h4 id="启动线程"><a href="#启动线程" class="headerlink" title="启动线程"></a>启动线程</h4><p>start()方法将告知JVM，只要线程规划器空闲，应该立即启动start（）方法的线程。</p>
<h3 id="安全地终止线程"><a href="#安全地终止线程" class="headerlink" title="安全地终止线程"></a>安全地终止线程</h3><p>中断状态是线程的一个标识位，而中断操作是一种简便的线程间交互方式。</p>
<p>同时还可以利用一个boolean变量来控制是否需要停止任务并终止该线程。</p>
<p>main线程通过中断操作和cancel（）方法使得countThread得以终止。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class Shutdown &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line"></span><br><span class="line">        Runner one = new Runner();</span><br><span class="line">        Thread countThread = new Thread(one,&quot;CountThread&quot;);</span><br><span class="line">        countThread.start();</span><br><span class="line">        //睡眠1s,main线程对CountThread进行中断，使得CountThread能够感知中断而结束</span><br><span class="line">        TimeUnit.SECONDS.sleep(1);</span><br><span class="line">        countThread.interrupt();</span><br><span class="line">        Runner two = new Runner();</span><br><span class="line">        countThread = new Thread(two,&quot;CountThread&quot;);</span><br><span class="line">        countThread.start();</span><br><span class="line">        //睡眠1s,main线程对Runner Two进行取消，使CountThread 能够感知on 为false</span><br><span class="line">        TimeUnit.SECONDS.sleep(1);</span><br><span class="line">        two.cancel();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static class Runner implements Runnable&#123;</span><br><span class="line">        private long i;</span><br><span class="line">        private volatile boolean on = true;</span><br><span class="line">        public void run()&#123;</span><br><span class="line">          while(on&amp;&amp;!Thread.currentThread().isInterrupted())&#123;</span><br><span class="line">            i++;</span><br><span class="line">          &#125;</span><br><span class="line">          System.out.println(&quot;Count i = &quot;+i);</span><br><span class="line">        &#125;</span><br><span class="line">        public void cancel()&#123;</span><br><span class="line">            on = false;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="线程间通信"><a href="#线程间通信" class="headerlink" title="线程间通信"></a>线程间通信</h3><p>线程开始运行的时候，拥有自己栈空间。java支持多个线程同时访问一个对象或者对象的成员变量，由于每个线程可以拥有这个变量的拷贝（对象以及成员变量分配的内存是在共享内存中，但每个线程可以拥有一份拷贝，可以加速程序的执行）</p>
<h4 id="volatile关键字"><a href="#volatile关键字" class="headerlink" title="volatile关键字"></a>volatile关键字</h4><p>其他线程对该变量进行改变的时候，可以让所有线程感知到变化。保证所有线程对变量访问的可见性。</p>
<h4 id="synchronized-关键字"><a href="#synchronized-关键字" class="headerlink" title="synchronized 关键字"></a>synchronized 关键字</h4><p>主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，保证了线程对变量访问的可见性与排他性，</p>
<p>任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用时，执行方法的线程必须先获取该对象的监视器才能进入同步块和同步方法，如果没有获取到监视器的线程将会被阻塞在同步块和同步方法的入口处，进入到BLOCKED状态</p>
<p><img src="/home/alex/图片/2018-119.png" alt="filename already exists, rend"></p>
<h3 id="线程同步"><a href="#线程同步" class="headerlink" title="线程同步"></a>线程同步</h3><p>1、synchronized关键字的作用域有二种：</p>
<p>1）是某个对象实例内，synchronized aMethod(){}可以防止多个线程同时访问这个对象的synchronized方法（如果一个对象有多个synchronized方法，只要一个线程访问了其中的一个synchronized方法，其它线程不能同时访问这个对象中任何一个synchronized方法）。这时，不同的对象实例的synchronized方法是不相干扰的。也就是说，其它线程照样可以同时访问相同类的另一个对象实例中的synchronized方法；</p>
<p>假设P1，P2是同一个类的不同对象，这个类中定义了以下几种情况的同步块或者同步方法，P1，P2都可以调用他们</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public synchronized void methodAAA()</span><br></pre></td></tr></table></figure>
<p>上面这个函数当对象P1的不同线程执行这个同步方法的时候，会形成互斥，达到同步的效果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public void methodAAA()&#123;</span><br><span class="line">synchronized(this)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>this指调用这个方法的对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public void method3(SomeObject so)&#123;</span><br><span class="line"></span><br><span class="line">	synchronized(so)</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2）是某个类的范围，synchronized static aStaticMethod{}防止多个线程同时访问这个类中的synchronized static 方法。它可以对类的所有对象实例起作用。</p>
<p>2、除了方法前用synchronized关键字，synchronized关键字还可以用于方法中的某个区块中，表示只对这个区块的资源实行互斥访问。用法是: synchronized(this){/<em>区块</em>/}，它的作用域是当前对象；</p>
<p>1.线程同步的目的是为了保护多个线程访问一个资源的时候破坏资源</p>
<p>2.线程同步方法通过锁来实现，每个对象有且仅有一个锁，这个锁与一个特定的对象关联，线程一旦获取了对象的锁，其他这个对象的线程就无法再访问该对象的其他非同步方法。</p>
<p>3.对于静态同步方法，锁是针对这个类的，锁对象是该类的Class对象。静态和非静态方法的锁互不干预。一个线程获得锁，当在一个同步方法中访问另外对象上的同步方法时，会获取这两个对象锁。</p>
<h3 id="线程数据传递"><a href="#线程数据传递" class="headerlink" title="线程数据传递"></a>线程数据传递</h3><h4 id="通过构造方法"><a href="#通过构造方法" class="headerlink" title="通过构造方法"></a>通过构造方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class MyThreadPrint extends Thread&#123;</span><br><span class="line"></span><br><span class="line">    private String name;</span><br><span class="line">    public MyThreadPrint(String name)&#123;</span><br><span class="line">      this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void run()&#123;</span><br><span class="line">        System.out.println(&quot;hello&quot;+name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String [] args)&#123;</span><br><span class="line"></span><br><span class="line">      Thread thread = new MyThreadPrint(&quot;world&quot;);</span><br><span class="line">      thread.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="通过变量与方法"><a href="#通过变量与方法" class="headerlink" title="通过变量与方法"></a>通过变量与方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public class MyThreadPrint implements Runnable&#123;</span><br><span class="line"></span><br><span class="line">    private String name;</span><br><span class="line"></span><br><span class="line">    public void setName(String name)&#123;</span><br><span class="line">      this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void run()&#123;</span><br><span class="line">        System.out.println(&quot;hello&quot;+name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String [] args)&#123;</span><br><span class="line"></span><br><span class="line">      MyThreadPrint mythread = new MyThreadPrint();</span><br><span class="line">      mythread.setName(&quot;world&quot;);</span><br><span class="line">      Thread thread = new Thread(mythread);</span><br><span class="line">      thread.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="waiting-and-notifying"><a href="#waiting-and-notifying" class="headerlink" title="waiting and notifying"></a>waiting and notifying</h3><p>等待与通知机制</p>
<p>notify() 通知一个在对象上等待的线程，使其从wait()方法返回，返回的前提是该线程获得了对象的锁。</p>
<p>wait()调用该方法线程进入WAITING状态，只有等待另外线程的通知或者被中断才会返回，调用wait()后会释放对象的锁。</p>
<h3 id="Thread-join"><a href="#Thread-join" class="headerlink" title="Thread.join()"></a>Thread.join()</h3><p>每个线程终止的前提是前驱线程的终止，每个线程等待前驱线程终止后，才从join()方法返回</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class Join &#123;</span><br><span class="line">  public static void main(String[]args)throws Exception&#123;</span><br><span class="line"></span><br><span class="line">      Thread previous = Thread.currentThread();</span><br><span class="line">      for(int i=0;i&lt;10;i++) &#123;</span><br><span class="line">        Thread thread = new Thread(new Domino(previous), String.valueOf(i));</span><br><span class="line">        thread.start();</span><br><span class="line">        previous = thread;</span><br><span class="line">      &#125;</span><br><span class="line">      TimeUnit.SECONDS.sleep(5);</span><br><span class="line">      System.out.println(Thread.currentThread().getName()+&quot; terminate.&quot;);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  static class Domino implements Runnable&#123;</span><br><span class="line">    private Thread thread;</span><br><span class="line">    public Domino(Thread thread)&#123;</span><br><span class="line">        this.thread = thread;</span><br><span class="line">    &#125;</span><br><span class="line">    public void run()&#123;</span><br><span class="line">        try&#123;</span><br><span class="line">          thread.join();</span><br><span class="line">        &#125;catch (InterruptedException e)&#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(Thread.currentThread().getName()+&quot; terminate.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/26/DL-C2wk2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/26/DL-C2wk2/" itemprop="url">DL C2wk2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-26T20:20:00+08:00">
                2018-12-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL/" itemprop="url" rel="index">
                    <span itemprop="name">DL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="mini-batch-gradient-descent"><a href="#mini-batch-gradient-descent" class="headerlink" title="mini batch gradient descent"></a>mini batch gradient descent</h3><h4 id="mini-batch-size"><a href="#mini-batch-size" class="headerlink" title="mini-batch size"></a>mini-batch size</h4><ul>
<li>if mini-batch size = m （m=sample size）</li>
</ul>
<p>(X^[i],Y^[i]) = (X,Y)</p>
<p>收敛速度会很快（步长很大）</p>
<p>end up with batch gradient descent,which has to process the whole training set before making progress</p>
<ul>
<li>mini-batch size = 1 又叫做stocastic gradient descent</li>
</ul>
<p>收敛步长会很小，很有可能会不收敛，而且vectorization会很慢</p>
<p>lose the benefits of vectorization</p>
<p>如下图</p>
<p><img src="/home/alex/图片/2018-120.png" alt="filename alrea exists, renamed"></p>
<p>紫色表示batch size=1的收敛，而蓝色表示batch size=m的收敛。</p>
<h4 id="how-to-choose-mini-batch-size"><a href="#how-to-choose-mini-batch-size" class="headerlink" title="how to choose mini-batch size?"></a>how to choose mini-batch size?</h4><p>1.通常用 64,128,256,512大小(power of two)</p>
<p><img src="/home/alex/图片/sto.png" alt="upload sucsful"></p>
<p><img src="/home/alex/图片/update.png" alt="upload sessful"></p>
<p><img src="/home/alex/图片/partition.png" alt="upload ssful"></p>
<h3 id="learning-rate-decay"><a href="#learning-rate-decay" class="headerlink" title="learning rate decay"></a>learning rate decay</h3><p><img src="/home/alex/图片/learning rate decay.png" alt="upload essful"></p>
<p>当梯度下降的时候，由于学习率是固定的，因此可能会在最低点附近徘徊而最终不能收敛。</p>
<h4 id="implementation"><a href="#implementation" class="headerlink" title="implementation"></a>implementation</h4><p><img src="/home/alex/图片/suppose.png" alt="upload essful"></p>
<p>alpha = 1/（1+decay_rate x epoch_num） *alpha</p>
<p>其他方法也可：</p>
<p><img src="/home/alex/图片/qita.png" alt="upload succeful"></p>
<h3 id="mini-Batch-gradient-descent"><a href="#mini-Batch-gradient-descent" class="headerlink" title="mini-Batch gradient descent"></a>mini-Batch gradient descent</h3><h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h4><p><img src="/home/alex/图片/miniBatch.png" alt="upload sful"></p>
<h4 id="parition"><a href="#parition" class="headerlink" title="parition"></a>parition</h4><p><img src="/home/alex/图片/par.png" alt="load successful"></p>
<h4 id="code"><a href="#code" class="headerlink" title="code"></a>code</h4><p>注意最后一个batch_size有可能和前面的size不同，因为样本总数可能不等于batch_size的倍数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a list of random minibatches from (X, Y)</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input data, of shape (input size, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span><br><span class="line">    mini_batch_size -- size of the mini-batches, integer</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(seed)            # To make your &quot;random&quot; minibatches the same as ours</span><br><span class="line">    m = X.shape[1]                  # number of training examples</span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    # Step 1: Shuffle (X, Y)</span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((1,m))</span><br><span class="line"></span><br><span class="line">    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning</span><br><span class="line">    for k in range(0, num_complete_minibatches):</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        mini_batch_X = shuffled_X[:,mini_batch_size*(k):mini_batch_size*(k+1)]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,mini_batch_size*(k):mini_batch_size*(k+1)]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    # Handling the end case (last mini-batch &lt; mini_batch_size)</span><br><span class="line">    if m % mini_batch_size != 0:</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        mini_batch_X = shuffled_X[:,mini_batch_size*(num_complete_minibatches):]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,mini_batch_size*(num_complete_minibatches):]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    return mini_batches</span><br></pre></td></tr></table></figure>
<h3 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h3><p><img src="/home/alex/图片/2018-121.png" alt="filename already exists, named"></p>
<p>蓝色是gradient的方向，而红色是实际velocity的方向，我们让gradient影响velocty下降的方向</p>
<h4 id="code-1"><a href="#code-1" class="headerlink" title="code"></a>code</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def initialize_velocity(parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Initializes the velocity as a python dictionary with:</span><br><span class="line">                - keys: &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span><br><span class="line">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters.</span><br><span class="line">                    parameters[&apos;W&apos; + str(l)] = Wl</span><br><span class="line">                    parameters[&apos;b&apos; + str(l)] = bl</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    v -- python dictionary containing the current velocity.</span><br><span class="line">                    v[&apos;dW&apos; + str(l)] = velocity of dWl</span><br><span class="line">                    v[&apos;db&apos; + str(l)] = velocity of dbl</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize velocity</span><br><span class="line">    for l in range(L):</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = np.zeros(parameters[&apos;W&apos;+str(l+1)].shape)</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = np.zeros(parameters[&apos;b&apos;+str(l+1)].shape)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    return v</span><br></pre></td></tr></table></figure>
<h4 id="update-parameters"><a href="#update-parameters" class="headerlink" title="update parameters"></a>update parameters</h4><p><img src="/home/alex/图片/2018-122.png" alt="filename already exists, remed"></p>
<p><img src="/home/alex/图片/zhanlang.png" alt="upload succesul"></p>
<h3 id="Adam-optimization"><a href="#Adam-optimization" class="headerlink" title="Adam optimization"></a>Adam optimization</h3><p><img src="/home/alex/图片/adam.png" alt="upload succeul"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def initialize_adam(parameters) :</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Initializes v and s as two python dictionaries with:</span><br><span class="line">                - keys: &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span><br><span class="line">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters.</span><br><span class="line">                    parameters[&quot;W&quot; + str(l)] = Wl</span><br><span class="line">                    parameters[&quot;b&quot; + str(l)] = bl</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span><br><span class="line">                    v[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">                    v[&quot;db&quot; + str(l)] = ...</span><br><span class="line">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span><br><span class="line">                    s[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">                    s[&quot;db&quot; + str(l)] = ...</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize v, s. Input: &quot;parameters&quot;. Outputs: &quot;v, s&quot;.</span><br><span class="line">    for l in range(L):</span><br><span class="line">    ### START CODE HERE ### (approx. 4 lines)</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = np.zeros_like(parameters[&apos;W&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;W&quot; + str(l+1)])</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = np.zeros_like(parameters[&apos;b&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;b&quot; + str(l+1)])</span><br><span class="line">        s[&quot;dW&quot; + str(l+1)] = np.zeros_like(parameters[&apos;W&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;W&quot; + str(l+1)])</span><br><span class="line">        s[&quot;db&quot; + str(l+1)] = np.zeros_like(parameters[&apos;b&apos;+str(l+1)]) #(numpy array of zeros with the same shape as parameters[&quot;b&quot; + str(l+1)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return v, s</span><br></pre></td></tr></table></figure>
<h4 id="update-parameters-1"><a href="#update-parameters-1" class="headerlink" title="update parameters"></a>update parameters</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,</span><br><span class="line">                                beta1=0.9, beta2=0.999, epsilon=1e-8):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Update parameters using Adam</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters:</span><br><span class="line">                    parameters[&apos;W&apos; + str(l)] = Wl</span><br><span class="line">                    parameters[&apos;b&apos; + str(l)] = bl</span><br><span class="line">    grads -- python dictionary containing your gradients for each parameters:</span><br><span class="line">                    grads[&apos;dW&apos; + str(l)] = dWl</span><br><span class="line">                    grads[&apos;db&apos; + str(l)] = dbl</span><br><span class="line">    v -- Adam variable, moving average of the first gradient, python dictionary</span><br><span class="line">    s -- Adam variable, moving average of the squared gradient, python dictionary</span><br><span class="line">    learning_rate -- the learning rate, scalar.</span><br><span class="line">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span><br><span class="line">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span><br><span class="line">    epsilon -- hyperparameter preventing division by zero in Adam updates</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">    v -- Adam variable, moving average of the first gradient, python dictionary</span><br><span class="line">    s -- Adam variable, moving average of the squared gradient, python dictionary</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2                 # number of layers in the neural networks</span><br><span class="line">    v_corrected = &#123;&#125;                         # Initializing first moment estimate, python dictionary</span><br><span class="line">    s_corrected = &#123;&#125;                         # Initializing second moment estimate, python dictionary</span><br><span class="line">    </span><br><span class="line">    # Perform Adam update on all parameters</span><br><span class="line">    for l in range(L):</span><br><span class="line">        # Moving average of the gradients. Inputs: &quot;v, grads, beta1&quot;. Output: &quot;v&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        v[&apos;dW&apos;+str(l+1)] = beta1*v[&apos;dW&apos;+str(l+1)]+(1-beta1)*grads[&apos;dW&apos;+str(l+1)]</span><br><span class="line">        v[&apos;db&apos;+str(l+1)] = beta1*v[&apos;db&apos;+str(l+1)]+(1-beta1)*grads[&apos;db&apos;+str(l+1)]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected first moment estimate. Inputs: &quot;v, beta1, t&quot;. Output: &quot;v_corrected&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        v_corrected[&apos;dW&apos;+str(l+1)] = v[&apos;dW&apos;+str(l+1)]/(1-np.power(beta1,t))</span><br><span class="line">        v_corrected[&apos;db&apos;+str(l+1)] = v[&apos;db&apos;+str(l+1)]/(1-np.power(beta1,t))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Moving average of the squared gradients. Inputs: &quot;s, grads, beta2&quot;. Output: &quot;s&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        s[&apos;dW&apos;+str(l+1)] = beta2*s[&apos;dW&apos;+str(l+1)]+(1-beta2)*np.power(grads[&apos;dW&apos;+str(l+1)],2)</span><br><span class="line">        s[&apos;db&apos;+str(l+1)] = beta2*s[&apos;db&apos;+str(l+1)]+(1-beta2)*np.power(grads[&apos;db&apos;+str(l+1)],2)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected second raw moment estimate. Inputs: &quot;s, beta2, t&quot;. Output: &quot;s_corrected&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        s_corrected[&apos;dW&apos;+str(l+1)] = s[&apos;dW&apos;+str(l+1)]/(1-np.power(beta2,t))</span><br><span class="line">        s_corrected[&apos;db&apos;+str(l+1)] = s[&apos;db&apos;+str(l+1)]/(1-np.power(beta2,t))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">        # Update parameters. Inputs: &quot;parameters, learning_rate, v_corrected, s_corrected, epsilon&quot;. Output: &quot;parameters&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        parameters[&apos;W&apos;+str(l+1)] -= learning_rate * v_corrected[&apos;dW&apos;+str(l+1)] /(epsilon+np.sqrt(s_corrected[&apos;dW&apos;+str(l+1)]))</span><br><span class="line">        parameters[&apos;b&apos;+str(l+1)] -= learning_rate * v_corrected[&apos;db&apos;+str(l+1)] /(epsilon+np.sqrt(s_corrected[&apos;db&apos;+str(l+1)]))</span><br><span class="line">        </span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    return parameters, v, s</span><br></pre></td></tr></table></figure>
<h3 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h3><p>注意每次epoch的时候，分为多个batch学习参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,</span><br><span class="line">          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    3-layer neural network model which can be run in different optimizer modes.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input data, of shape (2, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span><br><span class="line">    layers_dims -- python list, containing the size of each layer</span><br><span class="line">    learning_rate -- the learning rate, scalar.</span><br><span class="line">    mini_batch_size -- the size of a mini batch</span><br><span class="line">    beta -- Momentum hyperparameter</span><br><span class="line">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span><br><span class="line">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span><br><span class="line">    epsilon -- hyperparameter preventing division by zero in Adam updates</span><br><span class="line">    num_epochs -- number of epochs</span><br><span class="line">    print_cost -- True to print the cost every 1000 epochs</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             # number of layers in the neural networks</span><br><span class="line">    costs = []                       # to keep track of the cost</span><br><span class="line">    t = 0                            # initializing the counter required for Adam update</span><br><span class="line">    seed = 10                        # For grading purposes, so that your &quot;random&quot; minibatches are the same as ours</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters</span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    # Initialize the optimizer</span><br><span class="line">    if optimizer == &quot;gd&quot;:</span><br><span class="line">        pass # no initialization required for gradient descent</span><br><span class="line">    elif optimizer == &quot;momentum&quot;:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    elif optimizer == &quot;adam&quot;:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line">    </span><br><span class="line">    # Optimization loop</span><br><span class="line">    for i in range(num_epochs):</span><br><span class="line">        </span><br><span class="line">        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span><br><span class="line">        seed = seed + 1</span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        for minibatch in minibatches:</span><br><span class="line"></span><br><span class="line">            # Select a minibatch</span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            # Forward propagation</span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            # Compute cost</span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            # Backward propagation</span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            # Update parameters</span><br><span class="line">            if optimizer == &quot;gd&quot;:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            elif optimizer == &quot;momentum&quot;:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            elif optimizer == &quot;adam&quot;:</span><br><span class="line">                t = t + 1 # Adam counter</span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line">                                                               t, learning_rate, beta1, beta2,  epsilon)</span><br><span class="line">        </span><br><span class="line">        # Print the cost every 1000 epoch</span><br><span class="line">        if print_cost and i % 1000 == 0:</span><br><span class="line">            print(&quot;Cost after epoch %i: %f&quot; % (i, cost))</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;epochs (per 100)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate = &quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/26/DL-wk2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/26/DL-wk2/" itemprop="url">DL C2wk1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-26T13:57:43+08:00">
                2018-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Train-dev-test-sets"><a href="#Train-dev-test-sets" class="headerlink" title="Train/dev/test sets"></a>Train/dev/test sets</h3><p>dev sets,also called hold-out sets, are used to decide the model’s performance.(give you an unbiased estimate of your model’s performance)</p>
<h4 id="example"><a href="#example" class="headerlink" title="example:"></a>example:</h4><p>If you have 10,000,000 examples, how would you split the train/dev/test set?</p>
<pre><code>98% train . 1% dev . 1% test
</code></pre><p>The dev and test set should:</p>
<pre><code>Come from the same distribution
</code></pre><h3 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h3><p><img src="/home/alex/图片/biad.png" alt="upload succesul"></p>
<p>underfitting -&gt; high bias</p>
<p>overfitting -&gt; high variance</p>
<h4 id="example-1"><a href="#example-1" class="headerlink" title="example"></a>example</h4><p><img src="/home/alex/图片/2018-105.png" alt="filename aady exists, renamed"></p>
<p>1.如果train error =1%,dev set error = 11%</p>
<p>则overfitting，说明是high variance</p>
<p>2.如果 train error都很大的话，说明是high bias</p>
<p>3.总的来说，如果dev set error 比train set error<br>大很多，可以说明是overfitting</p>
<h4 id="high-bias-and-high-variance"><a href="#high-bias-and-high-variance" class="headerlink" title="high bias and high variance"></a>high bias and high variance</h4><p><img src="/home/alex/图片/high.png" alt="upload essful"></p>
<p>部分数据过拟合，部分欠拟合</p>
<h3 id="basic-recipe-for-ML"><a href="#basic-recipe-for-ML" class="headerlink" title="basic recipe for ML"></a>basic recipe for ML</h3><p>如果high bias咋办</p>
<p>High bias （欠拟合）<br>(training data performance)</p>
<ul>
<li>1.bigger network</li>
<li>2.optimized neural network architecture</li>
<li>3.other optimizing techniques</li>
</ul>
<p>High Variance? （过拟合）<br>(dev data performance)</p>
<ul>
<li>1.more data</li>
<li>2.regularization</li>
</ul>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p><img src="/home/alex/图片/2018-106.png" alt="filename alrea exists, renamed"></p>
<p>在L1正则化中,w会是一个很sparse的向量，通常在实际应用中L2正则化会应用的更为广泛。</p>
<p>λ 又叫正则化参数</p>
<h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p><img src="/home/alex/图片/2018-107.png" alt="filename alrea exists, renamed"></p>
<h4 id="how-does-it-work"><a href="#how-does-it-work" class="headerlink" title="how does it work?"></a>how does it work?</h4><p>由上面推导我们可知，如果λ越大，那么w会越接近于0，那么以下图的激活函数为例，如果z很小的时候，tanh结果会接近于线性的，，神经网络每一层都将近似于一个线性神经元，那么就可以有效解决过拟合问题，往”欠拟合”或者刚好的方向。</p>
<p><img src="/home/alex/图片/linear.png" alt="upload successful"></p>
<h3 id="drop-out"><a href="#drop-out" class="headerlink" title="drop out"></a>drop out</h3><p>let’s say a nn with layer l = 3,keep_prob = 0.8</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d3= np.random.rand(a3.shape[0],a3.shape[1])&lt;keep_prob</span><br></pre></td></tr></table></figure>
<p>80%的unit会被保留，20%会被drop out</p>
<p>上面语句作用是d380%元素为1,20%元素为0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3 = np.multiply(a3,d3)</span><br></pre></td></tr></table></figure>
<p>然后再scale up</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a3/= keep_prob</span><br></pre></td></tr></table></figure>
<h4 id="why-does-it-work"><a href="#why-does-it-work" class="headerlink" title="why does it work?"></a>why does it work?</h4><p><img src="/home/alex/图片/why.png" alt="upload succeful"></p>
<p>1.在一些神经元很多的层，设置keep_probs低一点，可以有效减少过拟合，实际上是减弱regularization 的作用，在一些神经元很少的层，设置为1.0就好</p>
<p>2.在CV领域，由于输入数据的维数通常很大，一般都需要drop out</p>
<p>3.但是drop out会导致不能够通过画出cost function曲线来debug，解决方法是最开始先把所有keep_prob set to 1,then if no bug, turn on drop out</p>
<h3 id="other-technique-of-reducing-overfitting"><a href="#other-technique-of-reducing-overfitting" class="headerlink" title="other technique of reducing overfitting"></a>other technique of reducing overfitting</h3><h4 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h4><p><img src="/home/alex/图片/2018-108.png" alt="filena already exists, renamed"></p>
<p>防止dev set error增加，采取early stopping,最后会得到一个middle-size的||w||^2</p>
<h4 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h4><p><img src="/home/alex/图片/2018-109.png" alt="filename alre exists, renamed"></p>
<p>通过变换现有的数据集，来获得更多的数据集</p>
<h3 id="normalizing-inputs"><a href="#normalizing-inputs" class="headerlink" title="normalizing inputs"></a>normalizing inputs</h3><p><img src="/home/alex/图片/2018-110.png" alt="filename alrea exists, renamed"></p>
<p>by subtract the mean and scalling the variance</p>
<p><img src="/home/alex/图片/2018-111.png" alt="filename exists, renamed"></p>
<p>效果就是会加快optimizing 的速度</p>
<h3 id="Vanishing-gradients"><a href="#Vanishing-gradients" class="headerlink" title="Vanishing gradients"></a>Vanishing gradients</h3><p>很深层的神经网络，权重相乘累积起来的话后果很严重</p>
<p><img src="/home/alex/图片/2018-113.png" alt="filename already exists, renmed"></p>
<p>参数初始化方法：</p>
<p>这样可以使得w的值接近于1，不会导致梯度消失和梯度爆炸</p>
<p><img src="/home/alex/图片/single.png" alt="upload cessful"></p>
<p>其中的Xavior initialization</p>
<p>可以用来保证输入输出数据的分布相近，加快收敛速度（方差与均值大概相同）</p>
<p><a href="https://blog.csdn.net/shuzfan/article/details/51338178" target="_blank" rel="noopener">https://blog.csdn.net/shuzfan/article/details/51338178</a></p>
<h3 id="gradient-checking"><a href="#gradient-checking" class="headerlink" title="gradient checking"></a>gradient checking</h3><p><img src="/home/alex/图片/gradient.png" alt="upload ccessful"></p>
<h4 id="grad-check"><a href="#grad-check" class="headerlink" title="grad check"></a>grad check</h4><p><img src="/home/alex/图片/gradcheck.png" alt="upload essful"></p>
<h4 id="notes"><a href="#notes" class="headerlink" title="notes"></a>notes</h4><p>1.no use in training ,only to debug</p>
<p>2.if fails grad check,look at components to try to identify bug.</p>
<p>3.remember regularization</p>
<p>4.doesn’t work with dropout</p>
<p>5.run at random initialization </p>
<h3 id="initialization"><a href="#initialization" class="headerlink" title="initialization"></a>initialization</h3><h4 id="zero-initialization"><a href="#zero-initialization" class="headerlink" title="zero initialization"></a>zero initialization</h4><p>如果把W矩阵初始化为0的话，相当于在训练一个各层只有一个神经元的神经网络，因为每一层的每个神经元其实都在学习相同的参数。</p>
<p>这时候神经网络只相当于一个线性分类器。</p>
<p>但是bias可以设置处值为0</p>
<h4 id="large-random-initialization"><a href="#large-random-initialization" class="headerlink" title="large random initialization"></a>large random initialization</h4><p>1.poor initialization can lead to vanishing/exploding gradients</p>
<p>2.如果一开始w初值非常大，梯度下降所花时间会很长，（需要更多迭代次数）</p>
<h4 id="He-random-initialization"><a href="#He-random-initialization" class="headerlink" title="He random initialization"></a>He random initialization</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def initialize_parameters_he(layers_dims):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    layer_dims -- python array (list) containing the size of each layer.</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span><br><span class="line">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span><br><span class="line">                    b1 -- bias vector of shape (layers_dims[1], 1)</span><br><span class="line">                    ...</span><br><span class="line">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span><br><span class="line">                    bL -- bias vector of shape (layers_dims[L], 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - 1 # integer representing the number of layers</span><br><span class="line">     </span><br><span class="line">    for l in range(1, L + 1):</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        parameters[&apos;W&apos; + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * (np.sqrt(2. / layers_dims[l-1]))</span><br><span class="line">        parameters[&apos;b&apos; + str(l)] = np.zeros((layers_dims[l], 1))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>
<h4 id="xavier-random-initialization"><a href="#xavier-random-initialization" class="headerlink" title="xavier random initialization"></a>xavier random initialization</h4><p>只是把 sqrt（2./layers_dims[l-1]） 换作sqrt（1./layers_dims[l-1]）</p>
<h3 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h3><p><img src="/home/alex/图片/2018-114.png" alt="flename already exists, renamed"></p>
<p>同时code如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def compute_cost_with_regularization(A3, Y, parameters, lambd):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the cost function with L2 regularization. See formula (2) above.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span><br><span class="line">    parameters -- python dictionary containing parameters of the model</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    cost - value of the regularized loss function (formula (2))</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    m = Y.shape[1]</span><br><span class="line">    W1 = parameters[&quot;W1&quot;]</span><br><span class="line">    W2 = parameters[&quot;W2&quot;]</span><br><span class="line">    W3 = parameters[&quot;W3&quot;]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    sumW1 = np.sum(np.square(W1))</span><br><span class="line">    sumW2 = np.sum(np.square(W2))</span><br><span class="line">    sumW3 = np.sum(np.square(W3))</span><br><span class="line">    L2_regularization_cost = (0.5/m*lambd)*(sumW1+sumW2+sumW3)</span><br><span class="line">    ### END CODER HERE ###</span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure>
<p>同时引入正则化的话，在backword propa的时候，要加上正则项</p>
<p><img src="/home/alex/图片/www.png" alt="upload sussful"></p>
<p>code<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def backward_propagation_with_regularization(X, Y, cache, lambd):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (input size, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span><br><span class="line">    cache -- cache output from forward_propagation()</span><br><span class="line">    lambd -- regularization hyperparameter, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd/m)*W3</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd/m)*W2</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    dW1 = 1./m * np.dot(dZ1, X.T) + (lambd/m)*W1</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dZ3&quot;: dZ3, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3,&quot;dA2&quot;: dA2,</span><br><span class="line">                 &quot;dZ2&quot;: dZ2, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dA1&quot;: dA1, </span><br><span class="line">                 &quot;dZ1&quot;: dZ1, &quot;dW1&quot;: dW1, &quot;db1&quot;: db1&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure></p>
<p>lambd = 0.6</p>
<p><img src="/home/alex/图片/2018-115.png" alt="filename al exists, renamed"></p>
<p>lambd = 0.7</p>
<p><img src="/home/alex/图片/0.8.png" alt="upload succsful"></p>
<p>lambd = 0.8</p>
<p><img src="/home/alex/图片/2018-116.png" alt="filename already exists,amed"></p>
<p>λ增大，能够减少过拟合现象，但是training set error 也会随之减少</p>
<h3 id="drop-out-1"><a href="#drop-out-1" class="headerlink" title="drop out"></a>drop out</h3><h4 id="what-is-inverted-dropout"><a href="#what-is-inverted-dropout" class="headerlink" title="what is inverted dropout?"></a>what is inverted dropout?</h4><p><img src="/home/alex/图片/dropout.png" alt="upload cessful"></p>
<p><img src="/home/alex/图片/inve.png" alt="upload ful"></p>
<h4 id="drop-out-2"><a href="#drop-out-2" class="headerlink" title="drop out"></a>drop out</h4><p><img src="/home/alex/图片/drop.png" alt="upload successul"></p>
<p>1.创建一个np.array D1 which has the same size as A1（np.random.randn(A.shape[0],A.shape[1])）</p>
<p>2.当A的元素&lt;D[keep_prob]的时候为1，大于的时候为0</p>
<p>3.A = np.multiply(A,D)</p>
<p>4.scale A,i.e. A/=keep_prob (inverted dropout)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">def forward_propagation_with_dropout(X, parameters, keep_prob=0.5):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (2, number of examples)</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:</span><br><span class="line">                    W1 -- weight matrix of shape (20, 2)</span><br><span class="line">                    b1 -- bias vector of shape (20, 1)</span><br><span class="line">                    W2 -- weight matrix of shape (3, 20)</span><br><span class="line">                    b2 -- bias vector of shape (3, 1)</span><br><span class="line">                    W3 -- weight matrix of shape (1, 3)</span><br><span class="line">                    b3 -- bias vector of shape (1, 1)</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span><br><span class="line">    cache -- tuple, information stored for computing the backward propagation</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(1)</span><br><span class="line">    </span><br><span class="line">    # retrieve parameters</span><br><span class="line">    W1 = parameters[&quot;W1&quot;]</span><br><span class="line">    b1 = parameters[&quot;b1&quot;]</span><br><span class="line">    W2 = parameters[&quot;W2&quot;]</span><br><span class="line">    b2 = parameters[&quot;b2&quot;]</span><br><span class="line">    W3 = parameters[&quot;W3&quot;]</span><br><span class="line">    b3 = parameters[&quot;b3&quot;]</span><br><span class="line">    </span><br><span class="line">    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span><br><span class="line">    D1 = np.random.rand(A1.shape[0], A1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)</span><br><span class="line">    D1 = D1 &lt; keep_prob                            # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span><br><span class="line">    A1 = A1 * D1                                      # Step 3: shut down some neurons of A1</span><br><span class="line">    A1 = A1 / keep_prob                               # Step 4: scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    ### START CODE HERE ### (approx. 4 lines)</span><br><span class="line">    D2 = np.random.rand(A2.shape[0], A2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)</span><br><span class="line">    D2 = D2 &lt; keep_prob                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)                           </span><br><span class="line">    A2 = A2 * D2                                      # Step 3: shut down some neurons of A2</span><br><span class="line">    A2 = A2 / keep_prob                               # Step 4: scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    return A3, cache</span><br></pre></td></tr></table></figure>
<h4 id="drop-out-in-backward"><a href="#drop-out-in-backward" class="headerlink" title="drop out in backward"></a>drop out in backward</h4><p>just perform dA2*=D2 and scaling</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def backward_propagation_with_dropout(X, Y, cache, keep_prob):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the backward propagation of our baseline model to which we added dropout.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (2, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span><br><span class="line">    cache -- cache output from forward_propagation_with_dropout()</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = 1. / m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA2 = dA2 / keep_prob              # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    dW2 = 1. / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA1 = dA1 / keep_prob              # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    dW1 = 1. / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dZ3&quot;: dZ3, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3,&quot;dA2&quot;: dA2,</span><br><span class="line">                 &quot;dZ2&quot;: dZ2, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dA1&quot;: dA1, </span><br><span class="line">                 &quot;dZ1&quot;: dZ1, &quot;dW1&quot;: dW1, &quot;db1&quot;: db1&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure>
<h3 id="gradient-checking-1"><a href="#gradient-checking-1" class="headerlink" title="gradient checking"></a>gradient checking</h3><p>以J = x*theta 为例</p>
<p><img src="/home/alex/图片/theta.png" alt="upload succesul"></p>
<p>implementation</p>
<p><img src="/home/alex/图片/imple.png" alt="upload succeful"></p>
<p>注意<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.norm 是求范数的函数，默认是求二范数</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def gradient_check(x, theta, epsilon = 1e-7):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation presented in Figure 1.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    x -- a real-valued input</span><br><span class="line">    theta -- our parameter, a real number as well</span><br><span class="line">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don&apos;t need to worry about the limit.</span><br><span class="line">    ### START CODE HERE ### (approx. 5 lines)</span><br><span class="line">    thetaPlus = theta+epsilon</span><br><span class="line">    thetaMinus = theta-epsilon</span><br><span class="line">    JPlus = forward_propagation(x,thetaPlus)</span><br><span class="line">    JMinus = forward_propagation(x,thetaMinus)</span><br><span class="line">    gradapprox = (JPlus-JMinus)/(2*epsilon)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Check if gradapprox is close enough to the output of backward_propagation()</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    grad = backward_propagation(x, gradapprox)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox)</span><br><span class="line">    demurator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)</span><br><span class="line">    difference = numerator/demurator</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    if difference &lt; 1e-7:</span><br><span class="line">        print (&quot;The gradient is correct!&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print (&quot;The gradient is wrong!&quot;)</span><br><span class="line">    </span><br><span class="line">    return difference</span><br></pre></td></tr></table></figure>
<p>对于多维的情况</p>
<p><img src="/home/alex/图片/dimension.png" alt="upload success"></p>
<p>把所有params压缩到一个向量，然后一个for循环，计算每个参数的grad，gradapprox，并加入到一个向量当中，分别得到一个gradapprox与grad向量，再利用这两个向量求范数，求difference</p>
<p><img src="/home/alex/图片/2018-117.png" alt="filename already exists, reamed"></p>
<p>code<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:</span><br><span class="line">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. </span><br><span class="line">    x -- input datapoint, of shape (input size, 1)</span><br><span class="line">    y -- true &quot;label&quot;</span><br><span class="line">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Set-up variables</span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[0]</span><br><span class="line">    J_plus = np.zeros((num_parameters, 1))</span><br><span class="line">    J_minus = np.zeros((num_parameters, 1))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, 1))</span><br><span class="line">    </span><br><span class="line">    # Compute gradapprox</span><br><span class="line">    for i in range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        # Compute J_plus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_plus[i]&quot;.</span><br><span class="line">        # &quot;_&quot; is used because the function you have to outputs two parameters but we only care about the first one</span><br><span class="line">        ### START CODE HERE ### (approx. 3 lines)</span><br><span class="line">        thetaplus =  np.copy(parameters_values)                                       # Step 1</span><br><span class="line">        thetaplus[i][0] = thetaplus[i][0] + epsilon                                   # Step 2</span><br><span class="line">        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  # Step 3</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Compute J_minus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_minus[i]&quot;.</span><br><span class="line">        ### START CODE HERE ### (approx. 3 lines)</span><br><span class="line">        thetaminus = np.copy(parameters_values)                                       # Step 1</span><br><span class="line">        thetaminus[i][0] = thetaminus[i][0] - epsilon                                 # Step 2        </span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) # Step 3</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Compute gradapprox[i]</span><br><span class="line">        ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Compare gradapprox to backward propagation gradients by computing difference.</span><br><span class="line">    ### START CODE HERE ### (approx. 1 line)</span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                                     # Step 1&apos;</span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   # Step 2&apos;</span><br><span class="line">    difference = numerator / denominator                                              # Step 3&apos;</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    if difference &gt; 1e-7:</span><br><span class="line">        print(&quot;\033[93m&quot; + &quot;There is a mistake in the backward propagation! difference = &quot; + str(difference) + &quot;\033[0m&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;\033[92m&quot; + &quot;Your backward propagation works perfectly fine! difference = &quot; + str(difference) + &quot;\033[0m&quot;)</span><br><span class="line">    </span><br><span class="line">    return difference</span><br></pre></td></tr></table></figure></p>
<h3 id="concolusion"><a href="#concolusion" class="headerlink" title="concolusion"></a>concolusion</h3><p>1.L2正则化和drop out都可以帮你解决overfitting</p>
<p>2.regularization 会使得weight变得非常小</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/24/CS230-深层神经网络图像分类/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/24/CS230-深层神经网络图像分类/" itemprop="url">CS230-深层神经网络图像分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-24T16:58:35+08:00">
                2018-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p><img src="/home/alex/图片/2018-102.png" alt="filename eady exists, renamed"></p>
<p>每一张图片都可以把它转化为一个m*1的向量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Reshape the training and test examples </span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The &quot;-1&quot; makes reshape flatten the remaining dimensions</span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T</span><br><span class="line"></span><br><span class="line"># Standardize data to have feature values between 0 and 1.</span><br><span class="line">train_x = train_x_flatten/255.</span><br><span class="line">test_x = test_x_flatten/255.</span><br><span class="line"></span><br><span class="line">print (&quot;train_x&apos;s shape: &quot; + str(train_x.shape))</span><br><span class="line">print (&quot;test_x&apos;s shape: &quot; + str(test_x.shape))</span><br></pre></td></tr></table></figure>
<p><img src="/home/alex/图片/what.png" alt="upload sussful"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span><br><span class="line">    Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span><br><span class="line">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span><br><span class="line">    learning_rate -- learning rate of the gradient descent update rule</span><br><span class="line">    num_iterations -- number of iterations of the optimization loop</span><br><span class="line">    print_cost -- if True, it prints the cost every 100 steps</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- parameters learnt by the model. They can then be used to predict.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    np.random.seed(1)</span><br><span class="line">    costs = []                         # keep track of cost</span><br><span class="line">    </span><br><span class="line">    # Parameters initialization.</span><br><span class="line">    ### START CODE HERE ###</span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line">    for i in range(0, num_iterations):</span><br><span class="line"></span><br><span class="line">        # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Compute cost.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">        # Backward propagation.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"> </span><br><span class="line">        # Update parameters.</span><br><span class="line">        ### START CODE HERE ### (≈ 1 line of code)</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">                </span><br><span class="line">        # Print the cost every 100 training example</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(&apos;cost&apos;)</span><br><span class="line">    plt.xlabel(&apos;iterations (per tens)&apos;)</span><br><span class="line">    plt.title(&quot;Learning rate =&quot; + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://AlexanderChiuluvB.github.io/2018/12/24/CS230-搭建深层的神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Chiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex Chiu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/24/CS230-搭建深层的神经网络/" itemprop="url">CS230-搭建深层的神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-24T15:18:11+08:00">
                2018-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="参数的初始化"><a href="#参数的初始化" class="headerlink" title="参数的初始化"></a>参数的初始化</h3><p><img src="/home/alex/图片/搭建.png" alt="upload succeful"></p>
<p>搭建深层的神经网络，各层W,X,B的维度一定要搞清楚</p>
<p>如上图所示，第i层W的维度为（n^[i]，n^[i-1]）</p>
<p>其中12288是特征数量，209是样本数，n^[i]是第i层神经元的数量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">代码如下</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def initialize_parameters_deep(layer_dims):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span><br><span class="line">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span><br><span class="line">                    bl -- bias vector of shape (layer_dims[l], 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            # number of layers in the network</span><br><span class="line"></span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        parameters[&apos;W&apos;+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01</span><br><span class="line">        parameters[&apos;b&apos;+str(l)] = np.zeros((layer_dims[l],1))</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        assert(parameters[&apos;W&apos; + str(l)].shape == (layer_dims[l], layer_dims[l-1]))</span><br><span class="line">        assert(parameters[&apos;b&apos; + str(l)].shape == (layer_dims[l], 1))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>
<h3 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h3><p><img src="/home/alex/图片/实现.png" alt="upload successl"></p>
<p>实现Relu或者sigmoid激活函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid(Z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the sigmoid activation in numpy</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    Z -- numpy array of any shape</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    A -- output of sigmoid(z), same shape as Z</span><br><span class="line">    cache -- returns Z as well, useful during backpropagation</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    A = 1/(1+np.exp(-Z))</span><br><span class="line">    cache = Z</span><br><span class="line">    </span><br><span class="line">    return A, cache</span><br><span class="line"></span><br><span class="line">def relu(Z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the RELU function.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    Z -- Output of the linear layer, of any shape</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    A -- Post-activation parameter, of the same shape as Z</span><br><span class="line">    cache -- a python dictionary containing &quot;A&quot; ; stored for computing the backward pass efficiently</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    A = np.maximum(0,Z)</span><br><span class="line">    </span><br><span class="line">    assert(A.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    cache = Z </span><br><span class="line">    return A, cache</span><br></pre></td></tr></table></figure>
<p>以下是linear_activation_forward的函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def linear_activation_forward(A_prev, W, b, activation):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span><br><span class="line">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span><br><span class="line">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span><br><span class="line">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    A -- the output of the activation function, also called the post-activation value </span><br><span class="line">    cache -- a python dictionary containing &quot;linear_cache&quot; and &quot;activation_cache&quot;;</span><br><span class="line">             stored for computing the backward pass efficiently</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    if activation == &quot;sigmoid&quot;:</span><br><span class="line">        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        Z = np.dot(W,A_prev)+b</span><br><span class="line">        A,activation_cache = sigmoid(Z)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    elif activation == &quot;relu&quot;:</span><br><span class="line">        # Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        Z = np.dot(W,A_prev)+b</span><br><span class="line">        A,activation_cache = relu(Z)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    assert (A.shape == (W.shape[0], A_prev.shape[1]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    return A, cache</span><br></pre></td></tr></table></figure>
<h4 id="对于L层网络的forwarding"><a href="#对于L层网络的forwarding" class="headerlink" title="对于L层网络的forwarding"></a>对于L层网络的forwarding</h4><p><img src="/home/alex/图片/forwarding.png" alt="upload succul"></p>
<p>前L-1层作Relu变换，最后一层做sigmoid变换（由于是二分类）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def L_model_forward(X, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- data, numpy array of shape (input size, number of examples)</span><br><span class="line">    parameters -- output of initialize_parameters_deep()</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    AL -- last post-activation value</span><br><span class="line">    caches -- list of caches containing:</span><br><span class="line">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span><br><span class="line">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // 2                  # number of layers in the neural network</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.</span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        A,cache = linear_activation_forward(A_prev, parameters[&apos;W&apos;+str(l)], parameters[&apos;b&apos;+str(l)], activation=&apos;relu&apos;)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    AL,cache = linear_activation_forward(A, parameters[&apos;W&apos;+str(L)], parameters[&apos;b&apos;+str(L)], activation=&apos;sigmoid&apos;)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    assert(AL.shape == (1,X.shape[1]))</span><br><span class="line">            </span><br><span class="line">    return AL, caches</span><br></pre></td></tr></table></figure>
<p>在这里你得到的AL就是经过L曾训练后的parameters，可以用来测试结果，同时所有中间结果都保存在了caches中。</p>
<h3 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h3><p><img src="/home/alex/图片/computeTheCost.png" alt="upload succel"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def compute_cost(AL, Y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the cost function defined by equation (7).</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    cost -- cross-entropy cost</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m = Y.shape[1]</span><br><span class="line"></span><br><span class="line">    # Compute loss from aL and y.</span><br><span class="line">    ### START CODE HERE ### (≈ 1 lines of code)</span><br><span class="line">    ##attention! here Y,and np.log(AL) is scalar so use multiply rather than dot</span><br><span class="line">    cost = (-1./m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL)))</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      # To make sure your cost&apos;s shape is what we expect (e.g. this turns [[17]] into 17).</span><br><span class="line">    assert(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure>
<h3 id="backward-propagation"><a href="#backward-propagation" class="headerlink" title="backward propagation"></a>backward propagation</h3><p><img src="/home/alex/图片/如图.png" alt="upload succeful"></p>
<p>要通过计算dL/dz 来算出dw,dAprev,db 来进行梯度下降</p>
<p><img src="/home/alex/图片/怎么计算呢.png" alt="upload succesful"></p>
<p>假设已知第l层的dZ，同时我们在做forward propagation的时候把对应该层的A_PREV,W已经存起来了</p>
<h4 id="两个辅助函数"><a href="#两个辅助函数" class="headerlink" title="两个辅助函数"></a>两个辅助函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def relu_backward(dA, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for a single RELU unit.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient, of any shape</span><br><span class="line">    cache -- &apos;Z&apos; where we store for computing backward propagation efficiently</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dZ -- Gradient of the cost with respect to Z</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    dZ = np.array(dA, copy=True) # just converting dz to a correct object.</span><br><span class="line">    </span><br><span class="line">    # When z &lt;= 0, you should set dz to 0 as well. </span><br><span class="line">    dZ[Z &lt;= 0] = 0</span><br><span class="line">    </span><br><span class="line">    assert (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    return dZ</span><br><span class="line"></span><br><span class="line">def sigmoid_backward(dA, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for a single SIGMOID unit.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient, of any shape</span><br><span class="line">    cache -- &apos;Z&apos; where we store for computing backward propagation efficiently</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dZ -- Gradient of the cost with respect to Z</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = 1/(1+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (1-s)</span><br><span class="line">    </span><br><span class="line">    assert (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    return dZ</span><br></pre></td></tr></table></figure>
<h4 id="linear-backward"><a href="#linear-backward" class="headerlink" title="linear backward"></a>linear backward</h4><p>计算公式如上图所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def linear_backward(dZ, cache):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the linear portion of backward propagation for a single layer (layer l)</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span><br><span class="line">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span><br><span class="line">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span><br><span class="line">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[1]</span><br><span class="line"></span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines of code)</span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    dW = (1/m)*np.dot(dZ,A_prev.T)</span><br><span class="line">    db = (1/m)*np.sum(dZ,axis=1,keepdims = True)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    assert (dA_prev.shape == A_prev.shape)</span><br><span class="line">    assert (dW.shape == W.shape)</span><br><span class="line">    assert (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    return dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h4 id="linear-activation-backward"><a href="#linear-activation-backward" class="headerlink" title="linear_activation_backward"></a>linear_activation_backward</h4><p>那么上一个函数的dZ又是怎么计算？</p>
<p><img src="/home/alex/图片/at.png" alt="upload succsful"></p>
<p>我们运用最开始定义的两个辅助函数来计算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def linear_activation_backward(dA, cache, activation):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    dA -- post-activation gradient for current layer l </span><br><span class="line">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span><br><span class="line">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span><br><span class="line">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span><br><span class="line">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    if activation == &quot;relu&quot;:</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev,dW,db = linear_backward(dZ,linear_cache)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    elif activation == &quot;sigmoid&quot;:</span><br><span class="line">        ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev,dW,db = linear_backward(dZ,linear_cache)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h3 id="对L层的神经网络的backward"><a href="#对L层的神经网络的backward" class="headerlink" title="对L层的神经网络的backward"></a>对L层的神经网络的backward</h3><p>对最后一层是sigmoid_backward,剩下的所有层是linear_backward</p>
<p><img src="/home/alex/图片/2018-101.png" alt="filename already exists, renaed"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: L_model_backward</span><br><span class="line"></span><br><span class="line">def L_model_backward(AL, Y, caches):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    AL -- probability vector, output of the forward propagation (L_model_forward())</span><br><span class="line">    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat)</span><br><span class="line">    caches -- list of caches containing:</span><br><span class="line">                every cache of linear_activation_forward() with &quot;relu&quot; (it&apos;s caches[l], for l in range(L-1) i.e l = 0...L-2)</span><br><span class="line">                the cache of linear_activation_forward() with &quot;sigmoid&quot; (it&apos;s caches[L-1])</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    grads -- A dictionary with the gradients</span><br><span class="line">             grads[&quot;dA&quot; + str(l)] = ... </span><br><span class="line">             grads[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">             grads[&quot;db&quot; + str(l)] = ... </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) # the number of layers</span><br><span class="line">    m = AL.shape[1]</span><br><span class="line">    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL</span><br><span class="line">    </span><br><span class="line">    # Initializing the backpropagation</span><br><span class="line">    ### START CODE HERE ### (1 line of code)</span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;AL, Y, caches&quot;. Outputs: &quot;grads[&quot;dAL&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]</span><br><span class="line">    ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">    current_cache = caches[-1]</span><br><span class="line">    grads[&quot;dA&quot;+str(L)],grads[&quot;dW&quot;+str(L)],grads[&quot;db&quot;+str(L)] = linear_activation_backward(dAL, current_cache, &apos;sigmoid&apos;)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    for l in reversed(range(L-1)):</span><br><span class="line">        # lth layer: (RELU -&gt; LINEAR) gradients.</span><br><span class="line">        # Inputs: &quot;grads[&quot;dA&quot; + str(l + 2)], caches&quot;. Outputs: &quot;grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span><br><span class="line">        ### START CODE HERE ### (approx. 5 lines)</span><br><span class="line">        current_cache = caches[l];</span><br><span class="line">        current_A = grads[&quot;dA&quot;+str(l+2)]</span><br><span class="line">        grads[&quot;dA&quot; + str(l + 1)],grads[&quot;dW&quot; + str(l + 1)],grads[&quot;db&quot; + str(l + 1)] = linear_activation_backward(current_A, current_cache, &apos;relu&apos;)</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">    return grads</span><br></pre></td></tr></table></figure>
<h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def update_parameters(parameters, grads, learning_rate):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Update parameters using gradient descent</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters </span><br><span class="line">    grads -- python dictionary containing your gradients, output of L_model_backward</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">                  parameters[&quot;W&quot; + str(l)] = ... </span><br><span class="line">                  parameters[&quot;b&quot; + str(l)] = ...</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural network</span><br><span class="line"></span><br><span class="line">    # Update rule for each parameter. Use a for loop.</span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines of code)</span><br><span class="line">    for l in range(L):</span><br><span class="line">        parameters[&quot;W&quot;+str(l+1)]-=learning_rate*grads[&quot;dW&quot;+str(l+1)]</span><br><span class="line">        parameters[&quot;b&quot;+str(l+1)]-=learning_rate*grads[&quot;db&quot;+str(l+1)]</span><br><span class="line">        </span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.jpg"
                alt="Alex Chiu" />
            
              <p class="site-author-name" itemprop="name">Alex Chiu</p>
              <p class="site-description motion-element" itemprop="description">Alex's personal blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">52</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alex Chiu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
